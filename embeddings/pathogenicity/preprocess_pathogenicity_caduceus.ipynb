{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef55f5d-c55e-4318-b832-0601921b8eb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# Load pre-trained Caduceus model and tokenizer\n",
    "model_name = \"kuleshov-group/caduceus-ph_seqlen-131k_d_model-256_n_layer-16\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Move model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0753d30e-f444-4d8f-ad98-5ae90a1c0cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pathogenecity_type='noncoding'\n",
    "# pathogenecity_type='missense'\n",
    "\n",
    "df=pd.read_csv('dna_segment_'+pathogenecity_type+'.csv')\n",
    "# df.head()\n",
    "df_origin = df\n",
    "df_origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c420d36-8dd3-45fb-95b5-3ea29310fc46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置并行参数和批处理大小\n",
    "num_parallel = 10\n",
    "batch_size = 3000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b59b383-8e87-4261-9a41-6be9b4404403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import jax.numpy as jnp\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "# Vectorized tokenization function\n",
    "def vectorized_tokenizer(subsequences):\n",
    "    # Assuming tokenizer.batch_tokenize can handle a list of subsequences\n",
    "    # tokens_ids = [b[1] for b in tokenizer.batch_tokenize(subsequences)]\n",
    "    # return jnp.asarray(tokens_ids, dtype=jnp.int32)\n",
    "\n",
    "    \n",
    "    # Tokenize the batch of sequences\n",
    "    tokens = tokenizer(subsequences, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    # Move tokens to GPU\n",
    "    tokens = {key: val.to(device) for key, val in tokens.items()}\n",
    "    return tokens\n",
    "\n",
    "# Vectorized embedding function\n",
    "def vectorized_embedding(tokens):\n",
    "    # Forward pass to compute last layer embeddings for the batch\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens, output_hidden_states=True)  # Enable output of hidden states\n",
    "        hidden_states = outputs.hidden_states  # Access all hidden states\n",
    "        last_layer_embeddings = hidden_states[-1]  # Get the last layer embeddings (batch_size, seq_len, hidden_size)\n",
    "    \n",
    "    # Compute the mean of the last layer embeddings across the token (sequence) dimension for each sequence in the batch\n",
    "    # Dimension 1 corresponds to the token/sequence length, so we compute the mean along this axis\n",
    "    mean_embeddings = torch.mean(last_layer_embeddings, dim=1)  # (batch_size, hidden_size)\n",
    "    \n",
    "    # If needed, squeeze out any extra dimensions (though this shouldn't be necessary after mean calculation)\n",
    "    mean_embeddings_squeezed = mean_embeddings.squeeze(dim=1)\n",
    "\n",
    "    return mean_embeddings_squeezed\n",
    "\n",
    "# Tokenization and embedding combined in a batch-wise function\n",
    "def process_batch(subsequences):\n",
    "    tokens = vectorized_tokenizer(subsequences)\n",
    "    embeddings = vectorized_embedding(tokens)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def apply_get_embeddings_dask(df):\n",
    "    # subsequences = df['ref_forward_sequence'].values  # Get all subsequences in the batch\n",
    "    subsequences = df['sequence'].tolist() \n",
    "    embeddings = process_batch(subsequences)  # Process in a vectorized manner\n",
    "    embeddings_cpu = embeddings.cpu().numpy()\n",
    "    \n",
    "    # df['embedding'] = list(embeddings_cpu)  # Assign embeddings back to the DataFrame\n",
    "    df2 = pd.DataFrame(embeddings_cpu, columns=[f'{i+1}' for i in range(embeddings_cpu.shape[1])])\n",
    "    df = pd.concat([df.reset_index(drop=True), df2.reset_index(drop=True)], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec2bfdc-9725-4a6d-b5fc-2e26be0beb7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Define chunk size (number of rows per chunk)\n",
    "# chunksize = 10000  # Adjust chunk size according to your memory capacity\n",
    "# num_parallel = 10\n",
    "\n",
    "#================dataset -> df================\n",
    "# code for loading homo_sapiens_dataset/train\n",
    "# save_path=\"homo_sapiens_dataset\"\n",
    "# mydatasets = load_from_disk(save_path)\n",
    "# # print(mydatasets)\n",
    "# train = mydatasets['train']\n",
    "# df = train.to_pandas()\n",
    "\n",
    "\n",
    "# chunkid=0\n",
    "typename=\"pathogenecity\"\n",
    "\n",
    "for chunkid in range(0,1):\n",
    "\n",
    "    \n",
    "    # save_path=\"methylation_multisets\"\n",
    "    # chunk_save_path = f\"{save_path}/train_{chunkid}\"\n",
    "    # dataset_chunk = load_from_disk(chunk_save_path)\n",
    "    # df = dataset_chunk.to_pandas()\n",
    "    \n",
    "    # df = segments_df\n",
    "    \n",
    "    # code for loading pathgenicity_noncoding_multisets/combined_dataset\n",
    "    # save_path = \"pathgenicity_noncoding_multisets\"\n",
    "    # df = load_from_disk(save_path+\"/combined_dataset\")\n",
    "    \n",
    "    \n",
    "    #================df -> df's chunks================\n",
    "    # Assume df is your huge DataFrame\n",
    "    chunk_size = 10000  # Define the number of rows per chunk\n",
    "    num_parallel = 10\n",
    "    \n",
    "    num_chunks = int(np.ceil(len(df) / chunk_size))  # Calculate the number of chunks\n",
    "    \n",
    "    # Split the DataFrame into chunks using array_split\n",
    "    chunks = np.array_split(df, num_chunks)\n",
    "    \n",
    "    # Initialize an empty list to store the processed chunks\n",
    "    processed_chunks = []\n",
    "    # num_parallel=10\n",
    "    \n",
    "    #================process each chunk with dask's ddf================\n",
    "    # Iterate over each chunk\n",
    "    for chunk in chunks:\n",
    "        # Process each chunk\n",
    "        # print(chunk.head())  # Example: print the first few rows of the chunk\n",
    "    \n",
    "        ddf = dd.from_pandas(chunk, npartitions=num_parallel)  # Adjust 'npartitions' based on resources\n",
    "\n",
    "        # sample_subsequences = chunk['ref_forward_sequence'].tolist()\n",
    "        # sample_embeddings = process_batch(sample_subsequences).cpu().numpy()  # Get sample embeddings\n",
    "        # num_embedding_columns = sample_embeddings.shape[1]\n",
    "        num_embedding_columns = 256\n",
    "        \n",
    "        # Define metadata for the new DataFrame structure\n",
    "        # Drop the 'embedding' column since it no longer exists\n",
    "        meta = chunk.copy()\n",
    "        meta = meta.drop(columns=['embedding'], errors='ignore')  # Drop 'embedding' if it exists\n",
    "        # Add new embedding columns to the metadata\n",
    "        for i in range(num_embedding_columns):\n",
    "            meta[f'{i+1}'] = float  # Adjust type as necessary, float is common for embeddings\n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "        # Apply the function in parallel using Dask\n",
    "        ddf = ddf.map_partitions(apply_get_embeddings_dask, meta=meta)\n",
    "    \n",
    "        # Compute the result with progress tracking\n",
    "        with ProgressBar():\n",
    "            processed_chunk = ddf.compute()\n",
    "    \n",
    "        # Append processed chunk to list\n",
    "        processed_chunks.append(processed_chunk)\n",
    "\n",
    "        \n",
    "    # Concatenate all processed chunks into a final DataFrame\n",
    "    final_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "\n",
    "    now = datetime.now()\n",
    "    formatted_time = now.strftime(\"%y-%m-%d-%H-%M-%S\")\n",
    "    # print(formatted_time)\n",
    "\n",
    "\n",
    "    final_df = final_df.drop(columns=['sequence'])\n",
    "    first_col = final_df.iloc[:, 0]  # Get the first column\n",
    "    final_df = final_df.drop(final_df.columns[0], axis=1)  # Drop the first column\n",
    "    final_df[first_col.name] = first_col  # Add it back as the last column\n",
    "\n",
    "    final_df.to_csv(typename+'_caduceus_'+pathogenecity_type+'_'+str(chunkid)+'_'+formatted_time+'.csv', index=False)\n",
    "\n",
    "    # final_df.to_csv(typename+'_train_'+str(chunkid)+'_with_embedding.csv', index=False)\n",
    "    print(f\"{typename}_caduceus_{pathogenecity_type}_{chunkid}_{formatted_time}.csv is created.\")\n",
    "\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea40ba3f-0f39-466d-be00-5525a35cd0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./pathogenecity_gpn_missense.csv')\n",
    "df.head()\n",
    "len(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4242495a-523a-4b60-8603-37b010112276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import shutil\n",
    "import urllib\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "from tqdm import tqdm\n",
    "from ast import literal_eval\n",
    "\n",
    "import re\n",
    "import datasets\n",
    "from datasets import Dataset, DatasetDict, Features, Value\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets import DatasetInfo\n",
    "from pyfaidx import Fasta\n",
    "from abc import ABC, abstractmethod\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97365412-2292-42f2-abeb-91c909d60ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"\n",
    "# ----------------------------------------------------------------------------------------\n",
    "# Reference Genome URLS:\n",
    "# ----------------------------------------------------------------------------------------\n",
    "# \"\"\"\n",
    "# H38_REFERENCE_GENOME_URL = (\n",
    "#     \"https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/\" \"hg38.fa.gz\"\n",
    "# )\n",
    "# H19_REFERENCE_GENOME_URL = (\n",
    "#     \"https://hgdownload.soe.ucsc.edu/goldenPath/hg19/bigZips/\" \"hg19.fa.gz\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd83c76e-6843-46b2-b26e-3956c0aff4e0",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "----------------------------------------------------------------------------------------\n",
    "Task Specific Handlers:\n",
    "----------------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "class GenomicLRATaskHandler(ABC):\n",
    "    \"\"\"\n",
    "    Abstract method for the Genomic LRA task handlers.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def __init__(self, **kwargs):\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_info(self, description: str) -> DatasetInfo:\n",
    "        \"\"\"\n",
    "        Returns the DatasetInfo for the task\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def split_generators(\n",
    "            self, dl_manager, cache_dir_root\n",
    "    ) -> List[datasets.SplitGenerator]:\n",
    "        \"\"\"\n",
    "        Downloads required files using dl_manager and separates them by split.\n",
    "        \"\"\"\n",
    "        return [\n",
    "            datasets.SplitGenerator(\n",
    "                name=datasets.Split.TRAIN,\n",
    "                gen_kwargs={\"handler\": self, \"split\": \"train\"},\n",
    "            ),\n",
    "            datasets.SplitGenerator(\n",
    "                name=datasets.Split.TEST, gen_kwargs={\"handler\": self, \"split\": \"test\"}\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "    @abstractmethod\n",
    "    def generate_examples(self, split):\n",
    "        \"\"\"\n",
    "        A generator that yields examples for the specified split.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def hook(t):\n",
    "        last_b = [0]\n",
    "\n",
    "        def inner(b=1, bsize=1, tsize=None):\n",
    "            \"\"\"\n",
    "            b  : int, optional\n",
    "                Number of blocks just transferred [default: 1].\n",
    "            bsize  : int, optional\n",
    "                Size of each block (in tqdm units) [default: 1].\n",
    "            tsize  : int, optional\n",
    "                Total size (in tqdm units). If [default: None] remains unchanged.\n",
    "            \"\"\"\n",
    "            if tsize is not None:\n",
    "                t.total = tsize\n",
    "            t.update((b - last_b[0]) * bsize)\n",
    "            last_b[0] = b\n",
    "\n",
    "        return inner\n",
    "\n",
    "    def download_and_extract_gz(self, file_url, cache_dir_root):\n",
    "        \"\"\"\n",
    "        Downloads and extracts a gz file into the given cache directory. Returns the\n",
    "        full file path of the extracted gz file.\n",
    "        Args:\n",
    "            file_url: url of the gz file to be downloaded and extracted.\n",
    "            cache_dir_root: Directory to extract file into.\n",
    "        \"\"\"\n",
    "        file_fname = Path(file_url).stem\n",
    "        file_complete_path = os.path.join(cache_dir_root, \"downloads\", file_fname)\n",
    "\n",
    "        if not os.path.exists(file_complete_path):\n",
    "            if not os.path.exists(file_complete_path + \".gz\"):\n",
    "                with tqdm(\n",
    "                        unit=\"B\",\n",
    "                        unit_scale=True,\n",
    "                        unit_divisor=1024,\n",
    "                        miniters=1,\n",
    "                        desc=file_url.split(\"/\")[-1],\n",
    "                ) as t:\n",
    "                    urllib.request.urlretrieve(\n",
    "                        file_url, file_complete_path + \".gz\", reporthook=self.hook(t)\n",
    "                    )\n",
    "            with gzip.open(file_complete_path + \".gz\", \"rb\") as file_in:\n",
    "                with open(file_complete_path, \"wb\") as file_out:\n",
    "                    shutil.copyfileobj(file_in, file_out)\n",
    "        return file_complete_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f245b50-4bb3-4c42-b92a-ac365cf80b46",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class CagePredictionHandler(GenomicLRATaskHandler):\n",
    "    \"\"\"\n",
    "    Handler for the CAGE prediction task.\n",
    "    \"\"\"\n",
    "\n",
    "    NUM_TRAIN = 33891\n",
    "    NUM_TEST = 1922\n",
    "    NUM_VALID = 2195\n",
    "    DEFAULT_LENGTH = 114688  # 896 x 128bp\n",
    "    TARGET_SHAPE = (\n",
    "        896,\n",
    "        50,\n",
    "    )  # 50 is a subset of CAGE tracks from the original enformer dataset\n",
    "    NPZ_SPLIT = 1000  # number of files per npz file.\n",
    "    NUM_BP_PER_BIN = 128  # number of base pairs per bin in labels\n",
    "\n",
    "    def __init__(self, sequence_length=DEFAULT_LENGTH, **kwargs):\n",
    "        \"\"\"\n",
    "        Creates a new handler for the CAGE task.\n",
    "        Args:\n",
    "            sequence_length: allows for increasing sequence context. Sequence length\n",
    "            must be an even multiple of 128 to align with binned labels. Note:\n",
    "            increasing sequence length may decrease the number of usable samples.\n",
    "        \"\"\"\n",
    "        self.reference_genome = None\n",
    "        self.coordinate_csv_file = None\n",
    "        self.target_files_by_split = {}\n",
    "\n",
    "\n",
    "        assert (sequence_length // 128) % 2 == 0, (\n",
    "            f\"Requested sequence length must be an even multuple of 128 to align \"\n",
    "            f\"with the binned labels.\"\n",
    "        )\n",
    "\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        if self.sequence_length < self.DEFAULT_LENGTH:\n",
    "            self.TARGET_SHAPE = (self.sequence_length // 128, 50)\n",
    "\n",
    "    def get_info(self, description: str) -> DatasetInfo:\n",
    "        \"\"\"\n",
    "        Returns the DatasetInfo for the CAGE dataset. Each example\n",
    "        includes a genomic sequence and a 2D array of labels\n",
    "        \"\"\"\n",
    "        features = datasets.Features(\n",
    "            {\n",
    "                # DNA sequence\n",
    "                \"sequence\": datasets.Value(\"string\"),\n",
    "                # array of sequence length x num_labels\n",
    "                \"labels\": datasets.Array2D(shape=self.TARGET_SHAPE, dtype=\"float32\"),\n",
    "                # chromosome number\n",
    "                \"chromosome\": datasets.Value(dtype=\"string\"),\n",
    "                # start\n",
    "                \"labels_start\": datasets.Value(dtype=\"int32\"),\n",
    "                # stop\n",
    "                \"labels_stop\": datasets.Value(dtype=\"int32\")\n",
    "            }\n",
    "        )\n",
    "        return datasets.DatasetInfo(\n",
    "            # This is the description that will appear on the datasets page.\n",
    "            description=description,\n",
    "            # This defines the different columns of the dataset and their types\n",
    "            features=features,\n",
    "        )\n",
    "\n",
    "    def split_generators(self, dl_manager, cache_dir_root):\n",
    "        \"\"\"\n",
    "        Separates files by split and stores filenames in instance variables.\n",
    "        The CAGE dataset requires reference genome, coordinate\n",
    "        csv file,and npy files to be saved.\n",
    "        \"\"\"\n",
    "\n",
    "        # Manually download the reference genome since there are difficulties when\n",
    "        # streaming\n",
    "        reference_genome_file = self.download_and_extract_gz(\n",
    "            H38_REFERENCE_GENOME_URL, cache_dir_root\n",
    "        )\n",
    "        self.reference_genome = Fasta(reference_genome_file, one_based_attributes=False)\n",
    "\n",
    "        self.coordinate_csv_file = dl_manager.download_and_extract(\n",
    "            \"cage_prediction/sequences_coordinates.csv\"\n",
    "        )\n",
    "\n",
    "        train_file_dict = {}\n",
    "        for train_key, train_file in self.generate_npz_filenames(\n",
    "                \"train\", self.NUM_TRAIN, folder=\"cage_prediction/targets_subset\"\n",
    "        ):\n",
    "            train_file_dict[train_key] = dl_manager.download(train_file)\n",
    "\n",
    "        test_file_dict = {}\n",
    "        for test_key, test_file in self.generate_npz_filenames(\n",
    "                \"test\", self.NUM_TEST, folder=\"cage_prediction/targets_subset\"\n",
    "        ):\n",
    "            test_file_dict[test_key] = dl_manager.download(test_file)\n",
    "\n",
    "        valid_file_dict = {}\n",
    "        for valid_key, valid_file in self.generate_npz_filenames(\n",
    "                \"valid\", self.NUM_VALID, folder=\"cage_prediction/targets_subset\"\n",
    "        ):\n",
    "            valid_file_dict[valid_key] = dl_manager.download(valid_file)\n",
    "\n",
    "        # convert file list to a dict keyed by target number\n",
    "        self.target_files_by_split[\"train\"] = train_file_dict\n",
    "        self.target_files_by_split[\"test\"] = test_file_dict\n",
    "        self.target_files_by_split[\"validation\"] = valid_file_dict\n",
    "\n",
    "        return [\n",
    "            datasets.SplitGenerator(\n",
    "                name=datasets.Split.TRAIN,\n",
    "                gen_kwargs={\"handler\": self, \"split\": \"train\"},\n",
    "            ),\n",
    "            datasets.SplitGenerator(\n",
    "                name=datasets.Split.VALIDATION,\n",
    "                gen_kwargs={\"handler\": self, \"split\": \"validation\"},\n",
    "            ),\n",
    "            datasets.SplitGenerator(\n",
    "                name=datasets.Split.TEST,\n",
    "                gen_kwargs={\"handler\": self, \"split\": \"test\"}\n",
    "            ),\n",
    "        ]\n",
    "\n",
    "    def generate_examples(self, split):\n",
    "        \"\"\"\n",
    "        A generator which produces examples for the given split, each with a sequence\n",
    "        and the corresponding labels. The sequences are padded to the correct\n",
    "        sequence length and standardized before returning.\n",
    "        \"\"\"\n",
    "\n",
    "        target_files = self.target_files_by_split[split]\n",
    "\n",
    "        key = 0\n",
    "        coordinates_dataframe = pd.read_csv(self.coordinate_csv_file)\n",
    "        filtered = coordinates_dataframe[coordinates_dataframe[\"split\"] == split]\n",
    "        for sequential_idx, row in filtered.iterrows():\n",
    "            start, stop = int(row[\"start\"]) - 1, int(\n",
    "                row[\"stop\"]) - 1  # -1 since coords are 1-based\n",
    "\n",
    "            chromosome = row['chrom']\n",
    "\n",
    "            padded_sequence,new_start,new_stop = pad_sequence(\n",
    "                chromosome=self.reference_genome[chromosome],\n",
    "                start=start,\n",
    "                sequence_length=self.sequence_length,\n",
    "                end=stop,\n",
    "                return_new_start_stop=True\n",
    "            )\n",
    "\n",
    "            if self.sequence_length >= self.DEFAULT_LENGTH:\n",
    "                new_start = start\n",
    "                new_stop = stop\n",
    "\n",
    "            # floor npy_idx to the nearest 1000\n",
    "            npz_file = np.load(\n",
    "                target_files[int((row[\"npy_idx\"] // self.NPZ_SPLIT) * self.NPZ_SPLIT)]\n",
    "            )\n",
    "\n",
    "            if (\n",
    "                    split == \"validation\"\n",
    "            ):  # npy files are keyed by [\"train\", \"test\", \"valid\"]\n",
    "                split = \"valid\"\n",
    "            targets = npz_file[f\"target-{split}-{row['npy_idx']}.npy\"][\n",
    "                0]  # select 0 since there is extra dimension\n",
    "\n",
    "            # subset the targets if sequence length is smaller than 114688 (\n",
    "            # DEFAULT_LENGTH)\n",
    "            if self.sequence_length < self.DEFAULT_LENGTH:\n",
    "                idx_diff = (self.DEFAULT_LENGTH - self.sequence_length) // 2 // 128\n",
    "                targets = targets[idx_diff:-idx_diff]\n",
    "\n",
    "            if padded_sequence:\n",
    "                yield key, {\n",
    "                    \"labels\": targets,\n",
    "                    \"sequence\": standardize_sequence(padded_sequence),\n",
    "                    \"chromosome\": re.sub(\"chr\", \"\", chromosome),\n",
    "                    \"labels_start\": new_start,\n",
    "                    \"labels_stop\": new_stop\n",
    "                }\n",
    "                key += 1\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_npz_filenames(split, total, folder, npz_size=NPZ_SPLIT):\n",
    "        \"\"\"\n",
    "        Generates a list of filenames for the npz files stored in the dataset.\n",
    "        Yields a tuple of floored multiple of 1000, filename\n",
    "        Args:\n",
    "            split: split to generate filenames for. Must be in ['train', 'test', 'valid']\n",
    "                due to the naming of the files.\n",
    "            total: total number of npy targets for given split\n",
    "            folder: folder where data is stored.\n",
    "            npz_size: number of npy files per npz. Defaults to 1000 because\n",
    "                this is the number currently used in the dataset.\n",
    "        \"\"\"\n",
    "\n",
    "        for i in range(total // npz_size):\n",
    "            yield i * npz_size, f\"{folder}/targets-{split}-{i * npz_size}-{i * npz_size + (npz_size - 1)}.npz\"\n",
    "        if total % npz_size != 0:\n",
    "            yield (\n",
    "                npz_size * (total // npz_size),\n",
    "                f\"{folder}/targets-{split}-\"\n",
    "                f\"{npz_size * (total // npz_size)}-\"\n",
    "                f\"{npz_size * (total // npz_size) + (total % npz_size - 1)}.npz\",\n",
    "            )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb25b278-3276-4c27-a531-c9f0baf78e93",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class BulkRnaExpressionHandler(GenomicLRATaskHandler):\n",
    "    \"\"\"\n",
    "    Handler for the Bulk RNA Expression task.\n",
    "    \"\"\"\n",
    "\n",
    "    DEFAULT_LENGTH = 100000\n",
    "\n",
    "    def __init__(self, sequence_length=DEFAULT_LENGTH, **kwargs):\n",
    "        \"\"\"\n",
    "        Creates a new handler for the Bulk RNA Expression Prediction Task.\n",
    "        Args:\n",
    "            sequence_length: Length of the sequence around the TSS_CAGE start site\n",
    "\n",
    "        \"\"\"\n",
    "        self.reference_genome = None\n",
    "        self.coordinate_csv_file = None\n",
    "        self.labels_csv_file = None\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def get_info(self, description: str) -> DatasetInfo:\n",
    "        \"\"\"\n",
    "        Returns the DatasetInfo for the Bulk RNA Expression dataset. Each example\n",
    "        includes a genomic sequence and a list of label values.\n",
    "        \"\"\"\n",
    "        features = datasets.Features(\n",
    "            {\n",
    "                # DNA sequence\n",
    "                \"sequence\": datasets.Value(\"string\"),\n",
    "                # list of expression values in each tissue\n",
    "                \"labels\": datasets.Sequence(datasets.Value(\"float32\")),\n",
    "                # chromosome number\n",
    "                \"chromosome\": datasets.Value(dtype=\"string\"),\n",
    "                # position\n",
    "                \"position\": datasets.Value(dtype=\"int32\"),\n",
    "            }\n",
    "        )\n",
    "        return datasets.DatasetInfo(\n",
    "            # This is the description that will appear on the datasets page.\n",
    "            description=description,\n",
    "            # This defines the different columns of the dataset and their types\n",
    "            features=features,\n",
    "\n",
    "        )\n",
    "\n",
    "    def split_generators(self, dl_manager, cache_dir_root):\n",
    "        \"\"\"\n",
    "        Separates files by split and stores filenames in instance variables.\n",
    "        The Bulk RNA Expression dataset requires the reference hg19 genome, coordinate\n",
    "        csv file,and label csv file to be saved.\n",
    "        \"\"\"\n",
    "\n",
    "        reference_genome_file = self.download_and_extract_gz(\n",
    "            H19_REFERENCE_GENOME_URL, cache_dir_root\n",
    "        )\n",
    "        self.reference_genome = Fasta(reference_genome_file, one_based_attributes=False)\n",
    "\n",
    "        self.coordinate_csv_file = dl_manager.download_and_extract(\n",
    "            \"bulk_rna_expression/gene_coordinates.csv\"\n",
    "        )\n",
    "\n",
    "        self.labels_csv_file = dl_manager.download_and_extract(\n",
    "            \"bulk_rna_expression/rna_expression_values.csv\"\n",
    "        )\n",
    "\n",
    "        return super().split_generators(dl_manager, cache_dir_root)\n",
    "\n",
    "    def generate_examples(self, split):\n",
    "        \"\"\"\n",
    "        A generator which produces examples for the given split, each with a sequence\n",
    "        and the corresponding labels. The sequences are padded to the correct sequence\n",
    "        length and standardized before returning.\n",
    "        \"\"\"\n",
    "        coordinates_df = pd.read_csv(self.coordinate_csv_file)\n",
    "        labels_df = pd.read_csv(self.labels_csv_file)\n",
    "\n",
    "        coordinates_split_df = coordinates_df[coordinates_df[\"split\"] == split]\n",
    "\n",
    "        key = 0\n",
    "        for idx, coordinates_row in coordinates_split_df.iterrows():\n",
    "            start = coordinates_row[\n",
    "                        \"CAGE_representative_TSS\"] - 1  # -1 since coords are 1-based\n",
    "\n",
    "            chromosome = coordinates_row[\"chrom\"]\n",
    "            labels_row = labels_df.loc[idx].values\n",
    "            padded_sequence = pad_sequence(\n",
    "                chromosome=self.reference_genome[chromosome],\n",
    "                start=start,\n",
    "                sequence_length=self.sequence_length,\n",
    "                negative_strand=coordinates_row[\"strand\"] == \"-\",\n",
    "            )\n",
    "            if padded_sequence:\n",
    "                yield key, {\n",
    "                    \"labels\": labels_row,\n",
    "                    \"sequence\": standardize_sequence(padded_sequence),\n",
    "                    \"chromosome\": re.sub(\"chr\", \"\", chromosome),\n",
    "                    \"position\": coordinates_row[\"CAGE_representative_TSS\"]\n",
    "                }\n",
    "                key += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f4178315-da20-4856-8172-d5a362a06b44",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class VariantEffectCausalEqtl(GenomicLRATaskHandler):\n",
    "    \"\"\"\n",
    "    Handler for the Variant Effect Causal eQTL task.\n",
    "    \"\"\"\n",
    "\n",
    "    DEFAULT_LENGTH = 100000\n",
    "\n",
    "    def __init__(self, sequence_length=DEFAULT_LENGTH, **kwargs):\n",
    "        \"\"\"\n",
    "        Creates a new handler for the Variant Effect Causal eQTL Task.\n",
    "        Args:\n",
    "            sequence_length: Length of the sequence to pad around the SNP position\n",
    "\n",
    "        \"\"\"\n",
    "        self.reference_genome = None\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "    def get_info(self, description: str) -> DatasetInfo:\n",
    "        \"\"\"\n",
    "        Returns the DatasetInfo for the Variant Effect Causal eQTL dataset. Each example\n",
    "        includes a  genomic sequence with the reference allele as well as the genomic\n",
    "        sequence with the alternative allele, and a binary label.\n",
    "        \"\"\"\n",
    "        features = datasets.Features(\n",
    "            {\n",
    "                # DNA sequence\n",
    "                \"ref_forward_sequence\": datasets.Value(\"string\"),\n",
    "                \"alt_forward_sequence\": datasets.Value(\"string\"),\n",
    "                # binary label\n",
    "                \"label\": datasets.Value(dtype=\"int8\"),\n",
    "                # tissue type\n",
    "                \"tissue\": datasets.Value(dtype=\"string\"),\n",
    "                # chromosome number\n",
    "                \"chromosome\": datasets.Value(dtype=\"string\"),\n",
    "                # variant position\n",
    "                \"position\": datasets.Value(dtype=\"int32\"),\n",
    "                # distance to nearest tss\n",
    "                \"distance_to_nearest_tss\": datasets.Value(dtype=\"int32\")\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return datasets.DatasetInfo(\n",
    "            # This is the description that will appear on the datasets page.\n",
    "            description=description,\n",
    "            # This defines the different columns of the dataset and their types\n",
    "            features=features,\n",
    "        )\n",
    "\n",
    "    def split_generators(self, dl_manager, cache_dir_root):\n",
    "        \"\"\"\n",
    "        Separates files by split and stores filenames in instance variables.\n",
    "        The variant effect prediction dataset requires the reference hg38 genome and\n",
    "        coordinates_labels_csv_file to be saved.\n",
    "        \"\"\"\n",
    "\n",
    "        # Manually download the reference genome since there are difficulties\n",
    "        # when streaming\n",
    "        reference_genome_file = self.download_and_extract_gz(\n",
    "            H38_REFERENCE_GENOME_URL, cache_dir_root\n",
    "        )\n",
    "\n",
    "        self.reference_genome = Fasta(reference_genome_file, one_based_attributes=False)\n",
    "        self.coordinates_labels_csv_file = dl_manager.download_and_extract(\n",
    "            f\"variant_effect_causal_eqtl/All_Tissues.csv\"\n",
    "        )\n",
    "\n",
    "        return super().split_generators(dl_manager, cache_dir_root)\n",
    "\n",
    "    def generate_examples(self, split):\n",
    "        \"\"\"\n",
    "        A generator which produces examples each with ref/alt allele\n",
    "        and corresponding binary label. The sequences are extended to\n",
    "        the desired sequence length and standardized before returning.\n",
    "        \"\"\"\n",
    "\n",
    "        coordinates_df = pd.read_csv(self.coordinates_labels_csv_file)\n",
    "\n",
    "        coordinates_split_df = coordinates_df[coordinates_df[\"split\"] == split]\n",
    "\n",
    "        key = 0\n",
    "        for idx, row in coordinates_split_df.iterrows():\n",
    "            start = row[\"POS\"] - 1  # sub 1 to create idx since coords are 1-based\n",
    "            alt_allele = row[\"ALT\"]\n",
    "            label = row[\"label\"]\n",
    "            tissue = row['tissue']\n",
    "            chromosome = row[\"CHROM\"]\n",
    "            distance = int(row[\"distance_to_nearest_TSS\"])\n",
    "\n",
    "            # get reference forward sequence\n",
    "            ref_forward = pad_sequence(\n",
    "                chromosome=self.reference_genome[chromosome],\n",
    "                start=start,\n",
    "                sequence_length=self.sequence_length,\n",
    "                negative_strand=False,\n",
    "            )\n",
    "\n",
    "            # only if a valid sequence returned\n",
    "            if ref_forward:\n",
    "                # Mutate sequence with the alt allele at the SNP position,\n",
    "                # which is always centered in the string returned from pad_sequence\n",
    "                alt_forward = list(ref_forward)\n",
    "                alt_forward[self.sequence_length // 2] = alt_allele\n",
    "                alt_forward = \"\".join(alt_forward)\n",
    "\n",
    "                yield key, {\n",
    "                    \"label\": label,\n",
    "                    \"tissue\": tissue,\n",
    "                    \"chromosome\": re.sub(\"chr\", \"\", chromosome),\n",
    "                    \"ref_forward_sequence\": standardize_sequence(ref_forward),\n",
    "                    \"alt_forward_sequence\": standardize_sequence(alt_forward),\n",
    "                    \"distance_to_nearest_tss\": distance,\n",
    "                    \"position\": row[\"POS\"]\n",
    "                }\n",
    "                key += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc17aff7-829f-47c2-9ffe-9ee6f5dfeefe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class VariantEffectPathogenicHandler(GenomicLRATaskHandler):\n",
    "    \"\"\"\n",
    "    Handler for the Variant Effect Pathogenic Prediction tasks.\n",
    "    \"\"\"\n",
    "\n",
    "    DEFAULT_LENGTH = 100000\n",
    "\n",
    "    def __init__(self, sequence_length=DEFAULT_LENGTH, task_name=None, subset=False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Creates a new handler for the Variant Effect Pathogenic Tasks.\n",
    "        Args:\n",
    "            sequence_length: Length of the sequence to pad around the SNP position\n",
    "            subset: Whether to return a pre-determined subset of the data.\n",
    "\n",
    "        \"\"\"\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        if task_name == 'variant_effect_pathogenic_clinvar':\n",
    "            self.data_file_name = \"variant_effect_pathogenic/vep_pathogenic_coding.csv\"\n",
    "        elif task_name == 'variant_effect_pathogenic_omim':\n",
    "            self.data_file_name = \"variant_effect_pathogenic/\" \\\n",
    "                                  \"vep_pathogenic_non_coding_subset.csv\" \\\n",
    "                if subset else \"variant_effect_pathogenic/vep_pathogenic_non_coding.csv\"\n",
    "\n",
    "    def get_info(self, description: str) -> DatasetInfo:\n",
    "        \"\"\"\n",
    "        Returns the DatasetInfo for the Variant Effect Pathogenic datasets. Each example\n",
    "        includes a  genomic sequence with the reference allele as well as the genomic\n",
    "        sequence with the alternative allele, and a binary label.\n",
    "        \"\"\"\n",
    "        features = datasets.Features(\n",
    "            {\n",
    "                # DNA sequence\n",
    "                \"ref_forward_sequence\": datasets.Value(\"string\"),\n",
    "                \"alt_forward_sequence\": datasets.Value(\"string\"),\n",
    "                # binary label\n",
    "                \"label\": datasets.Value(dtype=\"int8\"),\n",
    "                # chromosome number\n",
    "                \"chromosome\": datasets.Value(dtype=\"string\"),\n",
    "                # position\n",
    "                \"position\": datasets.Value(dtype=\"int32\")\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return datasets.DatasetInfo(\n",
    "            # This is the description that will appear on the datasets page.\n",
    "            description=description,\n",
    "            # This defines the different columns of the dataset and their types\n",
    "            features=features,\n",
    "        )\n",
    "\n",
    "    def split_generators(self, dl_manager, cache_dir_root):\n",
    "        \"\"\"\n",
    "        Separates files by split and stores filenames in instance variables.\n",
    "        The variant effect prediction datasets require the reference hg38 genome and\n",
    "        coordinates_labels_csv_file to be saved.\n",
    "        \"\"\"\n",
    "\n",
    "        reference_genome_file = self.download_and_extract_gz(\n",
    "            H38_REFERENCE_GENOME_URL, cache_dir_root\n",
    "        )\n",
    "\n",
    "        self.reference_genome = Fasta(reference_genome_file, one_based_attributes=False)\n",
    "        self.coordinates_labels_csv_file = dl_manager.download_and_extract(\n",
    "            self.data_file_name)\n",
    "\n",
    "        if 'non_coding' in self.data_file_name:\n",
    "            return [\n",
    "                datasets.SplitGenerator(\n",
    "                    name=datasets.Split.TEST,\n",
    "                    gen_kwargs={\"handler\": self, \"split\": \"test\"}\n",
    "                ), ]\n",
    "        else:\n",
    "            return super().split_generators(dl_manager, cache_dir_root)\n",
    "\n",
    "    def generate_examples(self, split):\n",
    "        \"\"\"\n",
    "        A generator which produces examples each with ref/alt allele\n",
    "        and corresponding binary label. The sequences are extended to\n",
    "        the desired sequence length and standardized before returning.\n",
    "        \"\"\"\n",
    "\n",
    "        coordinates_df = pd.read_csv(self.coordinates_labels_csv_file)\n",
    "        coordinates_split_df = coordinates_df[coordinates_df[\"split\"] == split]\n",
    "\n",
    "        key = 0\n",
    "        for idx, row in coordinates_split_df.iterrows():\n",
    "            start = row[\"POS\"] - 1  # sub 1 to create idx since coords are 1-based\n",
    "            alt_allele = row[\"ALT\"]\n",
    "            label = row[\"INT_LABEL\"]\n",
    "            chromosome = row[\"CHROM\"]\n",
    "\n",
    "            # get reference forward sequence\n",
    "            ref_forward = pad_sequence(\n",
    "                chromosome=self.reference_genome[chromosome],\n",
    "                start=start,\n",
    "                sequence_length=self.sequence_length,\n",
    "                negative_strand=False,\n",
    "            )\n",
    "\n",
    "            # only if a valid sequence returned\n",
    "            if ref_forward:\n",
    "                # Mutate sequence with the alt allele at the SNP position,\n",
    "                # which is always centered in the string returned from pad_sequence\n",
    "                alt_forward = list(ref_forward)\n",
    "                alt_forward[self.sequence_length // 2] = alt_allele\n",
    "                alt_forward = \"\".join(alt_forward)\n",
    "\n",
    "                yield key, {\n",
    "                    \"label\": label,\n",
    "                    \"chromosome\": re.sub(\"chr\", \"\", chromosome),\n",
    "                    \"ref_forward_sequence\": standardize_sequence(ref_forward),\n",
    "                    \"alt_forward_sequence\": standardize_sequence(alt_forward),\n",
    "                    \"position\": row['POS']\n",
    "                }\n",
    "                key += 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a8655486-284d-4d03-a1f4-b1d0cc38246b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class ChromatinFeaturesHandler(GenomicLRATaskHandler):\n",
    "    \"\"\"\n",
    "    Handler for the histone marks and DNA accessibility tasks also referred to\n",
    "    collectively as Chromatin features.\n",
    "    \"\"\"\n",
    "\n",
    "    DEFAULT_LENGTH = 100000\n",
    "\n",
    "    def __init__(self, task_name=None, sequence_length=DEFAULT_LENGTH, subset=False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Creates a new handler for the Deep Sea Histone and DNase tasks.\n",
    "        Args:\n",
    "            sequence_length: Length of the sequence around and including the\n",
    "            annotated 200bp bin\n",
    "            subset: Whether to return a pre-determined subset of the entire dataset.\n",
    "\n",
    "        \"\"\"\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        if sequence_length < 200:\n",
    "            raise ValueError(\n",
    "                'Sequence length for this task must be greater or equal to 200 bp')\n",
    "\n",
    "        if 'histone' in task_name:\n",
    "            self.label_name = 'HISTONES'\n",
    "        elif 'dna' in task_name:\n",
    "            self.label_name = 'DNASE'\n",
    "\n",
    "        self.data_file_name = \"chromatin_features/histones_and_dnase_subset.csv\" if \\\n",
    "            subset else \"chromatin_features/histones_and_dnase.csv\"\n",
    "\n",
    "    def get_info(self, description: str) -> DatasetInfo:\n",
    "        \"\"\"\n",
    "        Returns the DatasetInfo for the histone marks and dna accessibility datasets.\n",
    "        Each example includes a genomic sequence and a list of label values.\n",
    "        \"\"\"\n",
    "        features = datasets.Features(\n",
    "            {\n",
    "                # DNA sequence\n",
    "                \"sequence\": datasets.Value(\"string\"),\n",
    "                # list of binary chromatin marks\n",
    "                \"labels\": datasets.Sequence(datasets.Value(\"int8\")),\n",
    "                # chromosome number\n",
    "                \"chromosome\": datasets.Value(dtype=\"string\"),\n",
    "                # starting position in genome which corresponds to label\n",
    "                \"label_start\": datasets.Value(dtype=\"int32\"),\n",
    "                # end position in genome which corresponds to label\n",
    "                \"label_stop\": datasets.Value(dtype=\"int32\"),\n",
    "            }\n",
    "        )\n",
    "        return datasets.DatasetInfo(\n",
    "            # This is the description that will appear on the datasets page.\n",
    "            description=description,\n",
    "            # This defines the different columns of the dataset and their types\n",
    "            features=features,\n",
    "\n",
    "        )\n",
    "\n",
    "    def split_generators(self, dl_manager, cache_dir_root):\n",
    "        \"\"\"\n",
    "        Separates files by split and stores filenames in instance variables.\n",
    "        The histone marks and dna accessibility datasets require the reference hg19\n",
    "        genome and coordinate csv file to be saved.\n",
    "        \"\"\"\n",
    "        reference_genome_file = self.download_and_extract_gz(\n",
    "            H19_REFERENCE_GENOME_URL, cache_dir_root\n",
    "        )\n",
    "        self.reference_genome = Fasta(reference_genome_file, one_based_attributes=False)\n",
    "\n",
    "        self.coordinate_csv_file = dl_manager.download_and_extract(self.data_file_name)\n",
    "\n",
    "        return super().split_generators(dl_manager, cache_dir_root)\n",
    "\n",
    "    def generate_examples(self, split):\n",
    "        \"\"\"\n",
    "        A generator which produces examples for the given split, each with a sequence\n",
    "        and the corresponding labels. The sequences are padded to the correct sequence\n",
    "        length and standardized before returning.\n",
    "        \"\"\"\n",
    "        coordinates_df = pd.read_csv(self.coordinate_csv_file)\n",
    "        coordinates_split_df = coordinates_df[coordinates_df[\"split\"] == split]\n",
    "\n",
    "        key = 0\n",
    "        for idx, coordinates_row in coordinates_split_df.iterrows():\n",
    "            start = coordinates_row['POS'] - 1  # -1 since saved coords are 1-based\n",
    "            chromosome = coordinates_row[\"CHROM\"]\n",
    "\n",
    "            # literal eval used since lists are saved as strings in csv\n",
    "            labels_row = literal_eval(coordinates_row[self.label_name])\n",
    "\n",
    "            padded_sequence = pad_sequence(\n",
    "                chromosome=self.reference_genome[chromosome],\n",
    "                start=start,\n",
    "                sequence_length=self.sequence_length,\n",
    "            )\n",
    "            if padded_sequence:\n",
    "                yield key, {\n",
    "                    \"labels\": labels_row,\n",
    "                    \"sequence\": standardize_sequence(padded_sequence),\n",
    "                    \"chromosome\": re.sub(\"chr\", \"\", chromosome),\n",
    "                    \"label_start\": coordinates_row['POS']-100,\n",
    "                    \"label_stop\": coordinates_row['POS'] + 99,\n",
    "                }\n",
    "                key += 1\n",
    "\n",
    "\n",
    "class RegulatoryElementHandler(GenomicLRATaskHandler):\n",
    "    \"\"\"\n",
    "    Handler for the Regulatory Element Prediction tasks.\n",
    "    \"\"\"\n",
    "    DEFAULT_LENGTH = 100000\n",
    "\n",
    "    def __init__(self, task_name=None, sequence_length=DEFAULT_LENGTH, subset=False,\n",
    "                 **kwargs):\n",
    "        \"\"\"\n",
    "        Creates a new handler for the Regulatory Element Prediction tasks.\n",
    "        Args:\n",
    "            sequence_length: Length of the sequence around the element/non-element\n",
    "            subset: Whether to return a pre-determined subset of the entire dataset.\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        if sequence_length < 200:\n",
    "            raise ValueError(\n",
    "                'Sequence length for this task must be greater or equal to 200 bp')\n",
    "\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        if 'promoter' in task_name:\n",
    "            self.data_file_name = 'regulatory_elements/promoter_dataset'\n",
    "\n",
    "        elif 'enhancer' in task_name:\n",
    "            self.data_file_name = 'regulatory_elements/enhancer_dataset'\n",
    "\n",
    "        if subset:\n",
    "            self.data_file_name += '_subset.csv'\n",
    "        else:\n",
    "            self.data_file_name += '.csv'\n",
    "\n",
    "    def get_info(self, description: str) -> DatasetInfo:\n",
    "        \"\"\"\n",
    "        Returns the DatasetInfo for the Regulatory Element Prediction Tasks.\n",
    "        Each example includes a genomic sequence and a label.\n",
    "        \"\"\"\n",
    "        features = datasets.Features(\n",
    "            {\n",
    "                # DNA sequence\n",
    "                \"sequence\": datasets.Value(\"string\"),\n",
    "                # label corresponding to whether the sequence has\n",
    "                # the regulatory element of interest or not\n",
    "                \"labels\": datasets.Value(\"int8\"),\n",
    "                # chromosome number\n",
    "                \"chromosome\": datasets.Value(dtype=\"string\"),\n",
    "                # start\n",
    "                \"label_start\": datasets.Value(dtype=\"int32\"),\n",
    "                # stop\n",
    "                \"label_stop\": datasets.Value(dtype=\"int32\"),\n",
    "            }\n",
    "        )\n",
    "        return datasets.DatasetInfo(\n",
    "            # This is the description that will appear on the datasets page.\n",
    "            description=description,\n",
    "            # This defines the different columns of the dataset and their types\n",
    "            features=features,\n",
    "\n",
    "        )\n",
    "\n",
    "    def split_generators(self, dl_manager, cache_dir_root):\n",
    "        \"\"\"\n",
    "        Separates files by split and stores filenames in instance variables.\n",
    "        \"\"\"\n",
    "        reference_genome_file = self.download_and_extract_gz(\n",
    "            H38_REFERENCE_GENOME_URL, cache_dir_root\n",
    "        )\n",
    "        self.reference_genome = Fasta(reference_genome_file, one_based_attributes=False)\n",
    "\n",
    "        self.coordinate_csv_file = dl_manager.download_and_extract(\n",
    "            self.data_file_name\n",
    "        )\n",
    "\n",
    "        return super().split_generators(dl_manager, cache_dir_root)\n",
    "\n",
    "    def generate_examples(self, split):\n",
    "        \"\"\"\n",
    "        A generator which produces examples for the given split, each with a sequence\n",
    "        and the corresponding label. The sequences are padded to the correct sequence\n",
    "        length and standardized before returning.\n",
    "        \"\"\"\n",
    "        coordinates_df = pd.read_csv(self.coordinate_csv_file)\n",
    "\n",
    "        coordinates_split_df = coordinates_df[coordinates_df[\"split\"] == split]\n",
    "\n",
    "        key = 0\n",
    "        for _, coordinates_row in coordinates_split_df.iterrows():\n",
    "            start = coordinates_row[\"START\"] - 1  # -1 since vcf coords are 1-based\n",
    "            end = coordinates_row[\"STOP\"] - 1  # -1 since vcf coords are 1-based\n",
    "            chromosome = coordinates_row[\"CHROM\"]\n",
    "\n",
    "            label = coordinates_row['label']\n",
    "\n",
    "            padded_sequence = pad_sequence(\n",
    "                chromosome=self.reference_genome[chromosome],\n",
    "                start=start,\n",
    "                end=end,\n",
    "                sequence_length=self.sequence_length,\n",
    "            )\n",
    "\n",
    "            if padded_sequence:\n",
    "                yield key, {\n",
    "                    \"labels\": label,\n",
    "                    \"sequence\": standardize_sequence(padded_sequence),\n",
    "                    \"chromosome\": re.sub(\"chr\", \"\", chromosome),\n",
    "                    \"label_start\": coordinates_row[\"START\"],\n",
    "                    \"label_stop\": coordinates_row[\"STOP\"]\n",
    "                }\n",
    "                key += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c7bd867-919e-44c1-9148-35095632c14e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "----------------------------------------------------------------------------------------\n",
    "Dataset loader:\n",
    "----------------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "_DESCRIPTION = \"\"\"\n",
    "Dataset for benchmark of genomic deep learning models. \n",
    "\"\"\"\n",
    "\n",
    "_TASK_HANDLERS = {\n",
    "    \"cage_prediction\": CagePredictionHandler,\n",
    "    \"bulk_rna_expression\": BulkRnaExpressionHandler,\n",
    "    \"variant_effect_causal_eqtl\": VariantEffectCausalEqtl,\n",
    "    \"variant_effect_pathogenic_clinvar\": VariantEffectPathogenicHandler,\n",
    "    \"variant_effect_pathogenic_omim\": VariantEffectPathogenicHandler,\n",
    "    \"chromatin_features_histone_marks\": ChromatinFeaturesHandler,\n",
    "    \"chromatin_features_dna_accessibility\": ChromatinFeaturesHandler,\n",
    "    \"regulatory_element_promoter\": RegulatoryElementHandler,\n",
    "    \"regulatory_element_enhancer\": RegulatoryElementHandler,\n",
    "}\n",
    "\n",
    "\n",
    "# define dataset configs\n",
    "class GenomicsLRAConfig(datasets.BuilderConfig):\n",
    "    \"\"\"\n",
    "    BuilderConfig.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, task_name: str, **kwargs):  # type: ignore\n",
    "        \"\"\"BuilderConfig for the location tasks dataset.\n",
    "        Args:\n",
    "            **kwargs: keyword arguments forwarded to super.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.handler = _TASK_HANDLERS[task_name](task_name=task_name, **kwargs)\n",
    "\n",
    "\n",
    "# DatasetBuilder\n",
    "class GenomicsLRATasks(datasets.GeneratorBasedBuilder):\n",
    "    \"\"\"\n",
    "    Tasks to annotate human genome.\n",
    "    \"\"\"\n",
    "\n",
    "    VERSION = datasets.Version(\"1.1.0\")\n",
    "    BUILDER_CONFIG_CLASS = GenomicsLRAConfig\n",
    "\n",
    "    def _info(self) -> DatasetInfo:\n",
    "        return self.config.handler.get_info(description=_DESCRIPTION)\n",
    "\n",
    "    def _split_generators(\n",
    "            self, dl_manager: datasets.DownloadManager\n",
    "    ) -> List[datasets.SplitGenerator]:\n",
    "        \"\"\"\n",
    "        Downloads data files and organizes it into train/test/val splits\n",
    "        \"\"\"\n",
    "        return self.config.handler.split_generators(dl_manager, self._cache_dir_root)\n",
    "\n",
    "    def _generate_examples(self, handler, split):\n",
    "        \"\"\"\n",
    "        Read data files and create examples(yield)\n",
    "        Args:\n",
    "            handler: The handler for the current task\n",
    "            split: A string in ['train', 'test', 'valid']\n",
    "        \"\"\"\n",
    "        yield from handler.generate_examples(split)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae2559b5-90dd-4c7d-8a1b-b9c5d3542a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "----------------------------------------------------------------------------------------\n",
    "Global Utils:\n",
    "----------------------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def standardize_sequence(sequence: str):\n",
    "    \"\"\"\n",
    "    Standardizes the sequence by replacing all unknown characters with N and\n",
    "    converting to all uppercase.\n",
    "    Args:\n",
    "        sequence: genomic sequence to standardize\n",
    "    \"\"\"\n",
    "    pattern = \"[^ATCG]\"\n",
    "    # all characters to upper case\n",
    "    sequence = sequence.upper()\n",
    "    # replace all characters that are not A,T,C,G with N\n",
    "    sequence = re.sub(pattern, \"N\", sequence)\n",
    "    return sequence\n",
    "\n",
    "\n",
    "def pad_sequence(chromosome, start, sequence_length, end=None, negative_strand=False,\n",
    "                 return_new_start_stop=False):\n",
    "    \"\"\"\n",
    "    Extends a given sequence to length sequence_length. If\n",
    "    padding to the given length is outside the gene, returns\n",
    "    None.\n",
    "    Args:\n",
    "        chromosome: Chromosome from pyfaidx extracted Fasta.\n",
    "        start: Start index of original sequence.\n",
    "        sequence_length: Desired sequence length. If sequence length is odd, the\n",
    "            remainder is added to the end of the sequence.\n",
    "        end: End index of original sequence. If no end is specified, it creates a\n",
    "            centered sequence around the start index.\n",
    "        negative_strand: If negative_strand, returns the reverse compliment of the\n",
    "        sequence\n",
    "    \"\"\"\n",
    "    if end:\n",
    "        pad = (sequence_length - (end - start)) // 2\n",
    "        start = start - pad\n",
    "        end = end + pad + (sequence_length % 2)\n",
    "    else:\n",
    "        pad = sequence_length // 2\n",
    "        end = start + pad + (sequence_length % 2)\n",
    "        start = start - pad\n",
    "\n",
    "    if start < 0 or end >= len(chromosome):\n",
    "        return\n",
    "    if negative_strand:\n",
    "        if return_new_start_stop:\n",
    "            return chromosome[start:end].reverse.complement.seq ,start, end\n",
    "\n",
    "        return chromosome[start:end].reverse.complement.seq\n",
    "\n",
    "    if return_new_start_stop:\n",
    "        return chromosome[start:end].seq , start, end\n",
    "\n",
    "    return chromosome[start:end].seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f4804a-8c86-4639-841b-0adfe378e3a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "16d236f9-dc60-4f7b-8bdd-7c943fa4a93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def download_and_extract_gz(file_url, cache_dir_root):\n",
    "#         \"\"\"\n",
    "#         Downloads and extracts a gz file into the given cache directory. Returns the\n",
    "#         full file path of the extracted gz file.\n",
    "#         Args:\n",
    "#             file_url: url of the gz file to be downloaded and extracted.\n",
    "#             cache_dir_root: Directory to extract file into.\n",
    "#         \"\"\"\n",
    "#         file_fname = Path(file_url).stem\n",
    "#         file_complete_path = os.path.join(cache_dir_root, \"downloads\", file_fname)\n",
    "\n",
    "#         if not os.path.exists(file_complete_path):\n",
    "#             if not os.path.exists(file_complete_path + \".gz\"):\n",
    "#                 with tqdm(\n",
    "#                         unit=\"B\",\n",
    "#                         unit_scale=True,\n",
    "#                         unit_divisor=1024,\n",
    "#                         miniters=1,\n",
    "#                         desc=file_url.split(\"/\")[-1],\n",
    "#                 ) as t:\n",
    "#                     urllib.request.urlretrieve(\n",
    "#                         file_url, file_complete_path + \".gz\", reporthook=self.hook(t) \n",
    "#                     )\n",
    "#             with gzip.open(file_complete_path + \".gz\", \"rb\") as file_in:\n",
    "#                 with open(file_complete_path, \"wb\") as file_out:\n",
    "#                     shutil.copyfileobj(file_in, file_out)\n",
    "#         return file_complete_path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def download_and_extract_gz(file_url, cache_dir_root):\n",
    "#     file_complete_path = os.path.join(cache_dir_root, file_url.split(\"/\")[-1])\n",
    "#     if not os.path.exists(file_complete_path):\n",
    "#         # Download the file if not exists\n",
    "#         with tqdm(\n",
    "#             unit=\"B\",\n",
    "#             unit_scale=True,\n",
    "#             unit_divisor=1024,\n",
    "#             miniters=1,\n",
    "#             desc=file_url.split(\"/\")[-1],\n",
    "#         ) as t:\n",
    "#             urllib.request.urlretrieve(\n",
    "#                 file_url, file_complete_path + \".gz\", reporthook=lambda b, bs, tbs: t.update(bs)\n",
    "#             )\n",
    "#         with gzip.open(file_complete_path + \".gz\", \"rb\") as file_in:\n",
    "#             with open(file_complete_path, \"wb\") as file_out:\n",
    "#                 shutil.copyfileobj(file_in, file_out)\n",
    "\n",
    "#     return file_complete_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "57a4c8d5-721d-4c94-8309-012d0b4283be",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def PreProcess(data_filename, label):\n",
    "#     # df=pd.read_csv(data_filename, delimiter='\\t',  names=['chromosome', 'position', 'REF','ALT','info'])\n",
    "#     df=pd.read_csv(data_filename, header=0, sep='\\t', index_col=None)   # With head column row\n",
    "\n",
    "#     df=df[(df['REF'].str.len() == 1) & (df['ALT'].str.len() == 1)]\n",
    "#     # df['chromosome'] = df['chromosome'].str.replace('chr', '')\n",
    "#     df['label']=label\n",
    "#     df['ref_forward_sequence']=''\n",
    "#     df['alt_forward_sequence']=''\n",
    "\n",
    "#     df.drop(columns=['Consequence'], inplace=True)\n",
    "#     df = df.reset_index(drop=True)\n",
    "#     # df = df[~df['CHROM'].isin(['9', '10'])] # chromosome\n",
    "\n",
    "#     df = df.rename(columns={'CHROM': 'chromosome', 'POS':'position'})\n",
    "\n",
    "#     return df\n",
    "    \n",
    "# df1 = PreProcess('gnomad.v4.1.exon.txt',0)\n",
    "# df2 = PreProcess('gnomad.v4.1.intergenic.txt',1)\n",
    "# df3 = PreProcess('gnomad.v4.1.proximity.txt',2)\n",
    "# # df = df.drop(['Consequence'], axis=1)\n",
    "# # df = df.reset_index(drop=True)\n",
    "# # df = df[~df['chromosome'].isin(['9', '10'])]\n",
    "\n",
    "\n",
    "# df = pd.concat([df1, df2,df3], ignore_index=True)\n",
    "# # df.to_csv('gnomad.v4.1.caduceus.csv', index=False)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cecbc768-fe29-4d67-9dcc-25c3fb887cad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>CHROM</th>\n",
       "      <th>POS</th>\n",
       "      <th>REF</th>\n",
       "      <th>ALT</th>\n",
       "      <th>CLNDN</th>\n",
       "      <th>CLNREVSTAT</th>\n",
       "      <th>CLNSIGCONF</th>\n",
       "      <th>Pathogenicity</th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Transcript</th>\n",
       "      <th>Consequence</th>\n",
       "      <th>AAChange</th>\n",
       "      <th>gnomAD_AF_popmax</th>\n",
       "      <th>UKBB_AF</th>\n",
       "      <th>Ensembl</th>\n",
       "      <th>OMIM</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>753921</td>\n",
       "      <td>1</td>\n",
       "      <td>1020383</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>not_provided</td>\n",
       "      <td>criteria_provided,_single_submitter</td>\n",
       "      <td>.</td>\n",
       "      <td>B</td>\n",
       "      <td>AGRN</td>\n",
       "      <td>.</td>\n",
       "      <td>intron</td>\n",
       "      <td>.</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>ENSG00000188157</td>\n",
       "      <td>Myasthenic syndrome, congenital, 8, with pre- ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1616405</td>\n",
       "      <td>1</td>\n",
       "      <td>1020390</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>Congenital_myasthenic_syndrome_8</td>\n",
       "      <td>criteria_provided,_single_submitter</td>\n",
       "      <td>.</td>\n",
       "      <td>B</td>\n",
       "      <td>AGRN</td>\n",
       "      <td>.</td>\n",
       "      <td>intron</td>\n",
       "      <td>.</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>ENSG00000188157</td>\n",
       "      <td>Myasthenic syndrome, congenital, 8, with pre- ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2970137</td>\n",
       "      <td>1</td>\n",
       "      <td>1020391</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>Congenital_myasthenic_syndrome_8</td>\n",
       "      <td>criteria_provided,_single_submitter</td>\n",
       "      <td>.</td>\n",
       "      <td>B</td>\n",
       "      <td>AGRN</td>\n",
       "      <td>.</td>\n",
       "      <td>intron</td>\n",
       "      <td>.</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>ENSG00000188157</td>\n",
       "      <td>Myasthenic syndrome, congenital, 8, with pre- ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1642546</td>\n",
       "      <td>1</td>\n",
       "      <td>1020392</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>Congenital_myasthenic_syndrome_8</td>\n",
       "      <td>criteria_provided,_single_submitter</td>\n",
       "      <td>.</td>\n",
       "      <td>B</td>\n",
       "      <td>AGRN</td>\n",
       "      <td>.</td>\n",
       "      <td>intron</td>\n",
       "      <td>.</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>0.000024</td>\n",
       "      <td>ENSG00000188157</td>\n",
       "      <td>Myasthenic syndrome, congenital, 8, with pre- ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1663222</td>\n",
       "      <td>1</td>\n",
       "      <td>1020392</td>\n",
       "      <td>C</td>\n",
       "      <td>G</td>\n",
       "      <td>Congenital_myasthenic_syndrome_8</td>\n",
       "      <td>criteria_provided,_single_submitter</td>\n",
       "      <td>.</td>\n",
       "      <td>B</td>\n",
       "      <td>AGRN</td>\n",
       "      <td>.</td>\n",
       "      <td>intron</td>\n",
       "      <td>.</td>\n",
       "      <td>0.000994</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>ENSG00000188157</td>\n",
       "      <td>Myasthenic syndrome, congenital, 8, with pre- ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95755</th>\n",
       "      <td>1297951</td>\n",
       "      <td>X</td>\n",
       "      <td>154991043</td>\n",
       "      <td>A</td>\n",
       "      <td>G</td>\n",
       "      <td>not_provided</td>\n",
       "      <td>criteria_provided,_single_submitter</td>\n",
       "      <td>.</td>\n",
       "      <td>B</td>\n",
       "      <td>F8</td>\n",
       "      <td>.</td>\n",
       "      <td>intron</td>\n",
       "      <td>.</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>ENSG00000185010</td>\n",
       "      <td>Thrombophilia 13, X-linked, due to factor VIII...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95756</th>\n",
       "      <td>994413</td>\n",
       "      <td>X</td>\n",
       "      <td>154992919</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>Hereditary_factor_VIII_deficiency_disease</td>\n",
       "      <td>criteria_provided,_single_submitter</td>\n",
       "      <td>.</td>\n",
       "      <td>B</td>\n",
       "      <td>F8</td>\n",
       "      <td>.</td>\n",
       "      <td>intron</td>\n",
       "      <td>.</td>\n",
       "      <td>0.015161</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>ENSG00000185010</td>\n",
       "      <td>Thrombophilia 13, X-linked, due to factor VIII...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95757</th>\n",
       "      <td>368125</td>\n",
       "      <td>X</td>\n",
       "      <td>154993157</td>\n",
       "      <td>G</td>\n",
       "      <td>A</td>\n",
       "      <td>not_provided|Hereditary_factor_VIII_deficiency...</td>\n",
       "      <td>criteria_provided,_multiple_submitters,_no_con...</td>\n",
       "      <td>.</td>\n",
       "      <td>B</td>\n",
       "      <td>F8</td>\n",
       "      <td>.</td>\n",
       "      <td>intron</td>\n",
       "      <td>.</td>\n",
       "      <td>0.027232</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>ENSG00000185010</td>\n",
       "      <td>Thrombophilia 13, X-linked, due to factor VIII...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95758</th>\n",
       "      <td>804143</td>\n",
       "      <td>X</td>\n",
       "      <td>154997114</td>\n",
       "      <td>C</td>\n",
       "      <td>T</td>\n",
       "      <td>not_provided|Hereditary_factor_VIII_deficiency...</td>\n",
       "      <td>criteria_provided,_multiple_submitters,_no_con...</td>\n",
       "      <td>.</td>\n",
       "      <td>B</td>\n",
       "      <td>F8</td>\n",
       "      <td>.</td>\n",
       "      <td>intron</td>\n",
       "      <td>.</td>\n",
       "      <td>0.020523</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>ENSG00000185010</td>\n",
       "      <td>Thrombophilia 13, X-linked, due to factor VIII...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95759</th>\n",
       "      <td>1330609</td>\n",
       "      <td>X</td>\n",
       "      <td>154999613</td>\n",
       "      <td>A</td>\n",
       "      <td>C</td>\n",
       "      <td>Hereditary_factor_VIII_deficiency_disease</td>\n",
       "      <td>criteria_provided,_single_submitter</td>\n",
       "      <td>.</td>\n",
       "      <td>B</td>\n",
       "      <td>F8</td>\n",
       "      <td>.</td>\n",
       "      <td>intron</td>\n",
       "      <td>.</td>\n",
       "      <td>0.000146</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>ENSG00000185010</td>\n",
       "      <td>Thrombophilia 13, X-linked, due to factor VIII...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95760 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID CHROM        POS REF ALT  \\\n",
       "0       753921     1    1020383   C   T   \n",
       "1      1616405     1    1020390   C   T   \n",
       "2      2970137     1    1020391   C   G   \n",
       "3      1642546     1    1020392   C   T   \n",
       "4      1663222     1    1020392   C   G   \n",
       "...        ...   ...        ...  ..  ..   \n",
       "95755  1297951     X  154991043   A   G   \n",
       "95756   994413     X  154992919   C   T   \n",
       "95757   368125     X  154993157   G   A   \n",
       "95758   804143     X  154997114   C   T   \n",
       "95759  1330609     X  154999613   A   C   \n",
       "\n",
       "                                                   CLNDN  \\\n",
       "0                                           not_provided   \n",
       "1                       Congenital_myasthenic_syndrome_8   \n",
       "2                       Congenital_myasthenic_syndrome_8   \n",
       "3                       Congenital_myasthenic_syndrome_8   \n",
       "4                       Congenital_myasthenic_syndrome_8   \n",
       "...                                                  ...   \n",
       "95755                                       not_provided   \n",
       "95756          Hereditary_factor_VIII_deficiency_disease   \n",
       "95757  not_provided|Hereditary_factor_VIII_deficiency...   \n",
       "95758  not_provided|Hereditary_factor_VIII_deficiency...   \n",
       "95759          Hereditary_factor_VIII_deficiency_disease   \n",
       "\n",
       "                                              CLNREVSTAT CLNSIGCONF  \\\n",
       "0                    criteria_provided,_single_submitter          .   \n",
       "1                    criteria_provided,_single_submitter          .   \n",
       "2                    criteria_provided,_single_submitter          .   \n",
       "3                    criteria_provided,_single_submitter          .   \n",
       "4                    criteria_provided,_single_submitter          .   \n",
       "...                                                  ...        ...   \n",
       "95755                criteria_provided,_single_submitter          .   \n",
       "95756                criteria_provided,_single_submitter          .   \n",
       "95757  criteria_provided,_multiple_submitters,_no_con...          .   \n",
       "95758  criteria_provided,_multiple_submitters,_no_con...          .   \n",
       "95759                criteria_provided,_single_submitter          .   \n",
       "\n",
       "      Pathogenicity Symbol Transcript Consequence AAChange  gnomAD_AF_popmax  \\\n",
       "0                 B   AGRN          .      intron        .         -1.000000   \n",
       "1                 B   AGRN          .      intron        .         -1.000000   \n",
       "2                 B   AGRN          .      intron        .         -1.000000   \n",
       "3                 B   AGRN          .      intron        .          0.000713   \n",
       "4                 B   AGRN          .      intron        .          0.000994   \n",
       "...             ...    ...        ...         ...      ...               ...   \n",
       "95755             B     F8          .      intron        .         -1.000000   \n",
       "95756             B     F8          .      intron        .          0.015161   \n",
       "95757             B     F8          .      intron        .          0.027232   \n",
       "95758             B     F8          .      intron        .          0.020523   \n",
       "95759             B     F8          .      intron        .          0.000146   \n",
       "\n",
       "        UKBB_AF          Ensembl  \\\n",
       "0      0.000005  ENSG00000188157   \n",
       "1     -1.000000  ENSG00000188157   \n",
       "2      0.000024  ENSG00000188157   \n",
       "3      0.000024  ENSG00000188157   \n",
       "4      0.000015  ENSG00000188157   \n",
       "...         ...              ...   \n",
       "95755 -1.000000  ENSG00000185010   \n",
       "95756  0.000309  ENSG00000185010   \n",
       "95757  0.000142  ENSG00000185010   \n",
       "95758  0.000588  ENSG00000185010   \n",
       "95759 -1.000000  ENSG00000185010   \n",
       "\n",
       "                                                    OMIM  \n",
       "0      Myasthenic syndrome, congenital, 8, with pre- ...  \n",
       "1      Myasthenic syndrome, congenital, 8, with pre- ...  \n",
       "2      Myasthenic syndrome, congenital, 8, with pre- ...  \n",
       "3      Myasthenic syndrome, congenital, 8, with pre- ...  \n",
       "4      Myasthenic syndrome, congenital, 8, with pre- ...  \n",
       "...                                                  ...  \n",
       "95755  Thrombophilia 13, X-linked, due to factor VIII...  \n",
       "95756  Thrombophilia 13, X-linked, due to factor VIII...  \n",
       "95757  Thrombophilia 13, X-linked, due to factor VIII...  \n",
       "95758  Thrombophilia 13, X-linked, due to factor VIII...  \n",
       "95759  Thrombophilia 13, X-linked, due to factor VIII...  \n",
       "\n",
       "[95760 rows x 17 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# datafile='clinvar_20240805.noncoding'\n",
    "# csv_filename = datafile+'.txt'\n",
    "# df=pd.read_csv(datafile+'.txt', delimiter='\\t')\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fb01fba-644b-48c8-ae7a-c70080ad374b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chromosome</th>\n",
       "      <th>position</th>\n",
       "      <th>REF</th>\n",
       "      <th>ALT</th>\n",
       "      <th>size</th>\n",
       "      <th>label</th>\n",
       "      <th>ref_forward_sequence</th>\n",
       "      <th>alt_forward_sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1020382</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1020389</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1020390</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1020391</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1020391</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95755</th>\n",
       "      <td>X</td>\n",
       "      <td>154991042</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95756</th>\n",
       "      <td>X</td>\n",
       "      <td>154992918</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95757</th>\n",
       "      <td>X</td>\n",
       "      <td>154993156</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95758</th>\n",
       "      <td>X</td>\n",
       "      <td>154997113</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95759</th>\n",
       "      <td>X</td>\n",
       "      <td>154999612</td>\n",
       "      <td>A</td>\n",
       "      <td>A</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95760 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      chromosome   position REF ALT  size  label ref_forward_sequence  \\\n",
       "0              1    1020382   A   A   128      0                        \n",
       "1              1    1020389   A   A   128      0                        \n",
       "2              1    1020390   A   A   128      0                        \n",
       "3              1    1020391   A   A   128      0                        \n",
       "4              1    1020391   A   A   128      0                        \n",
       "...          ...        ...  ..  ..   ...    ...                  ...   \n",
       "95755          X  154991042   A   A   128      0                        \n",
       "95756          X  154992918   A   A   128      0                        \n",
       "95757          X  154993156   A   A   128      0                        \n",
       "95758          X  154997113   A   A   128      0                        \n",
       "95759          X  154999612   A   A   128      0                        \n",
       "\n",
       "      alt_forward_sequence  \n",
       "0                           \n",
       "1                           \n",
       "2                           \n",
       "3                           \n",
       "4                           \n",
       "...                    ...  \n",
       "95755                       \n",
       "95756                       \n",
       "95757                       \n",
       "95758                       \n",
       "95759                       \n",
       "\n",
       "[95760 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# csv_filename = 'Homo_sapiens.GRCh38.109.txt'\n",
    "datafile='clinvar_20240805.noncoding'\n",
    "\n",
    "csv_filename = datafile+'.txt'\n",
    "df=pd.read_csv(datafile+'.txt', delimiter='\\t')\n",
    "\n",
    "max_length= 128 # 186\n",
    "\n",
    "columns_to_keep=['CHROM','POS','ID','REF','ALT','Pathogenicity']\n",
    "df = df[columns_to_keep]\n",
    "\n",
    "for i in range(1,23):\n",
    "    df.loc[df['CHROM']==i,'CHROM']=str(i)\n",
    "\n",
    "df=df[~df['CHROM'].isna()]\n",
    "df = df[~df['CHROM'].str.contains('KI')]\n",
    "df = df[~df['CHROM'].str.contains('GL')]\n",
    "\n",
    "df['START']= df['POS']- max_length //2 -1\n",
    "df['END']  = df['START'] + max_length\n",
    "df['size'] = max_length\n",
    "\n",
    "Pathogenicity_dict={'B':0,'P':1}\n",
    "df['label'] = df['Pathogenicity'].map(Pathogenicity_dict)\n",
    "# df['CHROM'].value_counts()\n",
    "\n",
    "# Assuming df is your DataFrame and 'cluster' is the column with string labels\n",
    "# df['CHROM'] ='chr'+df['CHROM']\n",
    "df['POS'] = ((df['START'] + df['END']) / 2).round().astype(int)\n",
    "\n",
    "df['REF']='A'\n",
    "df['ALT']='A'\n",
    "df['ref_forward_sequence']=''\n",
    "df['alt_forward_sequence']=''\n",
    "\n",
    "df = df.rename(columns={'CHROM': 'chromosome', 'POS':'position'})\n",
    "# df.columns = df.columns.str.upper()\n",
    "# df.drop(columns=['START','END','type','cluster','size'], inplace=True)\n",
    "# df.drop(['ID','Pathogenicity','START','END','SIZE'], axis=1, inplace=True)\n",
    "df.drop(['ID','Pathogenicity','START','END'], axis=1, inplace=True)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c807182-c903-4d71-a91c-497de1421681",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# csv_filename = 'Homo_sapiens.GRCh38.109.txt'\n",
    "# datafile='clinvar_20240805.noncoding'\n",
    "# csv_filename = datafile+'.txt'\n",
    "# df=pd.read_csv(csv_filename, delimiter='\\t')\n",
    "# # df['cluster'].value_counts()\n",
    "# cluster_dict = {\n",
    "#     'first_exon': 0,\n",
    "#     'first_intron': 1,\n",
    "#     'first_three_prime_UTR': 2,\n",
    "#     'first_five_prime_UTR': 3,\n",
    "#     'ncRNA_gene': 4,\n",
    "#     'pseudogene': 5,\n",
    "#     'smallRNA': 6\n",
    "# }\n",
    "\n",
    "# # Assuming df is your DataFrame and 'cluster' is the column with string labels\n",
    "# # df['chrom'] ='chr'+df['chrom']\n",
    "# df['POS'] = ((df['start'] + df['end']) / 2).round().astype(int)\n",
    "# df['REF']='A'\n",
    "# df['ALT']='A'\n",
    "# df['label'] = df['cluster'].map(cluster_dict)\n",
    "# df['ref_forward_sequence']=''\n",
    "# df['alt_forward_sequence']=''\n",
    "\n",
    "# df = df.rename(columns={'chrom': 'chromosome', 'POS':'position'})\n",
    "# # df.columns = df.columns.str.upper()\n",
    "# df.drop(columns=['start','end','type','cluster','size'], inplace=True)\n",
    "\n",
    "# # filtered_df = df[df['chromosome'].str.contains('KI')]\n",
    "# df = df[~df['chromosome'].str.contains('KI')]\n",
    "# df = df[~df['chromosome'].str.contains('GL')]\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6dff00c6-0b9e-4610-a3aa-027f9d911bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtered_df = df[df['chromosome'].str.contains('GL')]\n",
    "# filtered_df \n",
    "# df_filtered = df[~df['chromosome'].str.contains('KI')]\n",
    "# df_filtered\n",
    "# print(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "42647881-f2f8-4095-a663-86068394ce65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.loc[df['label']==0]\n",
    "# df.loc[df['label']==1]\n",
    "# len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dbd51231-1190-4b55-ae0f-4fc356e49c0b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def generate_examples(df, sequence_length=131072):\n",
    "#         \"\"\"\n",
    "#         A generator which produces examples each with ref/alt allele\n",
    "#         and corresponding binary label. The sequences are extended to\n",
    "#         the desired sequence length and standardized before returning.\n",
    "#         \"\"\"\n",
    "\n",
    "#         cache_dir_root=\"/home/sunhuaikuan/ondemand/blue_caduceus/\"\n",
    "#         reference_genome_file = download_and_extract_gz(H38_REFERENCE_GENOME_URL, cache_dir_root)\n",
    "#         reference_genome = Fasta(reference_genome_file, one_based_attributes=False)\n",
    "\n",
    "#         key = 0\n",
    "#         for idx, row in df.iterrows():\n",
    "#             start = row[\"position\"] - 1  # sub 1 to create idx since coords are 1-based\n",
    "#             alt_allele = row[\"ALT\"]\n",
    "#             label = row[\"label\"]\n",
    "#             chromosome = row[\"chromosome\"]\n",
    "\n",
    "#             # get reference forward sequence\n",
    "#             ref_forward = pad_sequence(\n",
    "#                 chromosome=reference_genome[chromosome],\n",
    "#                 start=start,\n",
    "#                 sequence_length=sequence_length,\n",
    "#                 negative_strand=False,\n",
    "#             )\n",
    "\n",
    "#             # only if a valid sequence returned\n",
    "#             if ref_forward:\n",
    "#                 # Mutate sequence with the alt allele at the SNP position,\n",
    "#                 # which is always centered in the string returned from pad_sequence\n",
    "#                 alt_forward = list(ref_forward)\n",
    "#                 alt_forward[sequence_length // 2] = alt_allele\n",
    "#                 alt_forward = \"\".join(alt_forward)\n",
    "\n",
    "#                 yield key, {\n",
    "#                     \"label\": label,\n",
    "#                     # \"chromosome\": re.sub(\"chr\", \"\", chromosome),\n",
    "#                     \"chromosome\": chromosome,\n",
    "#                     \"ref_forward_sequence\": standardize_sequence(ref_forward),\n",
    "#                     \"alt_forward_sequence\": standardize_sequence(alt_forward),\n",
    "#                     \"position\": row['position']\n",
    "#                 }\n",
    "#                 key += 1\n",
    "\n",
    "\n",
    "# df2=generate_examples(df)\n",
    "\n",
    "# max_rows = 2\n",
    "# count = 0\n",
    "# for key, example in df2:\n",
    "#     if count >= max_rows:\n",
    "#         break\n",
    "        \n",
    "#     # print(f\"Key: {key}\")\n",
    "#     # print(f\"Label: {example['label']}\")\n",
    "#     # print(f\"Chromosome: {example['chromosome']}\")\n",
    "#     # print(f\"Reference Forward Sequence: {example['ref_forward_sequence'][:100]}\")\n",
    "#     # print(f\"Alternate Forward Sequence: {len(example['alt_forward_sequence'])}\")\n",
    "#     # print(f\"Position: {example['position']}\")\n",
    "#     # print()\n",
    "    \n",
    "#     list1 =example['ref_forward_sequence']\n",
    "#     list2 =example['alt_forward_sequence'] # [10, 8, 10, 10, 10, 10, 8, 10, 7]\n",
    "    \n",
    "#     print('list lenth='+str(len(list1)))\n",
    "    \n",
    "#     # Find the differing entries\n",
    "#     differing_entries = [(i, list1[i], list2[i]) for i in range(len(list1)) if list1[i] != list2[i]]\n",
    "    \n",
    "#     # Print the results\n",
    "#     if differing_entries:\n",
    "#         print(\"Entries where the values differ:\")\n",
    "#         for entry in differing_entries:\n",
    "#             index, value1, value2 = entry\n",
    "#             print(f\"Index {index}: List1 has {value1}, List2 has {value2}\")\n",
    "#     else:\n",
    "#         print(\"The lists are identical.\")\n",
    "    \n",
    "    \n",
    "#     count += 1\n",
    "# df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bcbb57a0-f60f-4ba2-ab26-d1747c732445",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# The below method will blow up memory since ref_forward_sequences,alt_forward_sequences are 131,072 each.\n",
    "\n",
    "# ref_forward_sequences = []\n",
    "# alt_forward_sequences = []\n",
    "\n",
    "# generator=generate_examples(df)\n",
    "\n",
    "# # Iterate over the generator and populate the dictionaries\n",
    "# for key, example in generator:\n",
    "#     ref_forward_sequences.append(example['ref_forward_sequence'])\n",
    "#     alt_forward_sequences.append(example['alt_forward_sequence'])\n",
    "\n",
    "\n",
    "# df['ref_forward_sequence'] = ref_forward_sequences\n",
    "# df['alt_forward_sequence'] = alt_forward_sequences\n",
    "\n",
    "# dataset = Dataset.from_pandas(df)\n",
    "# # Save the dataset to disk\n",
    "# dataset.save_to_disk('prof_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4c11c95c-a715-46b2-840a-fb70975ce68d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# def generate_and_yield_examples(df, sequence_length=131072):\n",
    "#         \"\"\"\n",
    "#         A generator which produces examples each with ref/alt allele\n",
    "#         and corresponding binary label. The sequences are extended to\n",
    "#         the desired sequence length and standardized before returning.\n",
    "#         \"\"\"\n",
    "\n",
    "#         cache_dir_root=\"/home/sunhuaikuan/ondemand/blue_caduceus/\"\n",
    "#         reference_genome_file = download_and_extract_gz(H38_REFERENCE_GENOME_URL, cache_dir_root)\n",
    "#         reference_genome = Fasta(reference_genome_file, one_based_attributes=False)\n",
    "\n",
    "      \n",
    "#         for idx, row in df.iterrows():\n",
    "#             start = row[\"position\"] - 1  # sub 1 to create idx since coords are 1-based\n",
    "#             alt_allele = row[\"ALT\"]\n",
    "#             label = row[\"label\"]\n",
    "#             chromosome = row[\"chromosome\"]\n",
    "\n",
    "#             # get reference forward sequence\n",
    "#             ref_forward = pad_sequence(\n",
    "#                 chromosome=reference_genome[chromosome],\n",
    "#                 start=start,\n",
    "#                 sequence_length=sequence_length,\n",
    "#                 negative_strand=False,\n",
    "#             )\n",
    "\n",
    "#             # only if a valid sequence returned\n",
    "#             if ref_forward:\n",
    "#                 # Mutate sequence with the alt allele at the SNP position,\n",
    "#                 # which is always centered in the string returned from pad_sequence\n",
    "#                 alt_forward = list(ref_forward)\n",
    "#                 alt_forward[sequence_length // 2] = alt_allele\n",
    "#                 alt_forward = \"\".join(alt_forward)\n",
    "\n",
    "#                 yield  {\n",
    "#                     \"label\": label,\n",
    "#                     # \"chromosome\": re.sub(\"chr\", \"\", chromosome),\n",
    "#                     \"chromosome\": chromosome,\n",
    "#                     \"ref_forward_sequence\": standardize_sequence(ref_forward),\n",
    "#                     \"alt_forward_sequence\": standardize_sequence(alt_forward),\n",
    "#                     \"position\": row['position']\n",
    "#                 }\n",
    "                \n",
    "                \n",
    "        \n",
    "# features = Features({\n",
    "#     'label': Value('int64'),\n",
    "#     'chromosome': Value('string'),\n",
    "#     'ref_forward_sequence': Value('string'),\n",
    "#     'alt_forward_sequence': Value('string'),\n",
    "#     'position': Value('int64')\n",
    "# })\n",
    "\n",
    "# dataset = Dataset.from_generator(lambda: generate_and_yield_examples(df), features=features)\n",
    "\n",
    "# dataset.save_to_disk('prof_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c0a9ab6-eac9-4816-8eb4-85a62c4b170d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85e6ca13be034a4caa548e09f9a65e08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:52\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:1125\u001b[0m, in \u001b[0;36mDataset.from_generator\u001b[0;34m(generator, features, cache_dir, keep_in_memory, gen_kwargs, num_proc, **kwargs)\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Create a Dataset from a generator.\u001b[39;00m\n\u001b[1;32m   1069\u001b[0m \n\u001b[1;32m   1070\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;124;03m```\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1115\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mio\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgenerator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GeneratorDatasetInputStream\n\u001b[1;32m   1117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mGeneratorDatasetInputStream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_in_memory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_memory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m-> 1125\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/io/generator.py:47\u001b[0m, in \u001b[0;36mGeneratorDatasetInputStream.read\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     44\u001b[0m     verification_mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     base_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbase_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mas_dataset(\n\u001b[1;32m     55\u001b[0m         split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m, verification_mode\u001b[38;5;241m=\u001b[39mverification_mode, in_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_in_memory\n\u001b[1;32m     56\u001b[0m     )\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/builder.py:1027\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, ignore_verifications, try_from_hf_gcs, dl_manager, base_path, use_auth_token, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1026\u001b[0m         prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[0;32m-> 1027\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1031\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1033\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/builder.py:1789\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[0m\n\u001b[1;32m   1788\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_download_and_prepare\u001b[39m(\u001b[38;5;28mself\u001b[39m, dl_manager, verification_mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_splits_kwargs):\n\u001b[0;32m-> 1789\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1790\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1791\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1792\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_duplicate_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mVerificationMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBASIC_CHECKS\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mVerificationMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mALL_CHECKS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1794\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_splits_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1795\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/builder.py:1122\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m   1118\u001b[0m split_dict\u001b[38;5;241m.\u001b[39madd(split_generator\u001b[38;5;241m.\u001b[39msplit_info)\n\u001b[1;32m   1120\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1121\u001b[0m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[0;32m-> 1122\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1124\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   1125\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find data file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1126\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1127\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1128\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m   1129\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/builder.py:1627\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split\u001b[0;34m(self, split_generator, check_duplicate_keys, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[1;32m   1625\u001b[0m job_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1626\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[0;32m-> 1627\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m job_id, done, content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_split_single(\n\u001b[1;32m   1628\u001b[0m         gen_kwargs\u001b[38;5;241m=\u001b[39mgen_kwargs, job_id\u001b[38;5;241m=\u001b[39mjob_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_prepare_split_args\n\u001b[1;32m   1629\u001b[0m     ):\n\u001b[1;32m   1630\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   1631\u001b[0m             result \u001b[38;5;241m=\u001b[39m content\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/builder.py:1766\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001b[0m\n\u001b[1;32m   1756\u001b[0m     writer \u001b[38;5;241m=\u001b[39m writer_class(\n\u001b[1;32m   1757\u001b[0m         features\u001b[38;5;241m=\u001b[39mwriter\u001b[38;5;241m.\u001b[39m_features,\n\u001b[1;32m   1758\u001b[0m         path\u001b[38;5;241m=\u001b[39mfpath\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSSSSS\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshard_id\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m05d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJJJJJ\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjob_id\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m05d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1763\u001b[0m         embed_local_files\u001b[38;5;241m=\u001b[39membed_local_files,\n\u001b[1;32m   1764\u001b[0m     )\n\u001b[1;32m   1765\u001b[0m example \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mfeatures\u001b[38;5;241m.\u001b[39mencode_example(record) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m record\n\u001b[0;32m-> 1766\u001b[0m \u001b[43mwriter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1767\u001b[0m num_examples_progress_update \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1768\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m>\u001b[39m _time \u001b[38;5;241m+\u001b[39m config\u001b[38;5;241m.\u001b[39mPBAR_REFRESH_TIME_INTERVAL:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_writer.py:500\u001b[0m, in \u001b[0;36mArrowWriter.write\u001b[0;34m(self, example, key, writer_batch_size)\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[38;5;66;03m# Re-intializing to empty list for next batch\u001b[39;00m\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhkey_record \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 500\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_examples_on_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_writer.py:458\u001b[0m, in \u001b[0;36mArrowWriter.write_examples_on_file\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    454\u001b[0m         batch_examples[col] \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    455\u001b[0m             row[\u001b[38;5;241m0\u001b[39m][col]\u001b[38;5;241m.\u001b[39mto_pylist()[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(row[\u001b[38;5;241m0\u001b[39m][col], (pa\u001b[38;5;241m.\u001b[39mArray, pa\u001b[38;5;241m.\u001b[39mChunkedArray)) \u001b[38;5;28;01melse\u001b[39;00m row[\u001b[38;5;241m0\u001b[39m][col]\n\u001b[1;32m    456\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_examples\n\u001b[1;32m    457\u001b[0m         ]\n\u001b[0;32m--> 458\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_examples\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_examples \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_writer.py:572\u001b[0m, in \u001b[0;36mArrowWriter.write_batch\u001b[0;34m(self, batch_examples, writer_batch_size)\u001b[0m\n\u001b[1;32m    570\u001b[0m schema \u001b[38;5;241m=\u001b[39m inferred_features\u001b[38;5;241m.\u001b[39marrow_schema \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpa_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mschema\n\u001b[1;32m    571\u001b[0m pa_table \u001b[38;5;241m=\u001b[39m pa\u001b[38;5;241m.\u001b[39mTable\u001b[38;5;241m.\u001b[39mfrom_arrays(arrays, schema\u001b[38;5;241m=\u001b[39mschema)\n\u001b[0;32m--> 572\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/datasets/arrow_writer.py:590\u001b[0m, in \u001b[0;36mArrowWriter.write_table\u001b[0;34m(self, pa_table, writer_batch_size)\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_bytes \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pa_table\u001b[38;5;241m.\u001b[39mnbytes\n\u001b[1;32m    589\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_examples \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pa_table\u001b[38;5;241m.\u001b[39mnum_rows\n\u001b[0;32m--> 590\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpa_writer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwriter_batch_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyarrow/ipc.pxi:529\u001b[0m, in \u001b[0;36mpyarrow.lib._CRecordBatchWriter.write_table\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyarrow/types.pxi:88\u001b[0m, in \u001b[0;36mpyarrow.lib._datatype_to_pep3118\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/fsspec/implementations/local.py:422\u001b[0m, in \u001b[0;36mLocalFileOpener.write\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrite\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def generate_and_yield_examples(df, sequence_length=131072):\n",
    "        \"\"\"\n",
    "        A generator which produces examples each with ref/alt allele\n",
    "        and corresponding binary label. The sequences are extended to\n",
    "        the desired sequence length and standardized before returning.\n",
    "        \"\"\"\n",
    "\n",
    "        cache_dir_root=\"/home/sunhuaikuan/ondemand/blue_caduceus/\"\n",
    "        reference_genome_file = download_and_extract_gz(H38_REFERENCE_GENOME_URL, cache_dir_root)\n",
    "        reference_genome = Fasta(reference_genome_file, one_based_attributes=False)\n",
    "\n",
    "      \n",
    "        for idx, row in df.iterrows():\n",
    "            start = row[\"position\"] - 1  # sub 1 to create idx since coords are 1-based\n",
    "            alt_allele = row[\"ALT\"]\n",
    "            label = row[\"label\"]\n",
    "            chromosome = row[\"chromosome\"]\n",
    "\n",
    "            # get reference forward sequence\n",
    "            ref_forward = pad_sequence(\n",
    "                chromosome=reference_genome[chromosome],\n",
    "                start=start,\n",
    "                sequence_length=sequence_length,\n",
    "                negative_strand=False,\n",
    "            )\n",
    "\n",
    "            # only if a valid sequence returned\n",
    "            if ref_forward:\n",
    "                # Mutate sequence with the alt allele at the SNP position,\n",
    "                # which is always centered in the string returned from pad_sequence\n",
    "                alt_forward = list(ref_forward)\n",
    "                alt_forward[sequence_length // 2] = alt_allele\n",
    "                alt_forward = \"\".join(alt_forward)\n",
    "\n",
    "                yield  {\n",
    "                    \"label\": label,\n",
    "                    # \"chromosome\": re.sub(\"chr\", \"\", chromosome),\n",
    "                    \"chromosome\": chromosome,\n",
    "                    \"ref_forward_sequence\": standardize_sequence(ref_forward),\n",
    "                    \"alt_forward_sequence\": standardize_sequence(alt_forward),\n",
    "                    \"position\": row['position']\n",
    "                }\n",
    "                        \n",
    "features = Features({\n",
    "    'label': Value('int64'),\n",
    "    'chromosome': Value('string'),\n",
    "    'ref_forward_sequence': Value('string'),\n",
    "    'alt_forward_sequence': Value('string'),\n",
    "    'position': Value('int64')\n",
    "})\n",
    "\n",
    "dataset = Dataset.from_generator(lambda: generate_and_yield_examples(df), features=features, keep_in_memory=True)\n",
    "# dataset.save_to_disk('prof_dataset')\n",
    "\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': dataset,\n",
    "    # 'test': dataset\n",
    "})\n",
    "\n",
    "save_path=\"pathgenicity_noncoding_dataset\"\n",
    "dataset_dict.save_to_disk(save_path)\n",
    "\n",
    "print(\"Dataset saved in batches successfully.\")\n",
    "\n",
    "dataset_dict = load_from_disk(save_path)\n",
    "train = dataset_dict['train']\n",
    "\n",
    "df = train.to_pandas()\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21468bc-94ec-4e49-9ebe-e49f035296e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da78195d-0146-4c28-87ea-f234c7230d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25ab5ceb2e214d4e99a8629262eb1a12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b03207d987342509e8a7e984ab5aa3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/30000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dataset chunk 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca71fe39a16f4e4aabb0a9e97306065a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "902fec11e4fc4544bc97c3b1c260e6d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/30000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dataset chunk 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04bad88de149447680c38ef9f7f434c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4223e8b23ce42a0bd57fd25bc29a1f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/30000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dataset chunk 3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e642b4c676c4cc79cae7c672812111e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67cc1ff286244d4d940bb213dfcd31cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved dataset chunk 4\n",
      "completed\n",
      "CPU times: user 11.5 s, sys: 573 ms, total: 12.1 s\n",
      "Wall time: 43 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# os.environ[\"HF_DATASETS_CACHE\"] = \"/blue/xiaofan/sunhuaikuan/larger/cache\"\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, Features, Value, concatenate_datasets\n",
    "from datasets.utils.file_utils import cached_path\n",
    "\n",
    "\n",
    "# Define the function to generate examples from a chunk of data\n",
    "def generate_and_yield_examples(df_chunk, sequence_length=131072):\n",
    "    # cache_dir_root = \"/home/sunhuaikuan/ondemand/blue_caduceus/\"\n",
    "    # reference_genome_file = download_and_extract_gz(H38_REFERENCE_GENOME_URL, cache_dir_root)\n",
    "    # reference_genome = Fasta(reference_genome_file, one_based_attributes=False)\n",
    "    reference_genome = Fasta('/home/sunhuaikuan/ondemand/blue_caduceus/hg38.fa', one_based_attributes=False)\n",
    "\n",
    "    for idx, row in df_chunk.iterrows():\n",
    "        start = row[\"position\"] - 1  # sub 1 to create idx since coords are 1-based\n",
    "        alt_allele = row[\"ALT\"]\n",
    "        label = row[\"label\"]\n",
    "        chromosome = row[\"chromosome\"]\n",
    "        sequence_length = row[\"size\"]\n",
    "\n",
    "        # Get reference forward sequence\n",
    "        ref_forward = pad_sequence(\n",
    "            chromosome=reference_genome[chromosome],\n",
    "            start=start,\n",
    "            sequence_length=sequence_length,\n",
    "            negative_strand=False,\n",
    "        )\n",
    "\n",
    "        # Only if a valid sequence returned\n",
    "        if ref_forward:\n",
    "            # Mutate sequence with the alt allele at the SNP position\n",
    "            alt_forward = list(ref_forward)\n",
    "            alt_forward[sequence_length // 2] = alt_allele\n",
    "            alt_forward = \"\".join(alt_forward)\n",
    "\n",
    "            yield {\n",
    "                \"label\": label,\n",
    "                \"chromosome\": chromosome,\n",
    "                \"ref_forward_sequence\": standardize_sequence(ref_forward),\n",
    "                \"alt_forward_sequence\": standardize_sequence(alt_forward),\n",
    "                \"position\": row['position']\n",
    "            }\n",
    "\n",
    "# Define the features of the dataset\n",
    "features = Features({\n",
    "    'label': Value('int64'),\n",
    "    'chromosome': Value('string'),\n",
    "    'ref_forward_sequence': Value('string'),\n",
    "    'alt_forward_sequence': Value('string'),\n",
    "    'position': Value('int64')\n",
    "})\n",
    "\n",
    "\n",
    "# Define the save path\n",
    "save_path = \"pathgenicity_noncoding_multisets\"\n",
    "\n",
    "\n",
    "# Check if the dataset already exists\n",
    "if os.path.exists(save_path):\n",
    "    # Load the existing dataset\n",
    "    dataset_dict = DatasetDict.load_from_disk(save_path)\n",
    "    existing_dataset = dataset_dict['train']\n",
    "else:\n",
    "    # Create an empty dataset if none exists\n",
    "    existing_dataset = None\n",
    "\n",
    "\n",
    "\n",
    "def clear_cache():\n",
    "    cache_dir = cached_path(\"/home/sunhuaikuan/.cache\")\n",
    "    if os.path.exists(cache_dir):\n",
    "        for filename in os.listdir(cache_dir):\n",
    "            file_path = os.path.join(cache_dir, filename)\n",
    "            try:\n",
    "                if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                    os.unlink(file_path)\n",
    "                elif os.path.isdir(file_path):\n",
    "                    shutil.rmtree(file_path)\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to delete {file_path}. Reason: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_from_disk, concatenate_datasets\n",
    "import gc\n",
    "\n",
    "\n",
    "# Define chunk size\n",
    "chunk_size = 30000  # Adjust chunk size based on available memory\n",
    "\n",
    "# Split df into chunks\n",
    "num_chunks = len(df) // chunk_size + int(len(df) % chunk_size != 0)\n",
    "\n",
    "existing_dataset = None\n",
    "\n",
    "# save_path_temp = save_path+'_mytemp'\n",
    "\n",
    "for i in range(num_chunks):\n",
    "    # Get the current chunk\n",
    "    df_chunk = df.iloc[i * chunk_size: (i + 1) * chunk_size]\n",
    "\n",
    "    # Create a new dataset from the current chunk\n",
    "    new_dataset = Dataset.from_generator(lambda: generate_and_yield_examples(df_chunk), features=features, keep_in_memory=True,cache_dir=None)  #,cache_dir=None\n",
    "\n",
    "    # Save the dataset chunk to disk\n",
    "    chunk_save_path = f\"{save_path}/train_{i}\"\n",
    "    new_dataset.save_to_disk(chunk_save_path)\n",
    "\n",
    "    print(f\"Saved dataset chunk {i + 1}\")\n",
    "    \n",
    "    clear_cache()\n",
    "    \n",
    "\n",
    "print(f\"completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b3641d-6a7e-4df0-acb8-33517e48f49b",
   "metadata": {},
   "source": [
    "## Combine all the save_path/train_# into one giant dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "583542a5-af8c-4ad5-a531-c3430304f7ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec4666f5113c413f8adf3d4d8bacd313",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/95760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_from_disk, concatenate_datasets\n",
    "\n",
    "save_path = \"pathgenicity_noncoding_multisets\"\n",
    "\n",
    "combined_dataset = None\n",
    "num_chunks = 4\n",
    "for i in range(num_chunks):\n",
    "    # Load each dataset chunk from disk\n",
    "    chunk_save_path = f\"{save_path}/train_{i}\"\n",
    "    dataset_chunk = load_from_disk(chunk_save_path)\n",
    "\n",
    "    # Concatenate incrementally\n",
    "    if combined_dataset is None:\n",
    "        combined_dataset = dataset_chunk\n",
    "    else:\n",
    "        combined_dataset = concatenate_datasets([combined_dataset, dataset_chunk])\n",
    "\n",
    "    # Optional: clean up memory\n",
    "    del dataset_chunk\n",
    "    gc.collect()\n",
    "\n",
    "# Save the combined dataset to disk\n",
    "combined_save_path = f\"{save_path}/combined_dataset\"\n",
    "combined_dataset.save_to_disk(combined_save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2a2346-df1a-4a2c-95f3-86372145d417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b52f2d84-db6d-4d33-9fcc-4c97896c1072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>chromosome</th>\n",
       "      <th>ref_forward_sequence</th>\n",
       "      <th>alt_forward_sequence</th>\n",
       "      <th>position</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>GGACGGTGGAGGAGATCCTCAACGTGGACCCGGTGCAGCACACGTA...</td>\n",
       "      <td>GGACGGTGGAGGAGATCCTCAACGTGGACCCGGTGCAGCACACGTA...</td>\n",
       "      <td>1020382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>GGAGGAGATCCTCAACGTGGACCCGGTGCAGCACACGTACTCCTGC...</td>\n",
       "      <td>GGAGGAGATCCTCAACGTGGACCCGGTGCAGCACACGTACTCCTGC...</td>\n",
       "      <td>1020389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>GAGGAGATCCTCAACGTGGACCCGGTGCAGCACACGTACTCCTGCA...</td>\n",
       "      <td>GAGGAGATCCTCAACGTGGACCCGGTGCAGCACACGTACTCCTGCA...</td>\n",
       "      <td>1020390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>AGGAGATCCTCAACGTGGACCCGGTGCAGCACACGTACTCCTGCAA...</td>\n",
       "      <td>AGGAGATCCTCAACGTGGACCCGGTGCAGCACACGTACTCCTGCAA...</td>\n",
       "      <td>1020391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>AGGAGATCCTCAACGTGGACCCGGTGCAGCACACGTACTCCTGCAA...</td>\n",
       "      <td>AGGAGATCCTCAACGTGGACCCGGTGCAGCACACGTACTCCTGCAA...</td>\n",
       "      <td>1020391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95755</th>\n",
       "      <td>0</td>\n",
       "      <td>X</td>\n",
       "      <td>ACATCATAGCTAAGGTCAAGCAATGGAAGGGGAAAGACAGAGAGAA...</td>\n",
       "      <td>ACATCATAGCTAAGGTCAAGCAATGGAAGGGGAAAGACAGAGAGAA...</td>\n",
       "      <td>154991042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95756</th>\n",
       "      <td>0</td>\n",
       "      <td>X</td>\n",
       "      <td>AAATGCTAACTGTTATAGATAGATTCAGTTGTTTGTACTTCTCTGC...</td>\n",
       "      <td>AAATGCTAACTGTTATAGATAGATTCAGTTGTTTGTACTTCTCTGC...</td>\n",
       "      <td>154992918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95757</th>\n",
       "      <td>0</td>\n",
       "      <td>X</td>\n",
       "      <td>GGAAGACTTTATCATCTTCTTTCTCCCTTTGACTGGTCTGATCATC...</td>\n",
       "      <td>GGAAGACTTTATCATCTTCTTTCTCCCTTTGACTGGTCTGATCATC...</td>\n",
       "      <td>154993156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95758</th>\n",
       "      <td>0</td>\n",
       "      <td>X</td>\n",
       "      <td>GACCACTGTATCATAAACCTCAGCCTGGATGGTAGGACCTAGCAGA...</td>\n",
       "      <td>GACCACTGTATCATAAACCTCAGCCTGGATGGTAGGACCTAGCAGA...</td>\n",
       "      <td>154997113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95759</th>\n",
       "      <td>0</td>\n",
       "      <td>X</td>\n",
       "      <td>TGTACACGACTGAGGTGTTGAATGGAAAAGATTTTGGCACTCTAGG...</td>\n",
       "      <td>TGTACACGACTGAGGTGTTGAATGGAAAAGATTTTGGCACTCTAGG...</td>\n",
       "      <td>154999612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>95760 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label chromosome                               ref_forward_sequence  \\\n",
       "0          0          1  GGACGGTGGAGGAGATCCTCAACGTGGACCCGGTGCAGCACACGTA...   \n",
       "1          0          1  GGAGGAGATCCTCAACGTGGACCCGGTGCAGCACACGTACTCCTGC...   \n",
       "2          0          1  GAGGAGATCCTCAACGTGGACCCGGTGCAGCACACGTACTCCTGCA...   \n",
       "3          0          1  AGGAGATCCTCAACGTGGACCCGGTGCAGCACACGTACTCCTGCAA...   \n",
       "4          0          1  AGGAGATCCTCAACGTGGACCCGGTGCAGCACACGTACTCCTGCAA...   \n",
       "...      ...        ...                                                ...   \n",
       "95755      0          X  ACATCATAGCTAAGGTCAAGCAATGGAAGGGGAAAGACAGAGAGAA...   \n",
       "95756      0          X  AAATGCTAACTGTTATAGATAGATTCAGTTGTTTGTACTTCTCTGC...   \n",
       "95757      0          X  GGAAGACTTTATCATCTTCTTTCTCCCTTTGACTGGTCTGATCATC...   \n",
       "95758      0          X  GACCACTGTATCATAAACCTCAGCCTGGATGGTAGGACCTAGCAGA...   \n",
       "95759      0          X  TGTACACGACTGAGGTGTTGAATGGAAAAGATTTTGGCACTCTAGG...   \n",
       "\n",
       "                                    alt_forward_sequence   position  \n",
       "0      GGACGGTGGAGGAGATCCTCAACGTGGACCCGGTGCAGCACACGTA...    1020382  \n",
       "1      GGAGGAGATCCTCAACGTGGACCCGGTGCAGCACACGTACTCCTGC...    1020389  \n",
       "2      GAGGAGATCCTCAACGTGGACCCGGTGCAGCACACGTACTCCTGCA...    1020390  \n",
       "3      AGGAGATCCTCAACGTGGACCCGGTGCAGCACACGTACTCCTGCAA...    1020391  \n",
       "4      AGGAGATCCTCAACGTGGACCCGGTGCAGCACACGTACTCCTGCAA...    1020391  \n",
       "...                                                  ...        ...  \n",
       "95755  ACATCATAGCTAAGGTCAAGCAATGGAAGGGGAAAGACAGAGAGAA...  154991042  \n",
       "95756  AAATGCTAACTGTTATAGATAGATTCAGTTGTTTGTACTTCTCTGC...  154992918  \n",
       "95757  GGAAGACTTTATCATCTTCTTTCTCCCTTTGACTGGTCTGATCATC...  154993156  \n",
       "95758  GACCACTGTATCATAAACCTCAGCCTGGATGGTAGGACCTAGCAGA...  154997113  \n",
       "95759  TGTACACGACTGAGGTGTTGAATGGAAAAGATTTTGGCACTCTAGG...  154999612  \n",
       "\n",
       "[95760 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_path = \"pathgenicity_noncoding_multisets\"\n",
    "df = load_from_disk(save_path+\"/combined_dataset\")\n",
    "df = df.to_pandas()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ebd9fa-4b90-419b-98ba-1cea30769197",
   "metadata": {},
   "source": [
    "## demo loading from datsetsdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ac4b9c42-be4e-4397-85b8-4a208a5161a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset, load_from_disk\n",
    "\n",
    "# mydatasets = load_from_disk(save_path)\n",
    "# print(mydatasets)\n",
    "\n",
    "# train = mydatasets['train']\n",
    "\n",
    "# df = train.to_pandas()\n",
    "# df\n",
    "# df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d1311d0-2bf6-4376-86f6-2707aacb8536",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

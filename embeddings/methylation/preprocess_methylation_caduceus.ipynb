{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731efd6b-1473-4a43-9f33-84b304573e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "max_length= 128 \n",
    "\n",
    "file_path = '../../datasets/task05-methylation/GSM6637962_CpG_coverage20_GRCh38.bed.gz'     \n",
    "df = pd.read_csv(file_path, sep='\\t')\n",
    "df['ROWID'] = df.index\n",
    "\n",
    "\n",
    "df['CHROM'] = df['CHROM'].str.replace('chr', '', regex=False)\n",
    "df=df[~df['CHROM'].str.contains('KI',na=False)]\n",
    "df=df[~df['CHROM'].str.contains('GL',na=False)]\n",
    "df=df[~df['CHROM'].str.contains('M',na=False)]\n",
    "\n",
    "\n",
    "df['SIZE']=max_length\n",
    "\n",
    "df['START']= df['FROM']-round(max_length / 2)\n",
    "df['END']= df['START'] + df['SIZE']\n",
    "\n",
    "df = df.rename(columns={'Percentage':'y'})\n",
    "df = df.drop(['FROM','TO','Coverage'], axis=1)\n",
    "\n",
    "df_origin = df\n",
    "df_origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2166e9-b2f5-43e5-83ed-d683f39d21ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "sequences = list(SeqIO.parse(\"/blue/xiaofan/chenyuanhan/data/genome.hg38rg.fa\", \"fasta\"))\n",
    "\n",
    "def extract_sequence_segment(seq_id, start, end, sequences):\n",
    "    for seq_record in sequences:\n",
    "        if seq_record.id == seq_id:\n",
    "            segment = str(seq_record.seq[start:end])\n",
    "#            print(segment)\n",
    "            return segment\n",
    "    return None\n",
    "    \n",
    "rows = []\n",
    "for index, row in df_origin.iterrows():\n",
    "    chrom=str(row['CHROM'])\n",
    "    start=int(row['START'])\n",
    "    end=int(row['END'])\n",
    "    y=row['y']\n",
    "    rowid=row['ROWID']\n",
    "    segment = extract_sequence_segment(chrom, start, end, sequences)\n",
    "    rows.append([segment,  y])  # rowid,\n",
    "\n",
    "columns=['sequence','y']  # 'rowid',\n",
    "df_sequence = pd.DataFrame(rows, columns=columns)\n",
    "df_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb45e4f-b2f2-4a50-8d66-8ff598a2e8b5",
   "metadata": {},
   "source": [
    "### Load tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef55f5d-c55e-4318-b832-0601921b8eb3",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# Load pre-trained Caduceus model and tokenizer\n",
    "model_name = \"kuleshov-group/caduceus-ph_seqlen-131k_d_model-256_n_layer-16\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Move model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fab776-63ab-42fb-9139-b983eb14c525",
   "metadata": {},
   "source": [
    "### compute embeddings in batches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b59b383-8e87-4261-9a41-6be9b4404403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import jax.numpy as jnp\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "# Vectorized tokenization function\n",
    "def vectorized_tokenizer(subsequences):\n",
    "    # Tokenize the batch of sequences\n",
    "    tokens = tokenizer(subsequences, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    # Move tokens to GPU\n",
    "    tokens = {key: val.to(device) for key, val in tokens.items()}\n",
    "    return tokens\n",
    "\n",
    "# Vectorized embedding function\n",
    "def vectorized_embedding(tokens):\n",
    "    # Forward pass to compute last layer embeddings for the batch\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens, output_hidden_states=True)  # Enable output of hidden states\n",
    "        hidden_states = outputs.hidden_states  # Access all hidden states\n",
    "        last_layer_embeddings = hidden_states[-1]  # Get the last layer embeddings (batch_size, seq_len, hidden_size)\n",
    "    \n",
    "    # Compute the mean of the last layer embeddings across the token (sequence) dimension for each sequence in the batch\n",
    "    # Dimension 1 corresponds to the token/sequence length, so we compute the mean along this axis\n",
    "    mean_embeddings = torch.mean(last_layer_embeddings, dim=1)  # (batch_size, hidden_size)\n",
    "    \n",
    "    # If needed, squeeze out any extra dimensions (though this shouldn't be necessary after mean calculation)\n",
    "    mean_embeddings_squeezed = mean_embeddings.squeeze(dim=1)\n",
    "\n",
    "    return mean_embeddings_squeezed\n",
    "\n",
    "# Tokenization and embedding combined in a batch-wise function\n",
    "def process_batch(subsequences):\n",
    "    tokens = vectorized_tokenizer(subsequences)\n",
    "    embeddings = vectorized_embedding(tokens)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def apply_get_embeddings_dask(df):\n",
    "    subsequences = df['sequence'].tolist() \n",
    "    embeddings = process_batch(subsequences)  # Process in a vectorized manner\n",
    "    embeddings_cpu = embeddings.cpu().numpy()\n",
    "    \n",
    "    df2 = pd.DataFrame(embeddings_cpu, columns=[f'{i+1}' for i in range(embeddings_cpu.shape[1])])\n",
    "    df = pd.concat([df.reset_index(drop=True), df2.reset_index(drop=True)], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec2bfdc-9725-4a6d-b5fc-2e26be0beb7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "%run preprocess_utility.py\n",
    "\n",
    "df = df_sequence\n",
    "\n",
    "typename=\"methylation\"\n",
    "\n",
    "for chunkid in range(0,1):\n",
    "    \n",
    "    #================df -> df's chunks================\n",
    "    # Assume df is your huge DataFrame\n",
    "    chunk_size = 10000  # Define the number of rows per chunk\n",
    "    num_parallel = 10\n",
    "    \n",
    "    num_chunks = int(np.ceil(len(df) / chunk_size))  # Calculate the number of chunks\n",
    "    \n",
    "    # Split the DataFrame into chunks using array_split\n",
    "    chunks = np.array_split(df, num_chunks)\n",
    "    \n",
    "    # Initialize an empty list to store the processed chunks\n",
    "    processed_chunks = []\n",
    "    \n",
    "    #================process each chunk with dask's ddf================\n",
    "    # Iterate over each chunk\n",
    "    for chunk in chunks:\n",
    "    \n",
    "        ddf = dd.from_pandas(chunk, npartitions=num_parallel)  # Adjust 'npartitions' based on resources\n",
    "    \n",
    "        num_embedding_columns = 256\n",
    "        \n",
    "        # Define metadata for the new DataFrame structure\n",
    "        # Drop the 'embedding' column since it no longer exists\n",
    "        meta = chunk.copy()\n",
    "        meta = meta.drop(columns=['embedding'], errors='ignore')  # Drop 'embedding' if it exists\n",
    "        for i in range(num_embedding_columns):\n",
    "            meta[f'{i+1}'] = float  # Adjust type as necessary, float is common for embeddings\n",
    "\n",
    "\n",
    "        # Apply the function in parallel using Dask\n",
    "        ddf = ddf.map_partitions(apply_get_embeddings_dask, meta=meta)\n",
    "    \n",
    "        # Compute the result with progress tracking\n",
    "        with ProgressBar():\n",
    "            processed_chunk = ddf.compute()\n",
    "    \n",
    "        # Append processed chunk to list\n",
    "        processed_chunks.append(processed_chunk)\n",
    "\n",
    "        \n",
    "    # Concatenate all processed chunks into a final DataFrame\n",
    "    final_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "\n",
    "    final_df = final_df.drop(columns=['sequence'])\n",
    "    final_df = swapfirst2last(final_df)\n",
    "    \n",
    "    final_df.to_csv(typename+'_caduceus_embedding_'+str(chunkid)+'.csv', index=False)\n",
    "    print(f\"{typename}_caduceus_embedding_{chunkid}.csv is created.\")\n",
    "\n",
    "final_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

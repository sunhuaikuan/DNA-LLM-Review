{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95346cfc-20f3-4774-8faf-7d5c16a38ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import jax\n",
    "import haiku as hk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "from nucleotide_transformer.pretrained import get_pretrained_model\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "%run preprocess_utility.py\n",
    "\n",
    "print(jax.devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1caa05-0f9a-4eea-92e0-5264dca56a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import nucleotide_transformer\n",
    "except:\n",
    "    !pip install numpy==1.23.5\n",
    "    !pip install git+https://github.com/instadeepai/nucleotide-transformer@main |tail -n 1\n",
    "    import nucleotide_transformer\n",
    "\n",
    "if \"COLAB_TPU_ADDR\" in os.environ:\n",
    "    from jax.tools import colab_tpu\n",
    "\n",
    "    colab_tpu.setup_tpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79b4634-4739-459f-bbf7-ab2b3d7bf974",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = {'A':1, 'C':2, 'G':3, 'T':4}\n",
    "\n",
    "#@title Select a model\n",
    "#@markdown ---\n",
    "model_name = '500M_human_ref'\n",
    "#@markdown ---\n",
    "\n",
    "# Get pretrained model\n",
    "parameters, forward_fn, tokenizer, config = get_pretrained_model(\n",
    "    model_name=model_name,\n",
    "    embeddings_layers_to_save=(20,),\n",
    "    attention_maps_to_save=((1, 4), (7, 18)),\n",
    "    max_positions=32,\n",
    "    # If the progress bar gets stuck at the start of the model wieghts download,\n",
    "    # you can set verbose=False to download without the progress bar.\n",
    "    verbose=False\n",
    ")\n",
    "forward_fn = hk.transform(forward_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3ddf34-3abf-401b-8a00-945559cd4c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_file = \"../genome.hg38rg.fa\"\n",
    "chrom_sequences = read_fasta(fasta_file)\n",
    "# print(chrom_sequences.keys()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecce3e20-77c6-4afe-a7fc-9738c3023839",
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile='methylation'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1afbb68-fb6f-403c-b330-2c099c50676c",
   "metadata": {},
   "source": [
    "### Load datafile file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9093c3f-7dcc-4f16-9621-98ec9181f2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filename = '../../datasets/task05-methylation/GSM6637962_CpG_coverage20_GRCh38.bed.gz'     \n",
    "df = preprocess_datafile(data_filename)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99fb560-b0fb-4b15-83d7-6c26d93390bf",
   "metadata": {},
   "source": [
    "## Step 1: Obtain sequence section for all data and attach to dataframe df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571451d3-b297-4bf2-8c90-1a70f0824ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "def get_sequencesection(row):\n",
    "\n",
    "    chrom=row['CHROM']\n",
    "    pos_start=row['START']\n",
    "    y=row['y']\n",
    "    if row['SIZE'] % 6 == 0:\n",
    "        length = row['SIZE']\n",
    "    else:\n",
    "        length = 6 * round(row['SIZE'] / 6)\n",
    "    \n",
    "    subsequence = chrom_sequences[str(chrom)][pos_start:pos_start + length]\n",
    "    return subsequence\n",
    "\n",
    "\n",
    "def parallelize_dataframe(df, func, num_partitions=100):\n",
    "    # Split DataFrame into smaller chunks\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = mp.Pool(num_partitions)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "# Define a wrapper function to apply the get_sequencesection function\n",
    "def apply_get_sequencesection(df):\n",
    "    df['dna'] = df.apply(get_sequencesection, axis=1)\n",
    "    return df\n",
    "\n",
    "df_with_dna = df\n",
    "    \n",
    "df_with_dna = parallelize_dataframe(df_with_dna, apply_get_sequencesection)\n",
    "\n",
    "df_with_dna= df_with_dna.drop(columns=['CHROM','START','SIZE'])\n",
    "df_with_dna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6f11c95-b62a-4279-84ba-04733386fb2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_parallel = 10\n",
    "!export XLA_FLAGS=--xla_gpu_force_compilation_parallelism=num_parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7ae04b-72dd-4fa8-9e2f-889136a2629f",
   "metadata": {},
   "source": [
    "## THe below version complete noncoding 95760 in 1 min 29 second "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89514e99-f09f-4987-aae3-37423d84c925",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "# Set number of parallel partitions\n",
    "num_parallel = 10\n",
    "batch_size = 10000  # Set batch size to 64\n",
    "\n",
    "# Vectorized tokenization function\n",
    "def vectorized_tokenizer(subsequences):\n",
    "    tokens_ids = [b[1] for b in tokenizer.batch_tokenize(subsequences)]\n",
    "    return jnp.asarray(tokens_ids, dtype=jnp.int32)\n",
    "\n",
    "# Vectorized embedding function\n",
    "def vectorized_embedding(tokens):\n",
    "    random_key = jax.random.PRNGKey(0)\n",
    "    outs = forward_fn.apply(parameters, random_key, tokens)\n",
    "    return outs[\"embeddings_20\"][:, 0, :]  # Return embeddings\n",
    "\n",
    "# Tokenization and embedding combined in a batch-wise function\n",
    "def truncate_sequences(sequences, max_length=32):\n",
    "    \"\"\"Truncate each sequence to the max_length.\"\"\"\n",
    "    return [seq[:max_length] for seq in sequences]\n",
    "\n",
    "def process_batch(subsequences):\n",
    "    truncated_sequences = truncate_sequences(subsequences, max_length=32)\n",
    "    tokens = vectorized_tokenizer(truncated_sequences)  # Tokenize the subsequences\n",
    "    embeddings = vectorized_embedding(tokens)  # Get embeddings\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to apply on each Dask partition\n",
    "def apply_get_tokens_dask(df):\n",
    "    # Create an empty list to collect embeddings\n",
    "    all_embeddings = []\n",
    "\n",
    "    # Process subsequences in batches\n",
    "    for start in range(0, len(df), batch_size):\n",
    "        end = min(start + batch_size, len(df))\n",
    "        subsequences = df['dna'].values[start:end]  # Get the current batch\n",
    "        embeddings = process_batch(subsequences)  # Process the current batch\n",
    "        all_embeddings.append(embeddings)\n",
    "\n",
    "    # Concatenate all embeddings into a single array\n",
    "    all_embeddings = jnp.concatenate(all_embeddings, axis=0)\n",
    "\n",
    "    # Create a DataFrame from embeddings\n",
    "    embedding_df = pd.DataFrame(all_embeddings, columns=[str(i) for i in range(1, 1281)])\n",
    "\n",
    "    # Concatenate the original DataFrame with the embedding DataFrame\n",
    "    df = pd.concat([df.reset_index(drop=True), embedding_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "# df = pd.read_csv(datafile + '_with_dna.csv')\n",
    "df = df_with_dna\n",
    "\n",
    "# Convert the Pandas DataFrame to a Dask DataFrame\n",
    "ddf = dd.from_pandas(df, npartitions=num_parallel)  # Adjust 'npartitions' based on resources\n",
    "\n",
    "# Create metadata for Dask to understand the structure of the DataFrame\n",
    "meta = df.copy()\n",
    "for i in range(1, 1281):\n",
    "    meta[str(i)] = np.ndarray  # Prepare for 1280 embedding columns\n",
    "\n",
    "# Apply the function in parallel using Dask\n",
    "ddf = ddf.map_partitions(apply_get_tokens_dask, meta=meta)\n",
    "\n",
    "# Compute the result with progress tracking\n",
    "with ProgressBar():\n",
    "    df_top = ddf.compute()\n",
    "\n",
    "df_top= df_top.drop(columns=['dna'])\n",
    "df_top= swapfirst2last(df_top)\n",
    "\n",
    "# Save the results to CSV\n",
    "df_top.to_csv(datafile + '_nt_embedding.csv', index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_top.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3898cb2f-e6bb-4091-84e7-596650e413cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(datafile+'_nt_embedding.csv')\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

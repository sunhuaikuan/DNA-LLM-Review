{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "851684bf-dc13-4510-b34b-dbcb7a92efa9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/pytorch/2.2.0/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in links: https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
      "Requirement already satisfied: jax[cuda] in /home/sunhuaikuan/.local/lib/python3.10/site-packages (0.4.34)\n",
      "Requirement already satisfied: jaxlib<=0.4.34,>=0.4.34 in /home/sunhuaikuan/.local/lib/python3.10/site-packages (from jax[cuda]) (0.4.34)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /home/sunhuaikuan/.local/lib/python3.10/site-packages (from jax[cuda]) (0.4.0)\n",
      "Requirement already satisfied: numpy>=1.24 in /home/sunhuaikuan/.local/lib/python3.10/site-packages (from jax[cuda]) (1.26.4)\n",
      "Requirement already satisfied: opt-einsum in /home/sunhuaikuan/.local/lib/python3.10/site-packages (from jax[cuda]) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.10 in /home/sunhuaikuan/.local/lib/python3.10/site-packages (from jax[cuda]) (1.11.4)\n",
      "Requirement already satisfied: jax-cuda12-plugin<=0.4.34,>=0.4.34 in /home/sunhuaikuan/.local/lib/python3.10/site-packages (from jax-cuda12-plugin[with_cuda]<=0.4.34,>=0.4.34; extra == \"cuda\"->jax[cuda]) (0.4.34)\n",
      "Requirement already satisfied: jax-cuda12-pjrt==0.4.34 in /home/sunhuaikuan/.local/lib/python3.10/site-packages (from jax-cuda12-plugin<=0.4.34,>=0.4.34->jax-cuda12-plugin[with_cuda]<=0.4.34,>=0.4.34; extra == \"cuda\"->jax[cuda]) (0.4.34)\n",
      "Requirement already satisfied: nvidia-cublas-cu12>=12.1.3.1 in /home/sunhuaikuan/.local/lib/python3.10/site-packages (from jax-cuda12-plugin[with_cuda]<=0.4.34,>=0.4.34; extra == \"cuda\"->jax[cuda]) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12>=12.1.105 in /home/sunhuaikuan/.local/lib/python3.10/site-packages (from jax-cuda12-plugin[with_cuda]<=0.4.34,>=0.4.34; extra == \"cuda\"->jax[cuda]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-nvcc-cu12>=12.1.105 in /home/sunhuaikuan/.local/lib/python3.10/site-packages (from jax-cuda12-plugin[with_cuda]<=0.4.34,>=0.4.34; extra == \"cuda\"->jax[cuda]) (12.6.68)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12>=12.1.105 in /home/sunhuaikuan/.local/lib/python3.10/site-packages (from jax-cuda12-plugin[with_cuda]<=0.4.34,>=0.4.34; extra == \"cuda\"->jax[cuda]) (12.1.105)\n",
      "Collecting nvidia-cudnn-cu12<10.0,>=9.1 (from jax-cuda12-plugin[with_cuda]<=0.4.34,>=0.4.34; extra == \"cuda\"->jax[cuda])\n",
      "  Using cached nvidia_cudnn_cu12-9.5.0.50-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cufft-cu12>=11.0.2.54 in /home/sunhuaikuan/.local/lib/python3.10/site-packages (from jax-cuda12-plugin[with_cuda]<=0.4.34,>=0.4.34; extra == \"cuda\"->jax[cuda]) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12>=11.4.5.107 in /home/sunhuaikuan/.local/lib/python3.10/site-packages (from jax-cuda12-plugin[with_cuda]<=0.4.34,>=0.4.34; extra == \"cuda\"->jax[cuda]) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12>=12.1.0.106 in /home/sunhuaikuan/.local/lib/python3.10/site-packages (from jax-cuda12-plugin[with_cuda]<=0.4.34,>=0.4.34; extra == \"cuda\"->jax[cuda]) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12>=2.18.1 in /home/sunhuaikuan/.local/lib/python3.10/site-packages (from jax-cuda12-plugin[with_cuda]<=0.4.34,>=0.4.34; extra == \"cuda\"->jax[cuda]) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12>=12.1.105 in /home/sunhuaikuan/.local/lib/python3.10/site-packages (from jax-cuda12-plugin[with_cuda]<=0.4.34,>=0.4.34; extra == \"cuda\"->jax[cuda]) (12.5.40)\n",
      "Using cached nvidia_cudnn_cu12-9.5.0.50-py3-none-manylinux2014_x86_64.whl (562.9 MB)\n",
      "Installing collected packages: nvidia-cudnn-cu12\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 8.9.2.26\n",
      "    Uninstalling nvidia-cudnn-cu12-8.9.2.26:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-8.9.2.26\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "torch 2.3.0 requires nvidia-cudnn-cu12==8.9.2.26; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.5.0.50 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed nvidia-cudnn-cu12-9.5.0.50\n"
     ]
    }
   ],
   "source": [
    "# !pip install --upgrade jax[cuda] -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95346cfc-20f3-4774-8faf-7d5c16a38ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunhuaikuan/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CudaDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import jax\n",
    "import haiku as hk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# import torch\n",
    "# import pickle\n",
    "# import swifter\n",
    "# import seaborn as sns\n",
    "# from tqdm import tqdm\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "from nucleotide_transformer.pretrained import get_pretrained_model\n",
    "\n",
    "# device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# device\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "%run preprocess_utility.py\n",
    "\n",
    "print(jax.devices())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "544f3d5e-91fe-47e6-b3e0-56d6b56f2e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade \"jax[cuda]\" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html\n",
    "# !pip install git+https://github.com/deepmind/dm-haiku\n",
    "\n",
    "\n",
    "# import jax\n",
    "# print(jax.devices())\n",
    "# !pip install --upgrade pip\n",
    "\n",
    "# !pip install opt_einsum\n",
    "# !pip install swifter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed1caa05-0f9a-4eea-92e0-5264dca56a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import nucleotide_transformer\n",
    "except:\n",
    "    !pip install numpy==1.23.5\n",
    "    !pip install git+https://github.com/instadeepai/nucleotide-transformer@main |tail -n 1\n",
    "    import nucleotide_transformer\n",
    "\n",
    "if \"COLAB_TPU_ADDR\" in os.environ:\n",
    "    from jax.tools import colab_tpu\n",
    "\n",
    "    colab_tpu.setup_tpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c79b4634-4739-459f-bbf7-ab2b3d7bf974",
   "metadata": {},
   "outputs": [],
   "source": [
    "comp = {'A':1, 'C':2, 'G':3, 'T':4}\n",
    "\n",
    "#@title Select a model\n",
    "#@markdown ---\n",
    "model_name = '50M_multi_species_v2'#@param['500M_human_ref', '500M_1000G', '2B5_1000G', '2B5_multi_species', '50M_multi_species_v2', '100M_multi_species_v2', '250M_multi_species_v2', '500M_multi_species_v2', '1B_agro_nt']\n",
    "model_name = '500M_human_ref'#@param['500M_human_ref', '500M_1000G', '2B5_1000G', '2B5_multi_species', '50M_multi_species_v2', '100M_multi_species_v2', '250M_multi_species_v2', '500M_multi_species_v2', '1B_agro_nt']\n",
    "#@markdown ---\n",
    "\n",
    "# Get pretrained model\n",
    "parameters, forward_fn, tokenizer, config = get_pretrained_model(\n",
    "    model_name=model_name,\n",
    "    embeddings_layers_to_save=(20,),\n",
    "    attention_maps_to_save=((1, 4), (7, 18)),\n",
    "    max_positions=32,\n",
    "    # If the progress bar gets stuck at the start of the model wieghts download,\n",
    "    # you can set verbose=False to download without the progress bar.\n",
    "    verbose=False\n",
    ")\n",
    "forward_fn = hk.transform(forward_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f003ded8-e270-40e7-a9b9-5ecdd78d70e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get data and tokenize it\n",
    "# sequences = [\n",
    "#     \"ATTCCGAAATCGCTGACCGATCGTACGAAA\",\n",
    "#     \"ATTTCTCTCTCTCTCTGAGATCGATCGATCGATATCTCTCGAGCTAGC\",\n",
    "# ]\n",
    "# tokens_ids = [b[1] for b in tokenizer.batch_tokenize(sequences)]\n",
    "# tokens_str = [b[0] for b in tokenizer.batch_tokenize(sequences)]\n",
    "# tokens = jnp.asarray(tokens_ids, dtype=jnp.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "878bafd3-7e91-4228-9f2b-1ac1729198cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install jax jaxlib==0.1.87+cuda110 -f https://storage.googleapis.com/jax-releases/jax_releases.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b3ddf34-3abf-401b-8a00-945559cd4c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_file = \"/blue/xiaofan/sunhuaikuan/gpn/examples/msa/genome.hg38rg.fa\"\n",
    "chrom_sequences = read_fasta(fasta_file)\n",
    "# print(chrom_sequences.keys()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecce3e20-77c6-4afe-a7fc-9738c3023839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datafile='clinvar_20240805.noncoding'\n",
    "# datafile='clinvar_20240805.missense_matched'\n",
    "datafile='methylation'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1afbb68-fb6f-403c-b330-2c099c50676c",
   "metadata": {},
   "source": [
    "### input methylation datafile file, </br> output methylation_embedding.csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9093c3f-7dcc-4f16-9621-98ec9181f2b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CHROM</th>\n",
       "      <th>START</th>\n",
       "      <th>SIZE</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>788407</td>\n",
       "      <td>128</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>788472</td>\n",
       "      <td>128</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>788825</td>\n",
       "      <td>128</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>788844</td>\n",
       "      <td>128</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>788859</td>\n",
       "      <td>128</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54789</th>\n",
       "      <td>Y</td>\n",
       "      <td>56870903</td>\n",
       "      <td>128</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54790</th>\n",
       "      <td>Y</td>\n",
       "      <td>56870983</td>\n",
       "      <td>128</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54791</th>\n",
       "      <td>Y</td>\n",
       "      <td>56871109</td>\n",
       "      <td>128</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54792</th>\n",
       "      <td>Y</td>\n",
       "      <td>56871502</td>\n",
       "      <td>128</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54793</th>\n",
       "      <td>Y</td>\n",
       "      <td>56873899</td>\n",
       "      <td>128</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47953 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CHROM     START  SIZE    y\n",
       "0         1    788407   128  100\n",
       "1         1    788472   128   80\n",
       "2         1    788825   128   95\n",
       "3         1    788844   128   90\n",
       "4         1    788859   128  100\n",
       "...     ...       ...   ...  ...\n",
       "54789     Y  56870903   128   65\n",
       "54790     Y  56870983   128   17\n",
       "54791     Y  56871109   128   34\n",
       "54792     Y  56871502   128    0\n",
       "54793     Y  56873899   128   36\n",
       "\n",
       "[47953 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filename = '/home/sunhuaikuan/ondemand/blue_papers/DNA_LLM_REVIEW/datasets/task05-methylation/GSM6637962_CpG_coverage20_GRCh38.bed.gz' \n",
    "df = preprocess_datafile(data_filename)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d220cc5f-c7d2-48a1-982e-c03aa2dc7f79",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# # sub_df = pd.DataFrame()    \n",
    "# # final_df = pd.DataFrame()\n",
    "# # segment=800\n",
    "\n",
    "\n",
    "# # csv_Filename = './clinvar_20240805_noncoding_embeddings_nt.csv'\n",
    "# # csv_Filename = './clinvar_20240805_missense_embeddings_nt.csv'\n",
    "# # if os.path.exists(csv_Filename):\n",
    "#     # os.remove(csv_Filename)\n",
    "\n",
    "# max_length= 128 # 186\n",
    "\n",
    "\n",
    "\n",
    "# df=pd.read_csv(datafile+'.txt', delimiter='\\t')\n",
    "# # df=pd.read_csv('clinvar_20240805.missense_matched.txt', delimiter='\\t')\n",
    "\n",
    "# columns_to_keep=['CHROM','POS','ID','REF','ALT','Pathogenicity']\n",
    "# df = df[columns_to_keep]\n",
    "\n",
    "# # Merge CHROM=9 and '9' etc\n",
    "\n",
    "# for i in range(1,23):\n",
    "#     df.loc[df['CHROM']==str(i),'CHROM']=i\n",
    "\n",
    "\n",
    "# df['START']= df['POS']- max_length //2 -1\n",
    "# df['END']  = df['START'] + max_length\n",
    "# df['SIZE'] = max_length\n",
    "\n",
    "# df=df[~df['CHROM'].isna()]\n",
    "\n",
    "# Pathogenicity_dict={'B':0,'P':1}\n",
    "# df['y'] = df['Pathogenicity'].map(Pathogenicity_dict)\n",
    "# # df['CHROM'].value_counts()\n",
    "# df.drop(['POS','ID','Pathogenicity','END'], axis=1, inplace=True)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d2136a-e84a-410d-8859-6acc95a635be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e99fb560-b0fb-4b15-83d7-6c26d93390bf",
   "metadata": {},
   "source": [
    "## Step 1: Obtain sequence section for all data and attach to dataframe df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "571451d3-b297-4bf2-8c90-1a70f0824ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunhuaikuan/.local/lib/python3.10/site-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n",
      "/apps/pytorch/2.2.0/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n",
      "/apps/pytorch/2.2.0/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  self.pid = os.fork()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 123 ms, sys: 3.63 s, total: 3.75 s\n",
      "Wall time: 8.88 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>dna</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>CGATGAGAGACAGAGAGAAGGAGAGAGAAAGTACAAAAGAACGAAT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>80</td>\n",
       "      <td>CGAGCAGGAACCTTGGAGGACCTATTGCTTAAGGTGTGGGCCAAAG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>95</td>\n",
       "      <td>CGAATGGAATGGAACGGAACGGAATGGAATGGAACGCACTCGAATG...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>90</td>\n",
       "      <td>CGGAATGGAATGGAACGCACTCGAATGGAATGGAACGGACATGAAT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100</td>\n",
       "      <td>CGCACTCGAATGGAATGGAACGGACATGAATGGAATGGAATGGAAC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54789</th>\n",
       "      <td>65</td>\n",
       "      <td>GGCCTCAAGGAATAGGAGTGCTGGGTCTATCAGTCTCTCCCAACCA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54790</th>\n",
       "      <td>17</td>\n",
       "      <td>GAGCTCACTCTTGGATCAGATGGGGCCTCCCTGCCAGCAGTTCTAC...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54791</th>\n",
       "      <td>34</td>\n",
       "      <td>GGTGACTCAAACCTTCAGGCAGTGAAAATAAGGAGACTCACAAACT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54792</th>\n",
       "      <td>0</td>\n",
       "      <td>GGCCCTGTGAACATGCTGGTGACAGTGTGAGCAGGCAGGGCTGGTT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54793</th>\n",
       "      <td>36</td>\n",
       "      <td>GCACAGATGCCGTTGAGTATATCGTCTCCTCCTTCTGTGCAGGGAG...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47953 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         y                                                dna\n",
       "0      100  CGATGAGAGACAGAGAGAAGGAGAGAGAAAGTACAAAAGAACGAAT...\n",
       "1       80  CGAGCAGGAACCTTGGAGGACCTATTGCTTAAGGTGTGGGCCAAAG...\n",
       "2       95  CGAATGGAATGGAACGGAACGGAATGGAATGGAACGCACTCGAATG...\n",
       "3       90  CGGAATGGAATGGAACGCACTCGAATGGAATGGAACGGACATGAAT...\n",
       "4      100  CGCACTCGAATGGAATGGAACGGACATGAATGGAATGGAATGGAAC...\n",
       "...    ...                                                ...\n",
       "54789   65  GGCCTCAAGGAATAGGAGTGCTGGGTCTATCAGTCTCTCCCAACCA...\n",
       "54790   17  GAGCTCACTCTTGGATCAGATGGGGCCTCCCTGCCAGCAGTTCTAC...\n",
       "54791   34  GGTGACTCAAACCTTCAGGCAGTGAAAATAAGGAGACTCACAAACT...\n",
       "54792    0  GGCCCTGTGAACATGCTGGTGACAGTGTGAGCAGGCAGGGCTGGTT...\n",
       "54793   36  GCACAGATGCCGTTGAGTATATCGTCTCCTCCTTCTGTGCAGGGAG...\n",
       "\n",
       "[47953 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import multiprocessing as mp\n",
    "\n",
    "\n",
    "def get_sequencesection(row):\n",
    "\n",
    "    chrom=row['CHROM']\n",
    "    # pos_start=row['POS']-93\n",
    "    pos_start=row['START']\n",
    "    # ref=row['REF']\n",
    "    # alt=row['ALT']        \n",
    "    y=row['y']\n",
    "    if row['SIZE'] % 6 == 0:\n",
    "        length = row['SIZE']\n",
    "    else:\n",
    "        length = 6 * round(row['SIZE'] / 6)\n",
    "    \n",
    "    # chrom = 'chr' + chrom if chrom.isdigit() else chrom\n",
    "    # subsequence = get_subsequence(str(chrom), pos_start, length)\n",
    "    subsequence = chrom_sequences[str(chrom)][pos_start:pos_start + length]\n",
    "    return subsequence\n",
    "\n",
    "\n",
    "def parallelize_dataframe(df, func, num_partitions=100):\n",
    "    # Split DataFrame into smaller chunks\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = mp.Pool(num_partitions)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "# Define a wrapper function to apply the get_sequencesection function\n",
    "def apply_get_sequencesection(df):\n",
    "    df['dna'] = df.apply(get_sequencesection, axis=1)\n",
    "    return df\n",
    "\n",
    "\n",
    "# df_top = df.head(1000)\n",
    "df_with_dna = df\n",
    "    \n",
    "df_with_dna = parallelize_dataframe(df_with_dna, apply_get_sequencesection)\n",
    "\n",
    "# Save the DataFrame 'df' to a CSV file\n",
    "# df_with_dna.to_csv(datafile+'_with_dna.csv', index=False)\n",
    "df_with_dna= df_with_dna.drop(columns=['CHROM','START','SIZE'])\n",
    "df_with_dna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b54df5ce-62b3-40a8-9e2f-19cada2e97d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_top[df_top['START']==788407]['dna'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6f11c95-b62a-4279-84ba-04733386fb2c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/apps/pytorch/2.2.0/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    }
   ],
   "source": [
    "num_parallel = 10\n",
    "!export XLA_FLAGS=--xla_gpu_force_compilation_parallelism=num_parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7ae04b-72dd-4fa8-9e2f-889136a2629f",
   "metadata": {},
   "source": [
    "## THe below version complete noncoding 95760 in 1 min 29 second "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2c6554c-ea68-4aca-aee3-74dff4cc34df",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# num_parallel = 20\n",
    "# !export XLA_FLAGS=--xla_gpu_force_compilation_parallelism=num_parallel\n",
    "\n",
    "# import dask.dataframe as dd\n",
    "# import pandas as pd\n",
    "# import jax.numpy as jnp\n",
    "# from dask.diagnostics import ProgressBar\n",
    "\n",
    "# # Vectorized tokenization function\n",
    "# def vectorized_tokenizer(subsequences):\n",
    "#     # Assuming tokenizer.batch_tokenize can handle a list of subsequences\n",
    "#     tokens_ids = [b[1] for b in tokenizer.batch_tokenize(subsequences)]\n",
    "#     return jnp.asarray(tokens_ids, dtype=jnp.int32)\n",
    "\n",
    "# # Vectorized embedding function\n",
    "# def vectorized_embedding(tokens):\n",
    "#     random_key = jax.random.PRNGKey(0)\n",
    "#     # Assuming forward_fn.apply() can handle batches of tokens\n",
    "#     outs = forward_fn.apply(parameters, random_key, tokens)\n",
    "#     return outs[\"embeddings_20\"][:, 0, :]\n",
    "\n",
    "# # Tokenization and embedding combined in a batch-wise function\n",
    "# def process_batch(subsequences):\n",
    "#     tokens = vectorized_tokenizer(subsequences)\n",
    "#     embeddings = vectorized_embedding(tokens)\n",
    "#     return embeddings\n",
    "\n",
    "# # Function to apply on each Dask partition\n",
    "# def apply_get_tokens_dask(df):\n",
    "#     subsequences = df['dna'].values  # Get all subsequences in the batch\n",
    "#     embeddings = process_batch(subsequences)  # Process in a vectorized manner\n",
    "    \n",
    "#     df['embedding'] = list(embeddings)  # Assign embeddings back to the DataFrame\n",
    "#     # df['embedding'] =embeddings  # Assign embeddings back to the DataFrame\n",
    "#     # embedding_df = pd.DataFrame(embeddings, columns=[str(i) for i in range(0, 1280)])\n",
    "#     # df = pd.concat([df, embedding_df], axis=1)\n",
    "    \n",
    "#     return df\n",
    "\n",
    "# # Load the CSV file into a Pandas DataFrame\n",
    "# df = pd.read_csv(datafile + '_with_dna.csv')\n",
    "\n",
    "# # For demonstration, process a small subset, but this will work for larger datasets\n",
    "# # df =df.head(10000)\n",
    "\n",
    "# # Convert the Pandas DataFrame to a Dask DataFrame\n",
    "# ddf = dd.from_pandas(df, npartitions=num_parallel)  # Adjust 'npartitions' based on resources\n",
    "\n",
    "# # Create metadata for Dask to understand the structure of the DataFrame\n",
    "# meta = df.copy()\n",
    "# meta['embedding'] = object  # Tokens will be of type object (adjust type accordingly)\n",
    "# # meta['embedding'] = np.ndarray  # Tokens will be of type object (adjust type accordingly)\n",
    "\n",
    "# # Apply the function in parallel using Dask\n",
    "# ddf = ddf.map_partitions(apply_get_tokens_dask, meta=meta)\n",
    "\n",
    "# # Compute the result with progress tracking\n",
    "# with ProgressBar():\n",
    "#     df_top = ddf.compute()\n",
    "\n",
    "# df_top.to_csv(datafile+'_with_dna'+'_with_embedding.csv', index=False)\n",
    "# df_top\n",
    "# # print(df_top.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "89514e99-f09f-4987-aae3-37423d84c925",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "<timed exec>:70: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                        ] | 0% Completed | 776.54 ms"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-31 16:33:20.194388: W external/xla/xla/service/gpu/nvptx_compiler.cc:930] The NVIDIA driver's CUDA version is 12.4 which is older than the PTX compiler version 12.6.68. Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                        ] | 0% Completed | 2.22 s msBefore attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.2840297   0.4719833  -2.2279875  ... -0.1427693   0.6460447\n",
      "   -2.1994483 ]\n",
      "  [ 0.62872744  1.1597593   0.4775821  ...  1.3492723   0.6983742\n",
      "   -1.6324239 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]\n",
      "\n",
      " [[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.9324149   0.52861655 -1.6110799  ... -0.6671132   0.90874654\n",
      "   -2.2312975 ]\n",
      "  [-1.1377717   2.135611   -0.28993845 ... -0.16258729  0.6702845\n",
      "   -0.18329185]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]\n",
      "\n",
      " [[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.03945142  0.29913372 -2.6876822  ... -1.1123767  -0.8584865\n",
      "   -0.43751985]\n",
      "  [-0.98873067  0.45005095  0.9823558  ...  0.28199804  0.785151\n",
      "   -1.1815039 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.6752423   0.66393006 -0.54584044 ... -0.17925128 -0.634591\n",
      "   -1.8646904 ]\n",
      "  [ 0.7814847   1.9471185   1.3258171  ...  0.93342996  0.8764138\n",
      "   -1.377321  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]\n",
      "\n",
      " [[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.9511638   0.40679783 -1.6066558  ... -1.2187393  -1.4957452\n",
      "    0.09809917]\n",
      "  [ 0.7859607  -0.33984625  2.101737   ...  1.4828024   1.5713067\n",
      "   -2.040255  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]\n",
      "\n",
      " [[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.796682    1.2564366  -1.6184776  ... -0.56688184  0.16534835\n",
      "   -1.886595  ]\n",
      "  [-0.17375278 -0.10076034  1.5810585  ...  0.335765    0.28419122\n",
      "   -1.6057639 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "[###                                     ] | 9% Completed | 6.80 sBefore attention blocks: [[[-2.1331871e-01 -3.7896746e-01  9.8948789e-01 ... -1.4485674e+00\n",
      "    2.9604977e-01 -8.1329250e-01]\n",
      "  [-1.1925986e+00  1.7798164e+00 -9.8816997e-01 ... -6.0704815e-01\n",
      "   -7.0413518e-01 -2.1075783e+00]\n",
      "  [-1.2710340e-02  1.0694885e+00  1.1395538e-01 ... -2.8300166e-02\n",
      "    1.3587140e+00 -2.2316630e+00]\n",
      "  ...\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]]\n",
      "\n",
      " [[-2.1331871e-01 -3.7896746e-01  9.8948789e-01 ... -1.4485674e+00\n",
      "    2.9604977e-01 -8.1329250e-01]\n",
      "  [ 2.9223502e-02  6.2215215e-01 -2.4038727e+00 ... -1.7890149e+00\n",
      "   -7.2031748e-01 -1.5607831e+00]\n",
      "  [-5.3924322e-04  2.5846519e+00 -1.8964267e-01 ...  5.2873224e-01\n",
      "   -5.9349561e-01 -1.1345985e+00]\n",
      "  ...\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]]\n",
      "\n",
      " [[-2.1331871e-01 -3.7896746e-01  9.8948789e-01 ... -1.4485674e+00\n",
      "    2.9604977e-01 -8.1329250e-01]\n",
      "  [-4.2130101e-01  1.2778744e+00 -1.9389675e+00 ...  4.7047308e-01\n",
      "    5.6145787e-01 -1.1401110e+00]\n",
      "  [-1.7852803e-01  7.0317823e-01  9.1085464e-01 ...  9.2303050e-01\n",
      "    7.1314776e-01 -1.3587255e+00]\n",
      "  ...\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-2.1331871e-01 -3.7896746e-01  9.8948789e-01 ... -1.4485674e+00\n",
      "    2.9604977e-01 -8.1329250e-01]\n",
      "  [-9.7265083e-01  1.0230451e+00  1.6883838e-01 ... -8.9509726e-01\n",
      "    2.8358942e-01 -1.9283453e+00]\n",
      "  [-6.2701368e-01  7.4518299e-01  1.0337565e+00 ...  1.0853660e+00\n",
      "    1.7752097e+00 -1.8995054e+00]\n",
      "  ...\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]]\n",
      "\n",
      " [[-2.1331871e-01 -3.7896746e-01  9.8948789e-01 ... -1.4485674e+00\n",
      "    2.9604977e-01 -8.1329250e-01]\n",
      "  [-2.5538661e+00  2.1275908e-01 -1.6284959e+00 ...  1.4032468e-01\n",
      "   -5.0512534e-01 -7.1239030e-01]\n",
      "  [-1.5182906e-01  3.7161553e-01  1.8657194e+00 ...  1.5916004e+00\n",
      "    4.8774171e-01 -7.2662091e-01]\n",
      "  ...\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]]\n",
      "\n",
      " [[-2.1331871e-01 -3.7896746e-01  9.8948789e-01 ... -1.4485674e+00\n",
      "    2.9604977e-01 -8.1329250e-01]\n",
      "  [-2.2803068e-02  9.7180009e-01 -6.5686536e-01 ...  5.4753506e-01\n",
      "   -6.6119224e-01 -6.7481744e-01]\n",
      "  [ 3.7357768e-01  1.6077037e+00 -1.6572750e-01 ...  1.0049746e+00\n",
      "    1.6420245e+00 -6.9073862e-01]\n",
      "  ...\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]]]\n",
      "[#######                                 ] | 18% Completed | 10.10 sBefore attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.21550006  2.5058374  -1.613018   ...  0.53859484 -0.59857327\n",
      "   -1.8081038 ]\n",
      "  [-1.5337235   1.6342543   0.98602474 ...  1.0245014   1.0557551\n",
      "   -0.5938839 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]\n",
      "\n",
      " [[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.9217943   1.221881   -0.25649703 ... -2.0052395  -1.1259001\n",
      "   -0.8304839 ]\n",
      "  [-0.7180816   1.957717    1.1562198  ... -1.0682256   0.08641954\n",
      "   -0.76203656]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]\n",
      "\n",
      " [[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.5108376   1.4906743  -1.1274405  ...  0.6044444   1.0219147\n",
      "   -0.41057476]\n",
      "  [ 0.03330087  1.6620357   0.5030789  ... -0.88412625 -0.2606898\n",
      "   -1.015313  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.35833472  2.0043252  -2.0992348  ... -0.611251   -0.64094806\n",
      "    0.38768244]\n",
      "  [ 0.72181594  1.5166299   2.2381558  ...  1.5544152   1.0927538\n",
      "   -0.39565158]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]\n",
      "\n",
      " [[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.4395722   2.6529226  -1.6241815  ... -0.72291243 -1.6560204\n",
      "    0.25053513]\n",
      "  [-1.1143694   2.8963196   1.8304468  ... -0.60536903  1.0534897\n",
      "   -2.1638799 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]\n",
      "\n",
      " [[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.5331196   1.4113605  -2.2152073  ...  0.16735753 -0.81919837\n",
      "   -1.516671  ]\n",
      "  [-0.09430167  1.3864594   0.68325037 ...  1.0708892   1.8940198\n",
      "   -0.25707245]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "[##########                              ] | 27% Completed | 13.24 sBefore attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.18083817  1.7118467  -0.9621138  ... -0.82511055 -1.1746383\n",
      "   -1.0641737 ]\n",
      "  [-0.1959223   1.7707274   2.0495472  ... -0.04985321  0.61623687\n",
      "   -1.2889928 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]\n",
      "\n",
      " [[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1095473  -0.57307637 -0.4778099  ... -1.2234824  -0.4315369\n",
      "   -0.54016775]\n",
      "  [ 0.11972035  1.9203379   1.5490978  ... -0.15067405  1.665161\n",
      "   -0.599663  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]\n",
      "\n",
      " [[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.0447638   2.7705822  -0.74476063 ... -1.5400289   0.4779914\n",
      "   -1.7786087 ]\n",
      "  [ 0.6142013   1.4549665   0.812459   ...  0.2327384   1.5829571\n",
      "   -2.181303  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.33015686  1.2888346  -2.3217955  ...  0.46311638 -0.5204026\n",
      "    0.4141674 ]\n",
      "  [ 0.21767437  0.63926464  1.046944   ...  1.5613005   1.2782571\n",
      "   -1.0808554 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]\n",
      "\n",
      " [[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.2404773   2.7059026  -1.3592575  ... -0.6151367  -0.01120239\n",
      "   -0.9697458 ]\n",
      "  [-0.40240297  0.89016426  0.7569593  ... -0.0409067  -0.61306936\n",
      "   -0.9999673 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]\n",
      "\n",
      " [[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1768852   1.1585985  -0.8856091  ...  0.9843645   0.6323133\n",
      "   -1.9557738 ]\n",
      "  [ 0.03330087  1.6620357   0.5030789  ... -0.88412625 -0.2606898\n",
      "   -1.015313  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "[##############                          ] | 36% Completed | 16.39 sBefore attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.65630066  0.9399086  -1.1149291  ... -0.30026382 -0.5412977\n",
      "   -0.70768297]\n",
      "  [ 1.1354284   1.3129804   0.5567646  ...  0.769516    0.60651493\n",
      "   -1.9698784 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]\n",
      "\n",
      " [[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.40530068  0.92961836 -2.3248825  ... -1.1109565   0.29706764\n",
      "    0.5257338 ]\n",
      "  [ 0.9732909   0.6769376   1.5126927  ...  1.4282933   0.67760956\n",
      "   -2.6190143 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]\n",
      "\n",
      " [[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.048182    0.38523388 -1.2240767  ...  0.87202656  0.94436806\n",
      "   -1.282588  ]\n",
      "  [-0.41408387  1.6776766   2.0435872  ...  0.57902783  1.2319646\n",
      "   -1.7927848 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.9471037   1.7572545  -1.4228324  ... -1.881281   -1.2184174\n",
      "   -2.2957401 ]\n",
      "  [-1.116275    1.4850446   0.17998576 ...  0.49213573  0.30449194\n",
      "    0.4883834 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]\n",
      "\n",
      " [[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.20323247  0.5089481  -2.2643952  ... -0.594947   -0.25142914\n",
      "   -1.7095168 ]\n",
      "  [ 1.4668233   2.6397169   1.0114225  ...  0.4580864  -0.38993502\n",
      "   -2.1959224 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]\n",
      "\n",
      " [[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.18105751  0.22218382 -1.3053768  ... -0.7302741   0.27421385\n",
      "   -2.1090894 ]\n",
      "  [-0.20458056  0.23781544  2.4389148  ... -0.24632704 -0.6343957\n",
      "   -1.6732795 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "[##################                      ] | 45% Completed | 19.54 sBefore attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.208125    1.114202   -1.574829   ...  0.46466634  0.13076448\n",
      "   -1.3055153 ]\n",
      "  [-0.87902033  2.0414224   1.3744669  ...  1.1634359   0.7449677\n",
      "   -1.5699509 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]\n",
      "\n",
      " [[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.9890853   2.2027574  -0.8418343  ... -1.4692633  -0.03899658\n",
      "   -2.0706463 ]\n",
      "  [ 0.23496002  0.87615246  1.5660287  ... -0.6224192   0.34148374\n",
      "    0.25242627]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]\n",
      "\n",
      " [[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.26161915  0.96716154 -1.6425511  ...  0.51893663 -0.94486064\n",
      "   -0.7713322 ]\n",
      "  [ 0.5498159   2.8226576   2.4506078  ...  1.5271473   0.9542951\n",
      "   -0.51323247]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.04289645  0.55120003 -1.0625148  ...  0.49363342  0.10211122\n",
      "   -2.233743  ]\n",
      "  [-0.45368773  1.0272056   2.3462012  ... -0.07550985 -0.41387892\n",
      "   -2.0730665 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]\n",
      "\n",
      " [[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.65630066  0.9399086  -1.1149291  ... -0.30026382 -0.5412977\n",
      "   -0.70768297]\n",
      "  [ 0.10031283  2.0745263   1.679052   ...  0.98192066  0.4761929\n",
      "   -0.6416507 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]\n",
      "\n",
      " [[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.59790087  1.4679044  -1.1623611  ... -1.2400624  -1.2112861\n",
      "    0.08844197]\n",
      "  [-0.5047802   0.32044888  1.120076   ... -0.49583906 -0.5916883\n",
      "   -1.3940599 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "[#####################                   ] | 54% Completed | 22.82 sBefore attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.6448106   1.777813   -1.6941803  ... -1.8100808  -0.9575447\n",
      "    0.07309741]\n",
      "  [ 0.08840987  0.13186419  2.2046194  ... -0.92737395  1.2459682\n",
      "   -1.667414  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]\n",
      "\n",
      " [[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.2636336   2.6387916  -2.356187   ... -1.0446856   0.00976294\n",
      "   -1.517488  ]\n",
      "  [-0.7873744   1.312363    1.2281312  ...  0.22809917 -0.33022684\n",
      "    0.50723076]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]\n",
      "\n",
      " [[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.2141706   0.2626713  -1.3259759  ... -0.9760712  -1.6146836\n",
      "   -0.583895  ]\n",
      "  [ 1.6263074   1.5174365   0.8834059  ...  1.2760289  -0.16323893\n",
      "   -0.14378846]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1592578   1.7925797  -1.7164482  ...  0.28647318 -1.2670026\n",
      "   -0.6821783 ]\n",
      "  [-0.4295684   1.2526312   1.1865838  ...  0.02912453  0.84945804\n",
      "   -0.9151194 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]\n",
      "\n",
      " [[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.7371514   1.2432991  -1.5106099  ... -0.55617076  0.4691577\n",
      "   -0.94208   ]\n",
      "  [ 0.5428114   1.2643384   0.59703696 ...  0.40901583 -0.31091762\n",
      "   -2.6447868 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]\n",
      "\n",
      " [[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.41906315  1.9805484  -1.7585162  ... -1.037635    0.20392239\n",
      "   -0.71341056]\n",
      "  [ 0.71587706  1.6420987   1.786257   ...  0.63519347  1.3218433\n",
      "   -0.38935715]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "[#########################               ] | 63% Completed | 26.58 sBefore attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.6056203   0.90105885 -1.8714256  ...  0.5841365  -1.553066\n",
      "   -1.4880773 ]\n",
      "  [-0.62851214  1.8612539   1.5592148  ...  0.254454   -0.65800554\n",
      "   -1.6925702 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]\n",
      "\n",
      " [[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1086128   1.7287987  -2.6137762  ... -0.538069   -0.708784\n",
      "   -0.6921389 ]\n",
      "  [ 0.18332042  1.7275326   0.5611606  ...  0.15692157 -0.45742202\n",
      "   -0.61869997]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]\n",
      "\n",
      " [[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1551062   0.63460904 -1.6329693  ...  0.49729964 -1.4556777\n",
      "   -2.1219485 ]\n",
      "  [-0.5047802   0.32044888  1.120076   ... -0.49583906 -0.5916883\n",
      "   -1.3940599 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.8980997   0.6223957  -1.2622695  ... -1.0544298  -1.2758043\n",
      "   -1.238514  ]\n",
      "  [-0.38732797  1.9437335  -0.11277711 ...  0.24598233  0.6702281\n",
      "   -2.4731019 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]\n",
      "\n",
      " [[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.10403144  0.58016264 -1.8125217  ...  0.11717638 -1.382114\n",
      "   -0.74751866]\n",
      "  [-0.41639593 -0.17066622  1.0415586  ...  0.41794038 -0.5474012\n",
      "   -1.1364186 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]\n",
      "\n",
      " [[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.3056529   0.12502027 -1.4621663  ... -1.1361343   0.07136166\n",
      "   -0.80024594]\n",
      "  [ 1.1250671   0.29367882  0.34707874 ... -0.28027546 -0.51608807\n",
      "   -2.014125  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "[#############################           ] | 72% Completed | 30.65 sBefore attention blocks: [[[-2.13318706e-01 -3.78967464e-01  9.89487886e-01 ... -1.44856739e+00\n",
      "    2.96049774e-01 -8.13292503e-01]\n",
      "  [-2.11214948e+00  1.01479566e+00 -1.18818593e+00 ... -1.46202803e+00\n",
      "    1.24874592e-01 -3.10632080e-01]\n",
      "  [ 6.99306130e-01  1.09043956e+00  2.54290879e-01 ...  6.91433191e-01\n",
      "    8.89541984e-01 -1.15721738e+00]\n",
      "  ...\n",
      "  [ 1.74847770e+00 -2.35013917e-01  1.59675574e+00 ...  1.48646069e+00\n",
      "   -1.91073132e+00 -2.37737626e-01]\n",
      "  [ 1.74847770e+00 -2.35013917e-01  1.59675574e+00 ...  1.48646069e+00\n",
      "   -1.91073132e+00 -2.37737626e-01]\n",
      "  [ 1.74847770e+00 -2.35013917e-01  1.59675574e+00 ...  1.48646069e+00\n",
      "   -1.91073132e+00 -2.37737626e-01]]\n",
      "\n",
      " [[-2.13318706e-01 -3.78967464e-01  9.89487886e-01 ... -1.44856739e+00\n",
      "    2.96049774e-01 -8.13292503e-01]\n",
      "  [-1.89809966e+00  6.22395694e-01 -1.26226950e+00 ... -1.05442977e+00\n",
      "   -1.27580428e+00 -1.23851395e+00]\n",
      "  [ 6.25624657e-01  5.17145395e-01  1.47481942e+00 ...  3.42054278e-01\n",
      "    7.80311227e-01 -1.11987257e+00]\n",
      "  ...\n",
      "  [ 1.74847770e+00 -2.35013917e-01  1.59675574e+00 ...  1.48646069e+00\n",
      "   -1.91073132e+00 -2.37737626e-01]\n",
      "  [ 1.74847770e+00 -2.35013917e-01  1.59675574e+00 ...  1.48646069e+00\n",
      "   -1.91073132e+00 -2.37737626e-01]\n",
      "  [ 1.74847770e+00 -2.35013917e-01  1.59675574e+00 ...  1.48646069e+00\n",
      "   -1.91073132e+00 -2.37737626e-01]]\n",
      "\n",
      " [[-2.13318706e-01 -3.78967464e-01  9.89487886e-01 ... -1.44856739e+00\n",
      "    2.96049774e-01 -8.13292503e-01]\n",
      "  [ 2.92235017e-02  6.22152150e-01 -2.40387273e+00 ... -1.78901494e+00\n",
      "   -7.20317483e-01 -1.56078315e+00]\n",
      "  [-8.00657272e-01  1.39057922e+00 -3.91717434e-01 ...  6.60265923e-01\n",
      "   -1.28985345e+00 -2.48855424e+00]\n",
      "  ...\n",
      "  [ 1.74847770e+00 -2.35013917e-01  1.59675574e+00 ...  1.48646069e+00\n",
      "   -1.91073132e+00 -2.37737626e-01]\n",
      "  [ 1.74847770e+00 -2.35013917e-01  1.59675574e+00 ...  1.48646069e+00\n",
      "   -1.91073132e+00 -2.37737626e-01]\n",
      "  [ 1.74847770e+00 -2.35013917e-01  1.59675574e+00 ...  1.48646069e+00\n",
      "   -1.91073132e+00 -2.37737626e-01]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-2.13318706e-01 -3.78967464e-01  9.89487886e-01 ... -1.44856739e+00\n",
      "    2.96049774e-01 -8.13292503e-01]\n",
      "  [-8.02325845e-01  1.27870476e+00 -1.41442776e+00 ...  2.05705434e-01\n",
      "   -9.00807738e-01 -9.51914310e-01]\n",
      "  [ 2.17863798e-01  1.27168894e-01  1.74066722e-01 ...  6.64364219e-01\n",
      "    1.27614605e+00 -4.99370813e-01]\n",
      "  ...\n",
      "  [ 1.74847770e+00 -2.35013917e-01  1.59675574e+00 ...  1.48646069e+00\n",
      "   -1.91073132e+00 -2.37737626e-01]\n",
      "  [ 1.74847770e+00 -2.35013917e-01  1.59675574e+00 ...  1.48646069e+00\n",
      "   -1.91073132e+00 -2.37737626e-01]\n",
      "  [ 1.74847770e+00 -2.35013917e-01  1.59675574e+00 ...  1.48646069e+00\n",
      "   -1.91073132e+00 -2.37737626e-01]]\n",
      "\n",
      " [[-2.13318706e-01 -3.78967464e-01  9.89487886e-01 ... -1.44856739e+00\n",
      "    2.96049774e-01 -8.13292503e-01]\n",
      "  [-2.40105510e+00  1.73712361e+00 -2.82664418e+00 ... -1.16568625e-01\n",
      "   -6.04601264e-01  7.09583640e-01]\n",
      "  [ 8.49789023e-01  1.00992501e+00  1.17331290e+00 ... -2.77319551e-03\n",
      "   -1.11620769e-01 -2.37025595e+00]\n",
      "  ...\n",
      "  [ 1.74847770e+00 -2.35013917e-01  1.59675574e+00 ...  1.48646069e+00\n",
      "   -1.91073132e+00 -2.37737626e-01]\n",
      "  [ 1.74847770e+00 -2.35013917e-01  1.59675574e+00 ...  1.48646069e+00\n",
      "   -1.91073132e+00 -2.37737626e-01]\n",
      "  [ 1.74847770e+00 -2.35013917e-01  1.59675574e+00 ...  1.48646069e+00\n",
      "   -1.91073132e+00 -2.37737626e-01]]\n",
      "\n",
      " [[-2.13318706e-01 -3.78967464e-01  9.89487886e-01 ... -1.44856739e+00\n",
      "    2.96049774e-01 -8.13292503e-01]\n",
      "  [ 2.92235017e-02  6.22152150e-01 -2.40387273e+00 ... -1.78901494e+00\n",
      "   -7.20317483e-01 -1.56078315e+00]\n",
      "  [ 1.56883150e-03  1.66490924e+00  2.84563303e-02 ...  3.53745341e-01\n",
      "    1.38294399e+00 -6.63102031e-01]\n",
      "  ...\n",
      "  [ 1.74847770e+00 -2.35013917e-01  1.59675574e+00 ...  1.48646069e+00\n",
      "   -1.91073132e+00 -2.37737626e-01]\n",
      "  [ 1.74847770e+00 -2.35013917e-01  1.59675574e+00 ...  1.48646069e+00\n",
      "   -1.91073132e+00 -2.37737626e-01]\n",
      "  [ 1.74847770e+00 -2.35013917e-01  1.59675574e+00 ...  1.48646069e+00\n",
      "   -1.91073132e+00 -2.37737626e-01]]]\n",
      "[################################        ] | 81% Completed | 33.89 sBefore attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.65630066  0.9399086  -1.1149291  ... -0.30026382 -0.5412977\n",
      "   -0.70768297]\n",
      "  [ 1.3270563   0.6453029   1.2007434  ...  1.283459    0.65116787\n",
      "   -2.3839273 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]\n",
      "\n",
      " [[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.215008   -0.07666767 -0.6011561  ... -1.5482868   0.7216913\n",
      "   -0.45578328]\n",
      "  [ 0.67483187  1.2145015  -0.4143839  ...  1.591675    0.19445828\n",
      "   -0.16524297]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]\n",
      "\n",
      " [[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.4610349   0.08792973 -0.59728324 ... -1.3483588   0.2141552\n",
      "   -0.00897765]\n",
      "  [-1.1027024   1.8105774   1.0266391  ...  1.8465219  -0.2015232\n",
      "   -0.6835803 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.15523893  2.3793306  -1.3586607  ...  0.7144307  -2.033327\n",
      "    0.62786996]\n",
      "  [-0.5567514   0.02200055  0.66008097 ...  0.8830926  -0.88896877\n",
      "   -0.8299296 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]\n",
      "\n",
      " [[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.15127975  1.8712398  -2.1194916  ... -0.26614553 -0.0625048\n",
      "   -1.6928456 ]\n",
      "  [ 0.02090275  2.9712572   0.8630784  ... -0.5369935   0.37923035\n",
      "   -1.4833899 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]\n",
      "\n",
      " [[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.5848163   1.2366532  -1.2357887  ... -0.6486484   0.3987859\n",
      "   -2.2921302 ]\n",
      "  [ 0.02637262  1.1002105   2.2777877  ...  0.8279053  -0.14502542\n",
      "   -1.9531537 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "[########################################] | 100% Completed | 36.93 s\n",
      "          1          2         3         4          5         6          7  \\\n",
      "0  0.014670   4.295811 -0.737905  6.260756 -10.438107  2.607501  -8.981725   \n",
      "1 -3.512577   9.073802  1.418918  3.949963  -5.045628  5.043728 -10.646418   \n",
      "2 -0.704749   5.617012  5.544855  7.722315  -7.870079  8.058717  -5.974363   \n",
      "3 -0.338165  10.950527  3.851943  6.362251 -11.123697  4.273367  -7.971213   \n",
      "4  0.736067   8.545635  7.348443  7.398864 -11.428895  7.280689  -5.710442   \n",
      "\n",
      "           8         9         10  ...      1272      1273      1274  \\\n",
      "0   4.586462  1.427386  31.805731  ...  1.407268 -3.548049 -5.542783   \n",
      "1  10.077398 -0.885716  30.536760  ... -2.543087 -4.519937 -4.593312   \n",
      "2   6.684918  0.054571  34.394932  ... -0.771225 -6.872328 -4.710540   \n",
      "3   9.178355  0.551098  34.639774  ...  0.414909 -7.359153 -2.953081   \n",
      "4   8.959304 -1.290090  32.276161  ...  0.988429 -8.099735 -2.944104   \n",
      "\n",
      "       1275      1276       1277       1278      1279      1280    y  \n",
      "0  6.938684 -6.547020 -16.476124 -17.860586 -8.432025  5.747534  100  \n",
      "1  3.605703 -4.628235 -18.188509 -17.656374 -4.776138  4.899364   80  \n",
      "2  4.730622 -4.009584 -15.282115 -16.462130 -6.342956  7.509364   95  \n",
      "3  5.102181 -6.127724 -16.965919 -14.996564 -7.426395  4.208138   90  \n",
      "4  4.286998 -6.282523 -16.236626 -14.968699 -3.311508  9.740802  100  \n",
      "\n",
      "[5 rows x 1281 columns]\n",
      "CPU times: user 59.2 s, sys: 2.19 s, total: 1min 1s\n",
      "Wall time: 1min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "# Set number of parallel partitions\n",
    "num_parallel = 10\n",
    "batch_size = 10000  # Set batch size to 64\n",
    "\n",
    "# Vectorized tokenization function\n",
    "def vectorized_tokenizer(subsequences):\n",
    "    tokens_ids = [b[1] for b in tokenizer.batch_tokenize(subsequences)]\n",
    "    return jnp.asarray(tokens_ids, dtype=jnp.int32)\n",
    "\n",
    "# Vectorized embedding function\n",
    "def vectorized_embedding(tokens):\n",
    "    random_key = jax.random.PRNGKey(0)\n",
    "    outs = forward_fn.apply(parameters, random_key, tokens)\n",
    "    return outs[\"embeddings_20\"][:, 0, :]  # Return embeddings\n",
    "\n",
    "# Tokenization and embedding combined in a batch-wise function\n",
    "def truncate_sequences(sequences, max_length=32):\n",
    "    \"\"\"Truncate each sequence to the max_length.\"\"\"\n",
    "    return [seq[:max_length] for seq in sequences]\n",
    "\n",
    "def process_batch(subsequences):\n",
    "    truncated_sequences = truncate_sequences(subsequences, max_length=32)\n",
    "    tokens = vectorized_tokenizer(truncated_sequences)  # Tokenize the subsequences\n",
    "    embeddings = vectorized_embedding(tokens)  # Get embeddings\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Function to apply on each Dask partition\n",
    "def apply_get_tokens_dask(df):\n",
    "    # Create an empty list to collect embeddings\n",
    "    all_embeddings = []\n",
    "\n",
    "    # Process subsequences in batches\n",
    "    for start in range(0, len(df), batch_size):\n",
    "        end = min(start + batch_size, len(df))\n",
    "        subsequences = df['dna'].values[start:end]  # Get the current batch\n",
    "        embeddings = process_batch(subsequences)  # Process the current batch\n",
    "        all_embeddings.append(embeddings)\n",
    "\n",
    "    # Concatenate all embeddings into a single array\n",
    "    all_embeddings = jnp.concatenate(all_embeddings, axis=0)\n",
    "\n",
    "    # Create a DataFrame from embeddings\n",
    "    embedding_df = pd.DataFrame(all_embeddings, columns=[str(i) for i in range(1, 1281)])\n",
    "\n",
    "    # Concatenate the original DataFrame with the embedding DataFrame\n",
    "    df = pd.concat([df.reset_index(drop=True), embedding_df.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Load the CSV file into a Pandas DataFrame\n",
    "# df = pd.read_csv(datafile + '_with_dna.csv')\n",
    "df = df_with_dna\n",
    "\n",
    "# Convert the Pandas DataFrame to a Dask DataFrame\n",
    "ddf = dd.from_pandas(df, npartitions=num_parallel)  # Adjust 'npartitions' based on resources\n",
    "\n",
    "# Create metadata for Dask to understand the structure of the DataFrame\n",
    "meta = df.copy()\n",
    "# meta = meta.drop(columns=['dna'])  # Drop 'dna' if you don't need it in the final DataFrame\n",
    "for i in range(1, 1281):\n",
    "    meta[str(i)] = np.ndarray  # Prepare for 1280 embedding columns\n",
    "\n",
    "# Apply the function in parallel using Dask\n",
    "ddf = ddf.map_partitions(apply_get_tokens_dask, meta=meta)\n",
    "\n",
    "# Compute the result with progress tracking\n",
    "with ProgressBar():\n",
    "    df_top = ddf.compute()\n",
    "\n",
    "df_top= df_top.drop(columns=['dna'])\n",
    "df_top= swapfirst2last(df_top)\n",
    "\n",
    "# Save the results to CSV\n",
    "df_top.to_csv(datafile + '_nt_embedding.csv', index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_top.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3898cb2f-e6bb-4091-84e7-596650e413cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>1272</th>\n",
       "      <th>1273</th>\n",
       "      <th>1274</th>\n",
       "      <th>1275</th>\n",
       "      <th>1276</th>\n",
       "      <th>1277</th>\n",
       "      <th>1278</th>\n",
       "      <th>1279</th>\n",
       "      <th>1280</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.014670</td>\n",
       "      <td>4.295811</td>\n",
       "      <td>-0.737905</td>\n",
       "      <td>6.260756</td>\n",
       "      <td>-10.438107</td>\n",
       "      <td>2.607501</td>\n",
       "      <td>-8.981725</td>\n",
       "      <td>4.586462</td>\n",
       "      <td>1.427386</td>\n",
       "      <td>31.805730</td>\n",
       "      <td>...</td>\n",
       "      <td>1.407268</td>\n",
       "      <td>-3.548049</td>\n",
       "      <td>-5.542783</td>\n",
       "      <td>6.938684</td>\n",
       "      <td>-6.547020</td>\n",
       "      <td>-16.476124</td>\n",
       "      <td>-17.860586</td>\n",
       "      <td>-8.432025</td>\n",
       "      <td>5.747534</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-3.512577</td>\n",
       "      <td>9.073802</td>\n",
       "      <td>1.418918</td>\n",
       "      <td>3.949963</td>\n",
       "      <td>-5.045628</td>\n",
       "      <td>5.043728</td>\n",
       "      <td>-10.646418</td>\n",
       "      <td>10.077398</td>\n",
       "      <td>-0.885716</td>\n",
       "      <td>30.536760</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.543087</td>\n",
       "      <td>-4.519937</td>\n",
       "      <td>-4.593312</td>\n",
       "      <td>3.605703</td>\n",
       "      <td>-4.628235</td>\n",
       "      <td>-18.188509</td>\n",
       "      <td>-17.656374</td>\n",
       "      <td>-4.776138</td>\n",
       "      <td>4.899363</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.704749</td>\n",
       "      <td>5.617012</td>\n",
       "      <td>5.544855</td>\n",
       "      <td>7.722315</td>\n",
       "      <td>-7.870079</td>\n",
       "      <td>8.058717</td>\n",
       "      <td>-5.974363</td>\n",
       "      <td>6.684918</td>\n",
       "      <td>0.054571</td>\n",
       "      <td>34.394930</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.771225</td>\n",
       "      <td>-6.872328</td>\n",
       "      <td>-4.710540</td>\n",
       "      <td>4.730622</td>\n",
       "      <td>-4.009584</td>\n",
       "      <td>-15.282115</td>\n",
       "      <td>-16.462130</td>\n",
       "      <td>-6.342956</td>\n",
       "      <td>7.509364</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.338165</td>\n",
       "      <td>10.950527</td>\n",
       "      <td>3.851943</td>\n",
       "      <td>6.362251</td>\n",
       "      <td>-11.123697</td>\n",
       "      <td>4.273367</td>\n",
       "      <td>-7.971213</td>\n",
       "      <td>9.178355</td>\n",
       "      <td>0.551098</td>\n",
       "      <td>34.639774</td>\n",
       "      <td>...</td>\n",
       "      <td>0.414909</td>\n",
       "      <td>-7.359153</td>\n",
       "      <td>-2.953081</td>\n",
       "      <td>5.102181</td>\n",
       "      <td>-6.127724</td>\n",
       "      <td>-16.965920</td>\n",
       "      <td>-14.996564</td>\n",
       "      <td>-7.426395</td>\n",
       "      <td>4.208138</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.736067</td>\n",
       "      <td>8.545635</td>\n",
       "      <td>7.348443</td>\n",
       "      <td>7.398864</td>\n",
       "      <td>-11.428895</td>\n",
       "      <td>7.280689</td>\n",
       "      <td>-5.710442</td>\n",
       "      <td>8.959304</td>\n",
       "      <td>-1.290090</td>\n",
       "      <td>32.276160</td>\n",
       "      <td>...</td>\n",
       "      <td>0.988429</td>\n",
       "      <td>-8.099735</td>\n",
       "      <td>-2.944104</td>\n",
       "      <td>4.286998</td>\n",
       "      <td>-6.282523</td>\n",
       "      <td>-16.236626</td>\n",
       "      <td>-14.968699</td>\n",
       "      <td>-3.311508</td>\n",
       "      <td>9.740802</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47948</th>\n",
       "      <td>3.272444</td>\n",
       "      <td>12.186039</td>\n",
       "      <td>2.545142</td>\n",
       "      <td>5.892594</td>\n",
       "      <td>-7.051033</td>\n",
       "      <td>6.465103</td>\n",
       "      <td>-9.658198</td>\n",
       "      <td>6.110871</td>\n",
       "      <td>1.952545</td>\n",
       "      <td>33.166122</td>\n",
       "      <td>...</td>\n",
       "      <td>2.459701</td>\n",
       "      <td>-5.179332</td>\n",
       "      <td>-2.247317</td>\n",
       "      <td>4.099805</td>\n",
       "      <td>-2.065813</td>\n",
       "      <td>-22.586046</td>\n",
       "      <td>-15.919465</td>\n",
       "      <td>-3.994073</td>\n",
       "      <td>2.435006</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47949</th>\n",
       "      <td>1.841231</td>\n",
       "      <td>8.751868</td>\n",
       "      <td>6.477094</td>\n",
       "      <td>6.309408</td>\n",
       "      <td>-7.666615</td>\n",
       "      <td>0.548814</td>\n",
       "      <td>-7.010179</td>\n",
       "      <td>6.689471</td>\n",
       "      <td>0.978388</td>\n",
       "      <td>25.027870</td>\n",
       "      <td>...</td>\n",
       "      <td>5.891490</td>\n",
       "      <td>-2.354367</td>\n",
       "      <td>-2.778426</td>\n",
       "      <td>9.645118</td>\n",
       "      <td>-5.097598</td>\n",
       "      <td>-19.130960</td>\n",
       "      <td>-12.292728</td>\n",
       "      <td>-4.416072</td>\n",
       "      <td>8.519820</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47950</th>\n",
       "      <td>1.252075</td>\n",
       "      <td>9.786252</td>\n",
       "      <td>1.425042</td>\n",
       "      <td>7.753856</td>\n",
       "      <td>-6.902560</td>\n",
       "      <td>5.289457</td>\n",
       "      <td>-7.321176</td>\n",
       "      <td>9.746491</td>\n",
       "      <td>-0.111998</td>\n",
       "      <td>33.170880</td>\n",
       "      <td>...</td>\n",
       "      <td>2.000818</td>\n",
       "      <td>-5.716644</td>\n",
       "      <td>-2.034302</td>\n",
       "      <td>6.425899</td>\n",
       "      <td>-3.352080</td>\n",
       "      <td>-19.514114</td>\n",
       "      <td>-17.275158</td>\n",
       "      <td>-5.917955</td>\n",
       "      <td>5.764585</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47951</th>\n",
       "      <td>-3.014114</td>\n",
       "      <td>12.801002</td>\n",
       "      <td>2.652979</td>\n",
       "      <td>13.168077</td>\n",
       "      <td>-9.895447</td>\n",
       "      <td>7.341707</td>\n",
       "      <td>-7.027656</td>\n",
       "      <td>11.894932</td>\n",
       "      <td>1.691979</td>\n",
       "      <td>34.484936</td>\n",
       "      <td>...</td>\n",
       "      <td>1.941798</td>\n",
       "      <td>-5.452161</td>\n",
       "      <td>1.353195</td>\n",
       "      <td>4.320939</td>\n",
       "      <td>-3.568731</td>\n",
       "      <td>-20.419058</td>\n",
       "      <td>-18.996656</td>\n",
       "      <td>-4.189190</td>\n",
       "      <td>0.456586</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47952</th>\n",
       "      <td>-2.737846</td>\n",
       "      <td>12.138580</td>\n",
       "      <td>4.320749</td>\n",
       "      <td>8.675993</td>\n",
       "      <td>-9.750772</td>\n",
       "      <td>0.888013</td>\n",
       "      <td>-8.815243</td>\n",
       "      <td>7.002745</td>\n",
       "      <td>1.328981</td>\n",
       "      <td>34.336520</td>\n",
       "      <td>...</td>\n",
       "      <td>4.503747</td>\n",
       "      <td>-4.884579</td>\n",
       "      <td>-2.164232</td>\n",
       "      <td>9.033488</td>\n",
       "      <td>-4.307530</td>\n",
       "      <td>-18.244620</td>\n",
       "      <td>-18.653856</td>\n",
       "      <td>-2.428288</td>\n",
       "      <td>2.347241</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>47953 rows × 1281 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              1          2         3          4          5         6  \\\n",
       "0      0.014670   4.295811 -0.737905   6.260756 -10.438107  2.607501   \n",
       "1     -3.512577   9.073802  1.418918   3.949963  -5.045628  5.043728   \n",
       "2     -0.704749   5.617012  5.544855   7.722315  -7.870079  8.058717   \n",
       "3     -0.338165  10.950527  3.851943   6.362251 -11.123697  4.273367   \n",
       "4      0.736067   8.545635  7.348443   7.398864 -11.428895  7.280689   \n",
       "...         ...        ...       ...        ...        ...       ...   \n",
       "47948  3.272444  12.186039  2.545142   5.892594  -7.051033  6.465103   \n",
       "47949  1.841231   8.751868  6.477094   6.309408  -7.666615  0.548814   \n",
       "47950  1.252075   9.786252  1.425042   7.753856  -6.902560  5.289457   \n",
       "47951 -3.014114  12.801002  2.652979  13.168077  -9.895447  7.341707   \n",
       "47952 -2.737846  12.138580  4.320749   8.675993  -9.750772  0.888013   \n",
       "\n",
       "               7          8         9         10  ...      1272      1273  \\\n",
       "0      -8.981725   4.586462  1.427386  31.805730  ...  1.407268 -3.548049   \n",
       "1     -10.646418  10.077398 -0.885716  30.536760  ... -2.543087 -4.519937   \n",
       "2      -5.974363   6.684918  0.054571  34.394930  ... -0.771225 -6.872328   \n",
       "3      -7.971213   9.178355  0.551098  34.639774  ...  0.414909 -7.359153   \n",
       "4      -5.710442   8.959304 -1.290090  32.276160  ...  0.988429 -8.099735   \n",
       "...          ...        ...       ...        ...  ...       ...       ...   \n",
       "47948  -9.658198   6.110871  1.952545  33.166122  ...  2.459701 -5.179332   \n",
       "47949  -7.010179   6.689471  0.978388  25.027870  ...  5.891490 -2.354367   \n",
       "47950  -7.321176   9.746491 -0.111998  33.170880  ...  2.000818 -5.716644   \n",
       "47951  -7.027656  11.894932  1.691979  34.484936  ...  1.941798 -5.452161   \n",
       "47952  -8.815243   7.002745  1.328981  34.336520  ...  4.503747 -4.884579   \n",
       "\n",
       "           1274      1275      1276       1277       1278      1279      1280  \\\n",
       "0     -5.542783  6.938684 -6.547020 -16.476124 -17.860586 -8.432025  5.747534   \n",
       "1     -4.593312  3.605703 -4.628235 -18.188509 -17.656374 -4.776138  4.899363   \n",
       "2     -4.710540  4.730622 -4.009584 -15.282115 -16.462130 -6.342956  7.509364   \n",
       "3     -2.953081  5.102181 -6.127724 -16.965920 -14.996564 -7.426395  4.208138   \n",
       "4     -2.944104  4.286998 -6.282523 -16.236626 -14.968699 -3.311508  9.740802   \n",
       "...         ...       ...       ...        ...        ...       ...       ...   \n",
       "47948 -2.247317  4.099805 -2.065813 -22.586046 -15.919465 -3.994073  2.435006   \n",
       "47949 -2.778426  9.645118 -5.097598 -19.130960 -12.292728 -4.416072  8.519820   \n",
       "47950 -2.034302  6.425899 -3.352080 -19.514114 -17.275158 -5.917955  5.764585   \n",
       "47951  1.353195  4.320939 -3.568731 -20.419058 -18.996656 -4.189190  0.456586   \n",
       "47952 -2.164232  9.033488 -4.307530 -18.244620 -18.653856 -2.428288  2.347241   \n",
       "\n",
       "         y  \n",
       "0      100  \n",
       "1       80  \n",
       "2       95  \n",
       "3       90  \n",
       "4      100  \n",
       "...    ...  \n",
       "47948   65  \n",
       "47949   17  \n",
       "47950   34  \n",
       "47951    0  \n",
       "47952   36  \n",
       "\n",
       "[47953 rows x 1281 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# datafile='clinvar_20240805.missense_matched'\n",
    "df=pd.read_csv(datafile+'_nt_embedding.csv')\n",
    "# df=df.head(100)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e6e86c-966b-4e72-bcdf-59b226d363a3",
   "metadata": {},
   "source": [
    "## Below codes are not used for now"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0564c7b5-369c-4f01-b837-34e59008983e",
   "metadata": {},
   "source": [
    "### Do the work then save to hf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e13e0ba8-40d8-4d9e-a1b1-b2be4a2a0a4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset, Features, Sequence, Value, load_from_disk ,concatenate_datasets # Add load_from_disk here\n",
    "import tempfile\n",
    "\n",
    "from datasets import Dataset, Features, Sequence, Value, load_from_disk, concatenate_datasets\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def FillHFset(rows, num_of_column, additional_columns, path_to_set):\n",
    "    # Create column names (embedding columns) and extend with additional columns\n",
    "    column_names = [str(i) for i in range(num_of_column)] + list(additional_columns.keys())\n",
    "    \n",
    "    # Create DataFrame with all rows and columns\n",
    "    df = pd.DataFrame(rows, columns=column_names)\n",
    "    \n",
    "    # Create embedding columns (float sequences) only for numeric columns\n",
    "    embedding_columns = {\n",
    "        str(col): Sequence(Value(\"float32\")) for col in range(num_of_column) if pd.api.types.is_numeric_dtype(df[str(col)])\n",
    "    }\n",
    "    \n",
    "    # Handle non-numeric columns properly (e.g., cast strings as Value('string'))\n",
    "    non_numeric_columns = {\n",
    "        str(col): Value(\"string\") for col in range(num_of_column) if pd.api.types.is_string_dtype(df[str(col)])\n",
    "    }\n",
    "\n",
    "    # Define all features (embedding + additional columns)\n",
    "    features = Features({**embedding_columns, **non_numeric_columns, **additional_columns})\n",
    "\n",
    "    # Create Hugging Face dataset and cast to features\n",
    "    hf_dataset = Dataset.from_pandas(df).cast(features)\n",
    "    \n",
    "    # Check if dataset exists, load and concatenate if necessary, otherwise save new dataset\n",
    "    if os.path.exists(path_to_set):\n",
    "        existing_dataset = load_from_disk(path_to_set)\n",
    "        combined_dataset = concatenate_datasets([existing_dataset, hf_dataset])\n",
    "\n",
    "        # Use a temporary directory to save the combined dataset first\n",
    "        with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "            combined_dataset.save_to_disk(tmp_dir)\n",
    "            \n",
    "            # Then replace the original dataset with the updated one\n",
    "            shutil.rmtree(path_to_set)  # Remove the old dataset\n",
    "            shutil.move(tmp_dir, path_to_set)  # Move new dataset to the original location\n",
    "\n",
    "    else:\n",
    "        hf_dataset.save_to_disk(path_to_set)\n",
    "    return\n",
    "\n",
    "\n",
    "def output2HFset(df, path_to_set):\n",
    "    \n",
    "    rows = []\n",
    "    additional_columns = {\n",
    "        'CHROM': Value(\"string\"),                  # Chromosome names\n",
    "        'REF': Value(\"string\"),                    # Reference alleles\n",
    "        'ALT': Value(\"string\"),                    # Alternate alleles\n",
    "        'SIZE': Value(\"int32\"),                    # Region sizes\n",
    "        'y': Value(\"int32\")                        # Target labels\n",
    "    }\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        if ((index % 1000) == 0):\n",
    "            print(f\"index={index}\")\n",
    "\n",
    "        chrom = row['CHROM']\n",
    "        ref = row['REF']\n",
    "        alt = row['ALT']\n",
    "        pos_start = row['START']\n",
    "        pos_end = row['START'] + row['SIZE']\n",
    "        y = row['y']\n",
    "\n",
    "        try:\n",
    "            tokens = row['tokens']\n",
    "\n",
    "\n",
    "            # Step 1: Remove brackets and newline characters\n",
    "            cleaned_str = tokens.replace('[', '').replace(']', '').replace('\\n', '')\n",
    "            \n",
    "            # Step 2: Split the string by whitespace to get individual numbers as strings\n",
    "            str_numbers = cleaned_str.split()\n",
    "            \n",
    "            # Step 3: Convert the list of string numbers into integers\n",
    "            int_numbers = list(map(int, str_numbers))\n",
    "            \n",
    "            # Now you can convert this clean list to NumPy or JAX array\n",
    "            import numpy as np\n",
    "            tokens_np = np.array([int_numbers], dtype=np.int32)\n",
    "\n",
    "            \n",
    "            # Now it's safe to convert it to a JAX array\n",
    "            import jax.numpy as jnp\n",
    "            \n",
    "            tokens_jax = jnp.asarray(tokens_np)\n",
    "\n",
    "            \n",
    "            embedding = get_embeddings(tokens_jax)  # Get embeddings\n",
    "\n",
    "            # print(\"output2HFset : embedding.shape=:\",embedding.shape)\n",
    "            one_row = np.append(embedding, [chrom, ref, alt, pos_end - pos_start, y])\n",
    "            # print(\"one_row=:\",one_row)\n",
    "            # print(\"one_row shape=:\",one_row.shape)\n",
    "            rows.append(one_row)\n",
    "\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Exception caught: {e} for {row['CHROM']}-{row['START']}\")\n",
    "    \n",
    "        # Every 5000 rows, save a batch to the dataset\n",
    "        if ((index % 5000) == 0 and index > 0):\n",
    "            num_of_column = embedding.shape[1]\n",
    "            FillHFset(rows, num_of_column, additional_columns, path_to_set)\n",
    "            rows = []\n",
    "            print(f\"Completed batch at index={index}\")\n",
    "\n",
    "    # If there are remaining rows after the loop, add them\n",
    "    if rows:\n",
    "        num_of_column = embedding.shape[1]\n",
    "        FillHFset(rows, num_of_column, additional_columns, path_to_set)     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be9649f-a307-47d6-bd3e-4c253be2b23d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset at my_hf_datasets/20240805/noncoding_embeddings_nt has been removed.\n",
      "index=0\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.50158495  0.779868   -1.8287711  ... -1.2115262  -0.2792523\n",
      "   -1.2028177 ]\n",
      "  [ 0.2674231   0.92444116  1.2842956  ...  0.08013222 -0.18082626\n",
      "   -1.1940216 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-25 17:28:21.862089: W external/xla/xla/service/gpu/gemm_fusion_autotuner.cc:836] Compiling 16 configs for gemm_fusion_dot.1 on a single thread.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.41093105  2.187459   -2.284892   ...  1.0488595   0.94676834\n",
      "   -1.5457382 ]\n",
      "  [ 1.6040431   2.4732795   1.469671   ...  0.9146555   0.5292439\n",
      "   -1.1846138 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.6173527   1.8079523  -2.9425523  ... -0.67080766 -0.51572955\n",
      "   -1.4554822 ]\n",
      "  [-0.54203916  0.7917464   0.4183545  ...  0.20731087 -0.23180176\n",
      "   -1.4138888 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.886745    1.9516306  -1.7044697  ... -0.5790743  -1.7615848\n",
      "   -1.4859283 ]\n",
      "  [ 0.48920557  1.4471362   2.682345   ... -1.04092    -1.1596994\n",
      "   -2.649862  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.886745    1.9516306  -1.7044697  ... -0.5790743  -1.7615848\n",
      "   -1.4859283 ]\n",
      "  [ 0.48920557  1.4471362   2.682345   ... -1.04092    -1.1596994\n",
      "   -2.649862  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.0543165   1.8943349  -0.8947369  ... -1.7603467   0.3270024\n",
      "    0.11836046]\n",
      "  [ 0.5545827   0.4044416   1.7322717  ...  1.5862153  -0.08422996\n",
      "   -0.433326  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.9632331   0.67086744 -1.0751132  ... -0.29927772 -0.23761903\n",
      "   -0.52831244]\n",
      "  [ 0.11839642  1.4743845   1.4717317  ... -0.50327027 -0.3231231\n",
      "   -0.42158085]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1092257   1.0752115  -0.2601998  ... -0.45290542  1.2273223\n",
      "   -2.4079924 ]\n",
      "  [-0.26426703  1.8727701   0.8566793  ...  0.7946198  -0.93916065\n",
      "    0.08268631]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.14983875  2.1927104  -1.5218589  ... -0.63956916  0.20138621\n",
      "   -0.83270085]\n",
      "  [ 0.50309515 -0.3222201   0.3993808  ...  0.90628064  0.8355911\n",
      "   -0.5239078 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.9215816  -0.02330267 -1.9968734  ...  0.21260348 -0.95630205\n",
      "    0.05496335]\n",
      "  [-0.5532026   2.342938    2.3486547  ...  0.47536248 -1.4306725\n",
      "   -2.4970412 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.483597    2.2172005  -0.2265526  ... -0.45929748 -2.0061707\n",
      "   -2.11177   ]\n",
      "  [ 0.3409097   1.1291143   1.4314976  ...  0.9818942  -0.01525033\n",
      "    0.06401527]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.6433926   1.3033302  -1.1325415  ... -1.1241199  -1.127521\n",
      "    0.8121146 ]\n",
      "  [-0.8763467   0.98752266  1.0820464  ...  0.947814    1.4899359\n",
      "   -1.2061176 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1094444   1.0631554   0.04093373 ... -1.4957249  -0.20951846\n",
      "    0.24493337]\n",
      "  [ 0.14463827  0.7451055   0.42844296 ...  0.60969365  0.84925026\n",
      "   -1.0397819 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.18781906  1.7497987  -0.5077276  ... -0.01797751 -1.5370104\n",
      "   -1.6893367 ]\n",
      "  [ 0.74285555 -0.35284877 -0.1097908  ...  0.38281825 -1.1037501\n",
      "   -1.3985677 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.2798713   0.16499197 -1.8650229  ... -0.21771547 -0.84189177\n",
      "   -1.875141  ]\n",
      "  [-0.44631365  0.772187    1.2170236  ...  1.3606102   1.6040848\n",
      "   -1.175871  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.8154007   0.27440673 -2.0227814  ... -0.12724513 -1.2428391\n",
      "   -0.5384144 ]\n",
      "  [ 1.1826968   2.1928475   0.5416482  ... -0.64907044 -0.08666287\n",
      "   -0.4591254 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.22976613  0.47123915 -1.4379859  ...  0.24019137  0.9411574\n",
      "   -0.84209615]\n",
      "  [ 0.0967941   1.441061    1.9898307  ...  1.983294   -1.1473639\n",
      "   -2.434067  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.76885605  0.67777693 -1.372334   ... -2.04092     0.17866379\n",
      "   -1.1662893 ]\n",
      "  [-0.7863122   1.218217    0.36377233 ...  1.7483683   1.6603391\n",
      "   -2.120698  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.9892318  -0.3267647  -2.187799   ... -1.7089092  -1.5781381\n",
      "   -0.71264845]\n",
      "  [ 1.3747532   0.6318078   0.45628506 ...  2.1111963   0.3249315\n",
      "   -1.6974932 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.061429    1.4355371  -1.83832    ...  0.13325515 -0.7915874\n",
      "   -1.6538618 ]\n",
      "  [-0.12006338  1.0201414   2.2470088  ...  0.47800803 -0.32420927\n",
      "   -2.2478104 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.89179754  1.7626144  -1.5092169  ... -1.6269903  -1.3059485\n",
      "   -0.3164961 ]\n",
      "  [ 0.43735316 -0.26144326  1.3848028  ... -0.16609937  0.05223344\n",
      "    0.02320588]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.2812281   0.81438375 -1.5153366  ... -1.4729571   0.10289824\n",
      "   -0.1964792 ]\n",
      "  [ 0.35718992  2.2732565   0.8988093  ... -0.55261296  0.72736335\n",
      "   -2.3137138 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.0890021   0.48866087 -1.025975   ...  1.0054431   0.37252825\n",
      "   -1.5508265 ]\n",
      "  [-0.6303556   1.1039629   0.26616216 ...  0.89032793  0.6111486\n",
      "   -0.13016939]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.4600205   1.79478    -0.75403523 ... -1.0906864   0.17518783\n",
      "   -0.8911976 ]\n",
      "  [ 0.352129    0.46608323  0.33880365 ...  0.578465   -0.6578559\n",
      "   -0.519656  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.8154007   0.27440673 -2.0227814  ... -0.12724513 -1.2428391\n",
      "   -0.5384144 ]\n",
      "  [ 1.1641108   1.8642355   1.1847112  ...  0.17523801  1.4913018\n",
      "    0.1771574 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.24318236  0.6321578  -2.5005398  ... -0.31924975  0.98723227\n",
      "   -1.7781719 ]\n",
      "  [-0.19410852  1.3105807   0.8651334  ...  1.6264007  -0.64304006\n",
      "   -1.6265659 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.0031898   1.50963    -1.5453904  ... -2.117535    1.2453804\n",
      "    0.14437246]\n",
      "  [ 0.88340855  0.72498536  0.33427942 ... -1.1833489  -0.61788666\n",
      "   -0.2601884 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.4117866  -0.5877241  -0.84224087 ... -0.51598835 -0.02370971\n",
      "   -2.021447  ]\n",
      "  [-1.6575539   1.029485    1.0666564  ... -0.30846363  0.5600364\n",
      "   -0.47778475]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.7050836   2.8099422  -1.9171131  ...  0.53942597 -1.0151193\n",
      "    0.2123062 ]\n",
      "  [-1.0708413   1.3822728   1.1403104  ... -0.9536677   0.673586\n",
      "   -1.279407  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.2715964   0.56103307 -2.7957969  ...  0.6733291  -0.4879147\n",
      "   -1.3056542 ]\n",
      "  [ 0.05059575  0.7805612   1.3860629  ...  0.433022    0.88378906\n",
      "   -0.9218375 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.253388    0.28027648 -0.08841896 ... -1.137845   -0.40212965\n",
      "   -1.3444366 ]\n",
      "  [-0.11546365  0.38310552 -0.2990445  ... -0.0906176  -0.594292\n",
      "   -1.4620684 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.13729912  0.33361387 -1.718294   ... -1.7356131  -0.4359595\n",
      "    0.404943  ]\n",
      "  [ 0.77659214 -0.09450686  0.21541315 ...  1.1025889   0.20085533\n",
      "   -1.0017862 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.358077    0.8156624  -1.1354933  ...  0.12443987 -1.1527917\n",
      "    0.4367839 ]\n",
      "  [ 0.4564874   2.7395058   1.432617   ...  0.91183454 -0.11758907\n",
      "    0.40955794]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.6629714   0.7987036  -1.2909119  ... -0.8545277  -0.7563246\n",
      "   -0.80875033]\n",
      "  [ 0.0565136   1.1251848   2.1245446  ... -0.6242482   0.47804803\n",
      "   -1.9435048 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.4981693   0.1848042  -1.1918973  ... -0.2001571   0.74978226\n",
      "   -0.60833585]\n",
      "  [ 0.02205215  1.383836    0.17139858 ... -0.19159842  0.63562167\n",
      "   -1.6696625 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.6222092   1.0379993  -1.2990618  ... -0.4038508   0.63164526\n",
      "   -0.26833457]\n",
      "  [ 0.48411414  2.383462    2.3170338  ...  1.2312059   1.2146215\n",
      "   -1.2774659 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.8154007   0.27440673 -2.0227814  ... -0.12724513 -1.2428391\n",
      "   -0.5384144 ]\n",
      "  [ 0.17368275  1.5942572   0.17747736 ...  0.8516419   1.8080804\n",
      "   -1.952918  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.3206491   2.789873   -0.86091346 ...  1.0929871  -1.1490849\n",
      "    0.13832927]\n",
      "  [-0.2490879   1.9190166  -0.0376929  ...  1.7553921   0.84508044\n",
      "   -0.4964767 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.04320222  2.0724583  -2.5684555  ...  0.7885766   1.3641376\n",
      "   -1.4775709 ]\n",
      "  [-0.9765631  -0.22683144  1.3716006  ...  0.17457232 -0.7992269\n",
      "   -2.516519  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.96863854  1.0335749  -1.6255862  ... -1.5023355  -0.5925858\n",
      "   -0.07188541]\n",
      "  [-0.01256183  2.427193    0.05502582 ...  0.2118541   0.3817038\n",
      "   -0.5946677 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.5393517   0.01412547 -1.7722107  ... -0.37488404 -1.2131804\n",
      "   -0.5480817 ]\n",
      "  [-0.8618283   0.7946254   1.025912   ... -0.9121794  -0.2768644\n",
      "   -0.14885926]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.26483577  2.5987203  -1.5194687  ... -0.59334695 -0.387187\n",
      "    0.47103608]\n",
      "  [-1.1789697   0.43673772  2.1742449  ...  1.6266339  -1.2066586\n",
      "   -1.5257857 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.45625287  0.30147713 -2.3914766  ... -1.3299311  -0.97038984\n",
      "    0.2607317 ]\n",
      "  [-0.2855371   1.8813791   0.72888005 ... -0.26555318  1.1386297\n",
      "   -0.81184405]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.45625287  0.30147713 -2.3914766  ... -1.3299311  -0.97038984\n",
      "    0.2607317 ]\n",
      "  [-0.2855371   1.8813791   0.72888005 ... -0.26555318  1.1386297\n",
      "   -0.81184405]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.07063836  1.5969447  -1.1583754  ...  0.63512623 -1.2058331\n",
      "   -1.7927483 ]\n",
      "  [-0.4189904   2.4730575   2.4414585  ...  1.95468     0.25111824\n",
      "   -0.553687  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.5034616   0.0386281  -1.3262019  ... -0.9309203   0.21816224\n",
      "   -1.5523058 ]\n",
      "  [-1.2832246   1.0448718   1.5207365  ...  1.0488389   0.27245352\n",
      "   -1.7969928 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.07063836  1.5969447  -1.1583754  ...  0.63512623 -1.2058331\n",
      "   -1.7927483 ]\n",
      "  [-0.47542298  1.713321    2.5978594  ...  1.0578512   0.861444\n",
      "   -1.6806331 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.4610945   0.5097193  -0.690351   ... -0.29128516  0.56228065\n",
      "   -0.4512961 ]\n",
      "  [ 0.33110508 -0.04045212  1.7475424  ...  0.35476178  1.0458238\n",
      "   -1.3001091 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.8727033   1.3016064  -1.6662797  ... -0.7015338   0.35174924\n",
      "   -0.41784877]\n",
      "  [ 0.16871665  1.027532    2.4025862  ...  1.7757273  -0.49225384\n",
      "   -1.6086439 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.6991988  -0.27938902 -1.5552118  ...  0.26346704  0.6082774\n",
      "   -0.32757363]\n",
      "  [-0.2669691   0.9276936   0.40381467 ... -0.7418702   0.7586741\n",
      "   -2.6745315 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.5495734   1.3780648  -1.3090575  ... -0.28243834  0.75456315\n",
      "   -2.095909  ]\n",
      "  [-0.15623209  2.9091053   2.4585803  ...  0.78096265  0.06394736\n",
      "   -0.25375003]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.6222092   1.0379993  -1.2990618  ... -0.4038508   0.63164526\n",
      "   -0.26833457]\n",
      "  [ 0.81011343  1.5219574   1.0526865  ...  0.19219258  0.75517017\n",
      "   -0.8571418 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.6014194   1.4984226  -1.0696347  ... -0.50297415  0.63670975\n",
      "   -0.74339724]\n",
      "  [-0.26225176  1.4768288   0.03503239 ...  1.3567214   0.05067658\n",
      "   -0.76388156]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.36491096  0.15584505 -0.30376887 ...  0.97429574 -0.369569\n",
      "   -0.9800675 ]\n",
      "  [-0.14246857  0.84710157  1.0985892  ...  1.3793063   1.1238524\n",
      "   -1.431736  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.072863    0.721364   -1.4766183  ...  0.44464645  0.5483541\n",
      "   -1.0464647 ]\n",
      "  [ 0.4170186   0.74863267  0.7738056  ...  1.6410859  -0.5430422\n",
      "   -1.1234977 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.9124306   1.746753   -0.84115875 ... -0.3488015   0.31697935\n",
      "   -0.23130322]\n",
      "  [ 0.7086594   1.192037    1.1936734  ...  0.4124819   1.4712175\n",
      "   -0.28949225]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.766567    0.5908703  -1.604187   ... -1.1881682  -0.58121204\n",
      "   -0.67304426]\n",
      "  [-0.3261995   1.3504413   0.7795841  ... -0.1193099   0.93238693\n",
      "    0.37247467]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.06923693 -0.51741755 -2.616054   ... -0.02409458 -0.3661384\n",
      "   -0.5191132 ]\n",
      "  [ 0.062374    0.05969954  0.7377763  ... -0.28734016  1.06919\n",
      "   -0.12194502]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.45963967  1.3163344  -2.3652666  ... -1.1701095  -0.39678892\n",
      "   -0.39160264]\n",
      "  [-0.19410852  1.3105807   0.8651334  ...  1.6264007  -0.64304006\n",
      "   -1.6265659 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.8671285  -0.26500797 -1.985825   ... -0.24378006  0.4965992\n",
      "   -1.8458781 ]\n",
      "  [-0.05444169  1.8591127   1.3183532  ...  0.30736387  0.6749232\n",
      "   -1.5709088 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.5949789   1.7003379  -0.66191196 ... -0.30842108  1.0539634\n",
      "   -0.5111242 ]\n",
      "  [ 0.27878225  2.5462592   0.5298713  ...  2.1944034   0.445135\n",
      "   -1.0210768 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.620225    2.0446773  -1.1275864  ...  0.67450476 -0.5807691\n",
      "   -0.61015946]\n",
      "  [ 0.83120954  1.5593609   1.2541823  ...  0.6001297  -0.6311627\n",
      "   -1.7022943 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.5364194   1.026899   -1.2491534  ... -0.7277963   0.01916513\n",
      "   -0.78543884]\n",
      "  [-0.8485255   0.5910962   1.5759898  ...  1.1218431   0.68301183\n",
      "   -2.2396832 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.33483362  0.92651415 -1.9202399  ...  0.45585397 -0.45168227\n",
      "   -0.80166346]\n",
      "  [ 1.5625345   1.9142709  -0.21454906 ...  0.7749672  -0.07139504\n",
      "   -1.0638238 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.40661114  0.66322243 -2.3589435  ... -0.22758386 -0.87423503\n",
      "   -0.44255587]\n",
      "  [-0.6416998   2.7300916   0.12064505 ...  1.4026593  -0.11530577\n",
      "   -0.32531935]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.41093105  2.187459   -2.284892   ...  1.0488595   0.94676834\n",
      "   -1.5457382 ]\n",
      "  [ 0.6655587   2.7244577   1.0557388  ...  0.341313    0.18831131\n",
      "    0.08576477]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.8323988   0.70352393 -1.9899567  ... -1.530485    0.7254397\n",
      "    0.90434575]\n",
      "  [-0.3831998   1.2673148   2.85647    ...  0.38225514  1.1726217\n",
      "   -0.0972513 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.6455539   0.7434898  -1.5317873  ... -0.531256    0.4233706\n",
      "    0.23906028]\n",
      "  [ 0.71797645  1.3220094   1.5084047  ...  0.4681142   0.6402883\n",
      "   -1.3494155 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.83360034  1.3153235  -0.58537674 ...  1.0486342  -1.7228622\n",
      "   -2.0487957 ]\n",
      "  [ 0.02570783  0.313581    0.06686616 ... -0.42922217  0.13496432\n",
      "   -0.11166573]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.9046866   0.18784344 -2.5083413  ... -1.3638821  -0.440534\n",
      "    0.27360559]\n",
      "  [ 0.30477506  0.85195243  1.7908553  ...  0.70677996  0.4218945\n",
      "   -0.45640498]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.8902037   0.6391599  -1.3162692  ...  0.40534797 -0.55297726\n",
      "   -1.5255785 ]\n",
      "  [ 0.29845396  0.3616538   1.3805127  ...  0.9533672   0.4449249\n",
      "   -1.6220577 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.330916    1.9395614  -1.0458435  ... -0.34536302 -0.6996976\n",
      "    0.0571875 ]\n",
      "  [ 0.95015025  2.2216387   1.0943282  ...  1.0674087   0.10938552\n",
      "   -1.3460732 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.330916    1.9395614  -1.0458435  ... -0.34536302 -0.6996976\n",
      "    0.0571875 ]\n",
      "  [ 0.95015025  2.2216387   1.0943282  ...  1.0674087   0.10938552\n",
      "   -1.3460732 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.54058987  1.5384312  -1.573059   ... -0.7790518  -1.2024007\n",
      "   -0.57746094]\n",
      "  [-0.7577745   1.3156874   0.4367959  ...  0.75757295  1.4747049\n",
      "   -2.574593  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.9410109   2.2403765  -1.1233857  ... -0.2673788  -0.79465294\n",
      "   -0.18306983]\n",
      "  [ 0.9121126   1.8483741   1.1472144  ...  0.6977384   0.22692415\n",
      "   -0.3893044 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.08563101  0.02088571 -2.728784   ... -0.58240575 -1.3517678\n",
      "   -2.009532  ]\n",
      "  [ 1.170624    0.1683513   0.26117784 ...  0.40550208 -0.2400911\n",
      "   -0.80024004]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.55666834  0.7238054  -2.050268   ... -0.65446305 -1.4351838\n",
      "   -0.53511214]\n",
      "  [ 1.0594878   2.3838425  -0.5992049  ... -0.9005862  -1.0945433\n",
      "   -2.0897026 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.21241796  1.1962719  -1.0668027  ... -0.46654573  0.06478995\n",
      "   -0.9641443 ]\n",
      "  [ 0.43735316 -0.26144326  1.3848028  ... -0.16609937  0.05223344\n",
      "    0.02320588]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.6365367   1.2153214  -0.810295   ... -1.4474151  -2.0446248\n",
      "    0.7011466 ]\n",
      "  [-0.09980999  1.1746004   1.8041419  ...  0.7446922  -0.10691692\n",
      "   -1.9574096 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.2197335   0.9037935  -1.9768744  ... -1.2136112  -2.129247\n",
      "   -0.8064696 ]\n",
      "  [-0.8378247   2.4676569   0.22408015 ...  1.521364   -0.12052049\n",
      "   -1.1343949 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.6341171   0.23114556 -1.9864969  ... -0.8881023   0.0293816\n",
      "   -1.4804146 ]\n",
      "  [ 0.52777386  2.4280825   2.649352   ...  0.0866819  -0.44369602\n",
      "   -1.617471  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.24316531  0.49295014 -1.1191328  ... -0.34459713 -0.67409146\n",
      "   -0.8738417 ]\n",
      "  [ 0.0967941   1.441061    1.9898307  ...  1.983294   -1.1473639\n",
      "   -2.434067  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.94838977  0.84370065 -1.9700929  ... -1.5327593  -0.4629489\n",
      "    0.56679714]\n",
      "  [ 0.3033941   2.2707462   2.0947733  ... -0.62896496  1.245763\n",
      "   -1.7192976 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.4426694   1.6068807  -0.24771738 ... -0.874315    0.19380933\n",
      "   -0.6308552 ]\n",
      "  [-0.7037654  -0.10275555  0.48378855 ...  0.7576314  -0.04372138\n",
      "   -0.10589707]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.4426694   1.6068807  -0.24771738 ... -0.874315    0.19380933\n",
      "   -0.6308552 ]\n",
      "  [-0.7037654  -0.10275555  0.48378855 ...  0.7576314  -0.04372138\n",
      "   -0.10589707]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.62700033  2.1450086  -0.4804343  ... -1.5636249   0.67026466\n",
      "   -1.3340263 ]\n",
      "  [-1.1892542   1.7896454   0.36831647 ...  0.596112   -1.4557158\n",
      "   -2.7698739 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.072863    0.721364   -1.4766183  ...  0.44464645  0.5483541\n",
      "   -1.0464647 ]\n",
      "  [-0.34947687  0.29072952  0.7101846  ...  0.71694446 -0.26639342\n",
      "   -2.2604122 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.29458869  0.8868762  -1.0268106  ...  1.2488691  -0.11685392\n",
      "   -1.1386735 ]\n",
      "  [ 0.464451    2.553342    0.44930607 ...  0.27687615  0.09660634\n",
      "   -2.0224059 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.7070649   0.668378   -1.3576796  ... -0.33138245  0.10808623\n",
      "   -0.21049696]\n",
      "  [-0.23304734  1.4041984  -0.5337776  ...  0.10401753 -0.8963515\n",
      "   -2.907445  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.97205395  0.05277038 -2.6454225  ... -1.7660002  -0.33614677\n",
      "   -0.37961954]\n",
      "  [-0.4222823   2.3368044   1.494946   ...  0.57994485  0.22752464\n",
      "   -1.7783539 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.6493286   0.6635052  -2.0756238  ... -1.2441598   0.16952664\n",
      "   -0.28622752]\n",
      "  [-0.724211    0.7915058   0.05697703 ... -0.68337184  1.5171312\n",
      "   -2.3562305 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.5608301   2.206478   -0.6408242  ...  0.1659055  -0.40640306\n",
      "    0.5587946 ]\n",
      "  [-0.872795   -0.1358012   1.4600945  ...  1.5199578   0.09959999\n",
      "   -1.5540906 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.2199726   0.05645144 -1.2613424  ... -1.137973   -0.7041749\n",
      "   -1.4093922 ]\n",
      "  [ 0.11799929  1.3247973   0.6362239  ... -0.8927894   0.39334673\n",
      "    0.20575619]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.24508446  2.379273   -0.40471113 ... -1.3783977  -1.2365112\n",
      "   -0.16242474]\n",
      "  [-0.17313737  2.0691552   1.1247532  ...  0.5868475  -0.04129851\n",
      "   -1.3498726 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.33725256  0.57749563 -1.2117288  ... -0.33008283  0.14409417\n",
      "   -0.3086842 ]\n",
      "  [-1.1601816   1.0783509   1.2575084  ...  0.24940144  1.101606\n",
      "   -0.5192082 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.362706    1.0439205  -0.47092748 ... -1.1201689   0.28303266\n",
      "   -0.08875489]\n",
      "  [-0.57023656  0.8769915   1.6107836  ... -0.84170324 -0.8597514\n",
      "   -1.7860146 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.3891928   0.7783034  -2.2898583  ... -1.4182863  -1.9950187\n",
      "   -1.3087971 ]\n",
      "  [ 0.00952047  1.1977996   0.94152457 ...  0.47503906 -0.7574339\n",
      "   -1.2264158 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.7618473   2.1039586  -1.7909281  ...  0.09921297  0.13045359\n",
      "   -0.6657386 ]\n",
      "  [-0.2514733   1.5496424   0.49777257 ... -0.42034507 -0.46608073\n",
      "   -0.9455973 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.9029928   0.4983517  -1.4926152  ... -1.330502   -1.2749181\n",
      "   -0.34807286]\n",
      "  [ 0.0967941   1.441061    1.9898307  ...  1.983294   -1.1473639\n",
      "   -2.434067  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-2.1331871e-01 -3.7896746e-01  9.8948789e-01 ... -1.4485674e+00\n",
      "    2.9604977e-01 -8.1329250e-01]\n",
      "  [-1.0504578e+00  8.9440382e-01 -3.2819855e-01 ... -4.5665193e-01\n",
      "   -8.9970756e-01 -1.8625391e+00]\n",
      "  [-8.1148744e-04  1.8785937e+00  5.6788492e-01 ...  5.8196700e-01\n",
      "    1.5979501e+00 -7.5166720e-01]\n",
      "  ...\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.4055432   2.1784945  -1.0305563  ... -0.35710347  0.1913541\n",
      "   -0.86609936]\n",
      "  [-1.2819275   1.047023   -0.08538544 ...  0.33869255 -0.21877648\n",
      "   -1.3107469 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.212322    0.92128557 -2.660593   ... -0.59596735 -0.7942748\n",
      "   -0.9254756 ]\n",
      "  [ 1.5405833  -0.1382606  -0.31046212 ...  0.64049345 -0.47952157\n",
      "   -1.2630386 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.0244882   0.44850045 -0.94794816 ...  0.60990083  0.7030547\n",
      "   -0.5183505 ]\n",
      "  [-0.8672886   0.4548766   2.1338449  ... -0.28604424 -1.3020127\n",
      "   -1.1182537 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.5088996   1.7215062  -1.5024635  ... -0.6965186   0.03710341\n",
      "   -0.38138705]\n",
      "  [ 0.43982178  1.4258778   0.857834   ...  0.42766204  0.00914268\n",
      "    0.5627909 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.2614181   1.5085433  -1.2627037  ... -0.6033501   0.7615823\n",
      "   -1.5649612 ]\n",
      "  [ 0.57194436 -0.19106424  1.6386254  ...  0.7500334   1.6747534\n",
      "   -1.905829  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.35845006 -0.3168018  -0.936582   ... -0.18462658  1.0992551\n",
      "   -1.5205576 ]\n",
      "  [ 0.14463827  0.7451055   0.42844296 ...  0.60969365  0.84925026\n",
      "   -1.0397819 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.04842663  1.7252243  -3.1861095  ... -1.0822239  -1.4333875\n",
      "   -0.74264634]\n",
      "  [ 0.01743613 -0.3714354   1.6207372  ...  1.0929613   0.74687356\n",
      "    0.15351212]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.6677687   1.3467023  -1.6755415  ...  0.07458881  0.18712056\n",
      "   -0.6678492 ]\n",
      "  [ 1.521027    0.54771495  0.696924   ...  1.1801205  -0.7072418\n",
      "   -1.1073202 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.6677687   1.3467023  -1.6755415  ...  0.07458881  0.18712056\n",
      "   -0.6678492 ]\n",
      "  [ 1.521027    0.54771495  0.696924   ...  1.1801205  -0.7072418\n",
      "   -1.1073202 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.83121485  0.13245869 -1.2943081  ... -1.1223267  -1.3047545\n",
      "   -0.54627883]\n",
      "  [-0.8646934   1.1622168   0.24022263 ...  0.58615375 -0.65143853\n",
      "    0.1600976 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1142944   0.6282645  -2.2297654  ... -2.2034109  -1.8242171\n",
      "    0.746878  ]\n",
      "  [-1.0529766   1.5463883   0.5747204  ... -0.08431423  0.7103493\n",
      "   -2.0895882 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.1173031   0.44617504 -0.95648843 ... -1.4287494   0.05873352\n",
      "   -0.8565792 ]\n",
      "  [-0.58762705  0.18754852  1.1187005  ... -1.0781662   0.8896892\n",
      "   -0.6320699 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.5397153   0.82130337 -1.2357944  ... -1.2649649   0.43376124\n",
      "   -1.8537785 ]\n",
      "  [-0.07536239  0.64759684  1.0824373  ... -0.3676929  -1.0831357\n",
      "    0.5314115 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.49773768  0.16856462 -0.6988688  ... -1.2324984  -1.11659\n",
      "   -0.45668316]\n",
      "  [-0.8378247   2.4676569   0.22408015 ...  1.521364   -0.12052049\n",
      "   -1.1343949 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.5180216   0.06181097 -1.456507   ... -2.0128262   0.31419086\n",
      "   -0.24679857]\n",
      "  [ 1.4392941   1.8472438   1.072744   ...  0.23814134  0.61260176\n",
      "   -0.76665837]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.65161216  2.4205217  -2.0453362  ...  1.2597435  -0.13036335\n",
      "   -0.6358055 ]\n",
      "  [-1.7915531   0.8284763   0.82226247 ...  1.6437752  -0.4209143\n",
      "   -1.2204376 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.8154007   0.27440673 -2.0227814  ... -0.12724513 -1.2428391\n",
      "   -0.5384144 ]\n",
      "  [ 1.2260829   0.47935623  0.46520776 ...  0.3963222  -0.97993237\n",
      "   -1.1480744 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.9134673   1.7672602  -1.3952098  ... -1.8122145  -1.7582924\n",
      "   -2.0542731 ]\n",
      "  [ 1.022389    1.480657    1.1588182  ... -0.40502083 -0.15052392\n",
      "    0.1329422 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.0523849   1.5502582  -0.56573886 ...  0.6885315  -0.7171396\n",
      "    0.12987679]\n",
      "  [-1.7338685   0.9773828   0.9530758  ... -0.25840944  0.69646424\n",
      "    0.22670925]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.0523849   1.5502582  -0.56573886 ...  0.6885315  -0.7171396\n",
      "    0.12987679]\n",
      "  [-1.7338685   0.9773828   0.9530758  ... -0.25840944  0.69646424\n",
      "    0.22670925]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.7876638   0.84280694 -1.9840702  ... -1.6292355   0.66513795\n",
      "   -1.7464576 ]\n",
      "  [ 1.6286304   2.5691297   1.9911985  ...  1.0947622   0.7141935\n",
      "   -1.1568311 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.6385872   0.05316269 -0.45509565 ... -1.6064352  -0.02094671\n",
      "    0.04901224]\n",
      "  [-0.5364951  -0.00965738  2.2604415  ...  0.46163395  0.2832084\n",
      "   -1.3552898 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.072863    0.721364   -1.4766183  ...  0.44464645  0.5483541\n",
      "   -1.0464647 ]\n",
      "  [-0.4538777   1.196866    0.90589    ...  1.1270986  -0.68736964\n",
      "   -1.5302327 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.32635242 -0.26216853 -0.4650389  ... -1.1329803   1.3649688\n",
      "   -1.7951137 ]\n",
      "  [-0.3447084   1.5479634   0.35949504 ...  0.0055978  -1.1615548\n",
      "   -1.5294509 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.3269496   1.5635595  -1.9047965  ... -0.8977742  -1.2341981\n",
      "   -0.1518622 ]\n",
      "  [ 0.61105144  1.1992037  -0.03457797 ... -0.11235327  1.0848101\n",
      "   -0.8575413 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.21196193  1.2233768  -2.4202611  ... -1.9565048   0.11066645\n",
      "   -1.9062735 ]\n",
      "  [-1.684189    0.9073903   0.23665643 ...  0.46792474  0.56444323\n",
      "   -0.40644413]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.9457809   0.06047642 -1.0446422  ... -1.3552703  -1.3034923\n",
      "   -0.02056545]\n",
      "  [-1.609217    2.7422996   0.44130063 ...  0.5365212   0.00571765\n",
      "   -0.05508959]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.212322    0.92128557 -2.660593   ... -0.59596735 -0.7942748\n",
      "   -0.9254756 ]\n",
      "  [-0.9453795  -0.34134912 -0.20846784 ...  0.3286434  -1.178434\n",
      "    0.30733705]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.93656707  1.5115631  -1.9923961  ...  1.2093015   0.836621\n",
      "   -1.8291692 ]\n",
      "  [ 1.3866473   0.42721468  0.18373072 ... -0.39527118 -0.3948915\n",
      "   -0.12453961]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.59928936 -0.16618967 -0.8276651  ... -0.5798982   0.47032553\n",
      "   -0.9148379 ]\n",
      "  [-0.27490443  2.6324186   0.46635562 ...  0.47770858  1.5135187\n",
      "   -1.0213397 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.9605384   1.2493688  -1.4572594  ...  0.23356763 -0.27974004\n",
      "   -1.7200202 ]\n",
      "  [ 0.7814847   1.9471185   1.3258171  ...  0.93342996  0.8764138\n",
      "   -1.377321  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.607265    0.02382052 -1.1695963  ...  0.40783033 -1.0375452\n",
      "   -1.6151013 ]\n",
      "  [ 0.7195195   1.6956173   0.7601681  ...  1.0155501   0.633567\n",
      "   -0.32585698]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.2270672   1.6246183  -0.7111518  ...  0.97977924 -1.0356883\n",
      "   -1.2842333 ]\n",
      "  [ 0.08331236  0.7818339   2.3985262  ...  0.20716615 -0.70893866\n",
      "   -0.18814176]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.12643832  0.8992672  -2.0135226  ... -0.77297044  0.0352236\n",
      "    0.16593635]\n",
      "  [-0.22601947  1.0526289   0.760046   ...  0.05325156 -0.21857782\n",
      "   -0.26890606]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.8727033   1.3016064  -1.6662797  ... -0.7015338   0.35174924\n",
      "   -0.41784877]\n",
      "  [ 0.17010838  1.1005222   1.0273744  ...  0.9406871  -0.73784\n",
      "   -2.4904318 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.3076584  -0.20618832 -3.003406   ...  0.4735923   0.338579\n",
      "   -0.3687429 ]\n",
      "  [ 0.85935557  1.9119561  -0.05594122 ...  1.3516576   0.07517821\n",
      "   -0.9341631 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.9911978  -0.0451417  -1.0208945  ... -1.848179   -0.7805275\n",
      "   -1.2211826 ]\n",
      "  [-0.4131095   1.2818886   2.4304733  ...  0.8491601   0.9295079\n",
      "   -0.6663257 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.07103884  1.7862184  -2.6311488  ...  0.41699776 -0.50032014\n",
      "   -0.5488918 ]\n",
      "  [-1.3795686   1.6772668   0.43075562 ...  0.94212425  0.6298642\n",
      "   -1.712337  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.9083423   1.2580985  -2.4038088  ... -1.1262584   0.06012332\n",
      "   -1.2843912 ]\n",
      "  [ 1.0676936   0.45935142  0.85691345 ... -0.8009532   0.13953884\n",
      "    0.01967168]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.048942    1.1246736  -1.6192681  ... -0.97633517 -1.1815472\n",
      "   -0.53963447]\n",
      "  [ 0.31494117  2.5490713   1.4173281  ...  0.6617844  -0.3388585\n",
      "   -1.3483125 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.61545324  2.4233336  -1.1578794  ... -0.27287552 -0.9143568\n",
      "   -0.9630411 ]\n",
      "  [ 0.7469145   0.29318333  0.67265964 ...  0.36769158  0.0994903\n",
      "    0.08091331]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.44670832  1.5175958  -0.9966155  ... -0.673019    0.9947763\n",
      "   -0.47856194]\n",
      "  [ 0.84601486  1.9314361   0.61843354 ... -0.5810769   1.3118159\n",
      "   -0.62095314]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.44124895  2.6826737  -3.1966672  ... -0.9595181  -0.4194582\n",
      "    0.9791353 ]\n",
      "  [ 0.08613497  1.6841865   2.6289666  ...  0.06073692  1.5587373\n",
      "   -0.95458204]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.23433173  0.96646774 -1.9404864  ...  0.24374089 -1.7499318\n",
      "   -1.4604659 ]\n",
      "  [-0.25720292  0.35263842  0.2681468  ... -1.1675675  -0.57829005\n",
      "   -2.2497864 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.0121802   1.391667   -2.4896765  ... -0.73386306 -0.38928783\n",
      "   -0.05559587]\n",
      "  [-0.8314527   2.2296963   0.7842794  ...  1.0338728   0.7059519\n",
      "   -1.0510099 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.7618473   2.1039586  -1.7909281  ...  0.09921297  0.13045359\n",
      "   -0.6657386 ]\n",
      "  [-0.02161634  0.6215948   0.2531548  ... -0.3360235   0.91085064\n",
      "   -0.7507702 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.6222092   1.0379993  -1.2990618  ... -0.4038508   0.63164526\n",
      "   -0.26833457]\n",
      "  [-0.70972085  1.8450184   0.37070996 ... -0.6252608  -0.48031533\n",
      "   -0.5387778 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.300311    1.6938056  -0.97823924 ... -0.11705971  0.4162529\n",
      "   -1.5396242 ]\n",
      "  [-1.2710339   1.4805369   2.2269068  ...  0.17612717 -0.65728205\n",
      "   -2.6496565 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.76441175  2.1242013  -1.0213636  ... -0.7484643  -1.1413884\n",
      "    0.6986971 ]\n",
      "  [-0.14123468  1.7644081   0.49312502 ...  0.47860828  0.77560806\n",
      "   -1.1670084 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.6409347   0.8942011  -1.1877458  ... -0.23879229  0.58291864\n",
      "    0.7139231 ]\n",
      "  [ 0.0967941   1.441061    1.9898307  ...  1.983294   -1.1473639\n",
      "   -2.434067  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.4448006   0.25128514 -1.4095477  ...  0.01831988 -1.0678375\n",
      "   -1.0085704 ]\n",
      "  [-0.8547331   1.609801    0.82384384 ... -1.2216833   0.32095462\n",
      "   -1.46139   ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.278652    1.5383962  -0.8824327  ...  0.1311011   0.63860005\n",
      "    0.12509185]\n",
      "  [ 0.11150211 -0.01281977  1.3969446  ...  0.5265933   0.4470958\n",
      "   -2.4782827 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.5515571   1.3841505  -2.065411   ... -0.7941779  -0.6253767\n",
      "   -2.187746  ]\n",
      "  [-0.08461771  2.3418546   2.0384135  ...  0.20294493 -0.66115004\n",
      "   -0.74314255]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.4678524   0.8312627  -2.3292222  ... -1.1432662  -1.4133811\n",
      "   -1.9679086 ]\n",
      "  [ 0.3050861   1.262789    0.17836529 ...  2.1329691   0.57577085\n",
      "   -1.7174819 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.8810155   0.51788455 -0.999612   ... -1.8574036   0.30902243\n",
      "    0.7670182 ]\n",
      "  [ 0.32521257  1.7970314   1.8082502  ... -0.3264212   0.28541145\n",
      "   -1.3020493 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1499144   0.20154321 -1.6062899  ... -0.49401814  0.9231455\n",
      "   -1.3032606 ]\n",
      "  [ 0.05769116  1.427344    0.9089278  ...  0.2331261   0.9272476\n",
      "   -0.8031201 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-2.1331871e-01 -3.7896746e-01  9.8948789e-01 ... -1.4485674e+00\n",
      "    2.9604977e-01 -8.1329250e-01]\n",
      "  [-1.2650677e+00  6.9411981e-01 -7.6810712e-01 ... -2.1117449e-01\n",
      "   -1.7114203e+00 -1.6437905e+00]\n",
      "  [ 1.1763237e-03  1.5449409e+00  2.6633918e-01 ...  8.0826044e-01\n",
      "    7.9684883e-02 -6.3570535e-01]\n",
      "  ...\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.04673642  1.4539392  -0.89532983 ...  0.1267648  -0.5370168\n",
      "   -0.8172675 ]\n",
      "  [-0.7545922   1.1838675   0.85446465 ...  1.176211   -0.1355154\n",
      "   -0.277412  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.76484096  0.19297063 -2.1981378  ... -0.8429915  -0.33577663\n",
      "    0.73000646]\n",
      "  [ 0.02205215  1.383836    0.17139858 ... -0.19159842  0.63562167\n",
      "   -1.6696625 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.32475257  1.3552309  -1.8237346  ... -0.6386281   0.42512113\n",
      "    0.5482929 ]\n",
      "  [-0.5251137   0.8821396  -0.2748338  ...  0.5661992   0.4443125\n",
      "   -0.34385216]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.6849866   1.0581299  -1.7207428  ...  0.24155107 -0.71101373\n",
      "    0.10785931]\n",
      "  [ 0.21259159 -0.52298236  2.242088   ...  0.4642525   0.7018095\n",
      "   -1.5679944 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-2.1331871e-01 -3.7896746e-01  9.8948789e-01 ... -1.4485674e+00\n",
      "    2.9604977e-01 -8.1329250e-01]\n",
      "  [-1.8416291e+00  2.2135892e+00 -1.1342722e+00 ... -6.5200400e-01\n",
      "   -3.0715603e-01 -1.0030389e-02]\n",
      "  [-1.1537820e-03  1.0848246e+00  3.7616521e-01 ...  5.5057800e-01\n",
      "    1.0379903e+00 -1.2922410e+00]\n",
      "  ...\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.26483577  2.5987203  -1.5194687  ... -0.59334695 -0.387187\n",
      "    0.47103608]\n",
      "  [-0.7746892   2.93568     0.65809435 ...  1.4740858  -0.43962097\n",
      "   -0.17296511]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.26483577  2.5987203  -1.5194687  ... -0.59334695 -0.387187\n",
      "    0.47103608]\n",
      "  [-0.7746892   2.93568     0.65809435 ...  1.4740858  -0.43962097\n",
      "   -0.17296511]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.45625287  0.30147713 -2.3914766  ... -1.3299311  -0.97038984\n",
      "    0.2607317 ]\n",
      "  [ 0.19052847  2.1445284   1.9484905  ...  0.27280223  0.67677957\n",
      "   -0.37822723]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.04698586  0.5992478  -2.2409282  ... -2.1180089  -1.193385\n",
      "    0.12508291]\n",
      "  [ 0.92954624  0.7685518   1.8430524  ...  1.6827137   0.7267313\n",
      "   -0.7951998 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.73986596  2.0187907  -0.626717   ... -0.66185766  0.10128123\n",
      "    0.00704408]\n",
      "  [-0.11854758  1.2504112   0.95593935 ... -0.04167524 -0.6060489\n",
      "   -0.9249058 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.09199458  1.3549194  -1.4163892  ... -1.3396808  -0.72602224\n",
      "    0.5182135 ]\n",
      "  [ 1.3723191  -0.36439526  0.46975648 ... -0.20118785  0.47844112\n",
      "   -0.8086337 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.09199458  1.3549194  -1.4163892  ... -1.3396808  -0.72602224\n",
      "    0.5182135 ]\n",
      "  [ 1.3723191  -0.36439526  0.46975648 ... -0.20118785  0.47844112\n",
      "   -0.8086337 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.09199458  1.3549194  -1.4163892  ... -1.3396808  -0.72602224\n",
      "    0.5182135 ]\n",
      "  [ 1.3723191  -0.36439526  0.46975648 ... -0.20118785  0.47844112\n",
      "   -0.8086337 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.6661489   0.43741924 -1.5606484  ... -0.79355747 -0.67902714\n",
      "   -2.0021462 ]\n",
      "  [ 0.79378676  0.80748594  0.6045641  ...  1.5488982   0.17594558\n",
      "    0.16280007]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.2249873   2.2510161  -2.4435043  ... -1.3186014  -1.0070179\n",
      "   -0.17505473]\n",
      "  [ 0.0565136   1.1251848   2.1245446  ... -0.6242482   0.47804803\n",
      "   -1.9435048 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.0357603   1.4046755  -2.2590823  ... -0.13435099 -1.3596032\n",
      "   -1.4985797 ]\n",
      "  [-1.2819275   1.047023   -0.08538544 ...  0.33869255 -0.21877648\n",
      "   -1.3107469 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.3910002   0.7774495  -2.3101068  ... -0.2959172  -1.3082368\n",
      "   -0.0820666 ]\n",
      "  [ 0.7699425   1.4898862   1.4560415  ...  0.18632022  0.493901\n",
      "   -1.0982878 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.3910002   0.7774495  -2.3101068  ... -0.2959172  -1.3082368\n",
      "   -0.0820666 ]\n",
      "  [ 0.7699425   1.4898862   1.4560415  ...  0.18632022  0.493901\n",
      "   -1.0982878 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.56096995  1.6518645  -0.45046234 ... -2.0479708  -0.47135815\n",
      "   -0.0567835 ]\n",
      "  [-0.0965137   1.7066419   2.645688   ...  0.3604448   0.37970334\n",
      "   -1.410692  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.84907615  1.9731114   0.19565189 ...  0.4056907   0.60485667\n",
      "   -1.842312  ]\n",
      "  [-0.5251137   0.8821396  -0.2748338  ...  0.5661992   0.4443125\n",
      "   -0.34385216]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.99216646  0.5123605  -1.6743138  ...  0.38798162  1.2089257\n",
      "   -0.40738398]\n",
      "  [-0.35512662  0.34851182  2.129962   ...  1.167949   -0.05099288\n",
      "   -2.3164358 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.29458869  0.8868762  -1.0268106  ...  1.2488691  -0.11685392\n",
      "   -1.1386735 ]\n",
      "  [-0.9598093   0.76489747  1.2589383  ...  1.3400079   0.02252106\n",
      "   -1.9108498 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.9564239   1.9298491  -1.0787637  ...  0.74576163 -0.7430708\n",
      "   -1.4177564 ]\n",
      "  [-0.14170097  0.6250086  -0.13316739 ... -0.3818161   1.4512085\n",
      "   -1.331359  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.0418852   2.0115273  -1.2839055  ...  0.79932857  0.7964093\n",
      "   -1.1100383 ]\n",
      "  [-0.5523598   2.229332    0.32956022 ...  1.2261957   0.38641775\n",
      "   -1.7627945 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.0720954   0.49927104 -2.708375   ... -1.316476    0.8757102\n",
      "   -0.9460877 ]\n",
      "  [-0.24496335  0.7153959  -0.03737187 ...  0.93291146  0.26692167\n",
      "    0.41521895]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.4827542   2.1035943  -2.2456472  ...  0.29153576 -0.18908057\n",
      "   -1.3775232 ]\n",
      "  [-0.80580664  0.93717563  0.70422703 ...  0.74997425  0.20252466\n",
      "   -1.1906595 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.2779967   2.0971904  -1.5487249  ... -0.9990692   0.00686005\n",
      "   -1.5227516 ]\n",
      "  [-0.53142405  2.9545445   0.794418   ... -0.05440235 -0.1246977\n",
      "   -0.53568447]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1254257   1.4535196  -0.9758272  ...  0.27781084 -0.5875454\n",
      "    0.84468174]\n",
      "  [ 0.3033941   2.2707462   2.0947733  ... -0.62896496  1.245763\n",
      "   -1.7192976 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.41884363  1.0071945  -0.9680899  ... -0.11031404 -0.875126\n",
      "    0.6003635 ]\n",
      "  [-0.7037654  -0.10275555  0.48378855 ...  0.7576314  -0.04372138\n",
      "   -0.10589707]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.41884363  1.0071945  -0.9680899  ... -0.11031404 -0.875126\n",
      "    0.6003635 ]\n",
      "  [-0.7037654  -0.10275555  0.48378855 ...  0.7576314  -0.04372138\n",
      "   -0.10589707]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.5810078   1.6337733  -1.5736809  ...  0.9374722  -0.2660191\n",
      "   -1.579181  ]\n",
      "  [ 0.2833775   1.6487418   0.41545385 ... -0.7019493   0.04608827\n",
      "   -1.3238115 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.62700033  2.1450086  -0.4804343  ... -1.5636249   0.67026466\n",
      "   -1.3340263 ]\n",
      "  [-0.6055342   0.4435864   0.8683087  ...  0.85532564  0.66903734\n",
      "   -2.1082683 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.7682191   2.3419194  -2.3511274  ...  0.58670413 -0.6960188\n",
      "   -0.7491235 ]\n",
      "  [-0.36860523  1.0545053   0.16393465 ... -0.37004328  1.7001027\n",
      "   -1.181744  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.12325907  0.97084385 -0.98431367 ...  0.9765264   0.06154263\n",
      "   -0.26022232]\n",
      "  [-0.19474506  1.2508441   1.5291973  ...  0.8778765   0.31419092\n",
      "   -1.556049  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.5782654   0.34034568 -2.236404   ... -0.35619494 -1.2333543\n",
      "   -0.13438469]\n",
      "  [-0.02602944  2.0555868   1.4964437  ...  1.6804216  -0.16757245\n",
      "   -1.8030277 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.4600205   1.79478    -0.75403523 ... -1.0906864   0.17518783\n",
      "   -0.8911976 ]\n",
      "  [ 0.03589983  1.2615383   2.64039    ...  0.56546533  1.3017095\n",
      "   -1.8437986 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.0504578   0.8944038  -0.32819855 ... -0.45665193 -0.89970756\n",
      "   -1.862539  ]\n",
      "  [ 1.3866473   0.42721468  0.18373072 ... -0.39527118 -0.3948915\n",
      "   -0.12453961]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.9564239   1.9298491  -1.0787637  ...  0.74576163 -0.7430708\n",
      "   -1.4177564 ]\n",
      "  [-0.724211    0.7915058   0.05697703 ... -0.68337184  1.5171312\n",
      "   -2.3562305 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.04236227  1.1157625  -1.7852294  ... -0.6256511  -0.63861674\n",
      "    0.43271136]\n",
      "  [ 0.2626257   1.4724399   0.899666   ...  1.0092487   0.7626189\n",
      "   -1.0531205 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.030875   -0.5009146  -2.4385128  ...  0.35039243 -1.0423177\n",
      "    0.33755946]\n",
      "  [ 1.3225728   2.9447541   1.9891355  ...  0.57049006 -0.09473576\n",
      "   -0.8441876 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.6677687   1.3467023  -1.6755415  ...  0.07458881  0.18712056\n",
      "   -0.6678492 ]\n",
      "  [ 0.07367779  0.5349529   0.06934679 ...  1.9990425   0.6139545\n",
      "   -1.9640213 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.4511838   1.7129693  -1.1173403  ... -0.34616268  0.2251774\n",
      "   -0.20582002]\n",
      "  [ 1.3426592   0.9688785   0.27128798 ... -0.93169206 -1.53145\n",
      "   -1.0968087 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.9605384   1.2493688  -1.4572594  ...  0.23356763 -0.27974004\n",
      "   -1.7200202 ]\n",
      "  [ 1.0530281   0.85101044  0.3183114  ...  1.1100218   0.35034883\n",
      "   -0.79906416]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.8211132   1.5959482  -0.75979185 ...  0.08450267  0.01858833\n",
      "    0.01962906]\n",
      "  [-0.10536583  1.5304132   0.3161252  ...  0.80030894 -0.7841048\n",
      "   -1.883851  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.9129583  -0.49717295 -0.9544703  ...  0.15830132  0.17137522\n",
      "    0.53878343]\n",
      "  [-0.14591855  1.574766   -0.02713466 ... -0.12584072 -0.0872197\n",
      "   -1.3430358 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.91440326  2.3757823  -0.26616597 ... -0.42534626 -0.3968538\n",
      "   -1.9142405 ]\n",
      "  [ 0.6447009   0.7770286   1.9612281  ...  0.37425566  1.9983706\n",
      "   -0.50500864]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.4175415   0.4102661  -1.7336327  ...  0.52607405 -0.6669587\n",
      "   -0.06156087]\n",
      "  [ 0.03869005  1.615377    1.675875   ...  0.76071584  0.95341593\n",
      "   -1.1565262 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.5112653   2.3457155  -1.1423262  ... -0.87936616 -0.18547873\n",
      "   -0.88769686]\n",
      "  [ 0.41912916  2.471453    1.4328812  ...  0.05529374  0.3900196\n",
      "   -1.2729682 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.7296533   1.4701946  -1.1331452  ... -0.34967178 -0.9132875\n",
      "   -1.2242364 ]\n",
      "  [-0.5740086   2.0496974   1.1523095  ...  0.42802408  0.19653845\n",
      "   -1.1078181 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.4639452   0.92064464 -2.0650563  ... -0.86912787  0.5424418\n",
      "   -1.5162524 ]\n",
      "  [-0.21699858 -0.00286365  0.69106627 ...  1.5768883   0.5583608\n",
      "   -2.2355533 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.16975027  2.497441   -1.1507969  ... -0.75039417 -0.28513932\n",
      "    0.06399602]\n",
      "  [ 0.38082105  1.5038023   1.26615    ...  0.6522216   1.3300614\n",
      "   -2.4811807 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1664742   0.76072204 -1.3834052  ... -0.1736919   0.76239485\n",
      "    0.63735604]\n",
      "  [ 0.83358264  1.0655578   0.5401704  ...  1.0127664   1.5402706\n",
      "   -1.1671945 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.01678818  0.66725695 -1.4527332  ... -0.8053383  -0.49645865\n",
      "   -0.14472407]\n",
      "  [ 1.1390826   1.5354763   0.6703691  ...  1.1797161  -0.14940749\n",
      "   -1.3355846 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.5696806   0.15592188 -1.8755857  ... -0.74831355  0.4821821\n",
      "   -2.0617151 ]\n",
      "  [-0.593992    0.09450614 -0.25258827 ...  1.8531089   0.14570762\n",
      "   -1.8121223 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.25606644  1.3996208  -1.7053341  ... -2.071867   -0.7836291\n",
      "   -0.5051407 ]\n",
      "  [-1.3397639   1.3981626   0.60360634 ...  0.4613501   0.8584405\n",
      "   -1.3465396 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.88209796  2.7202601  -1.3735683  ... -0.6370992  -0.6178636\n",
      "   -1.778091  ]\n",
      "  [ 0.8399559   1.5144331   0.22447264 ...  1.2921956  -0.246224\n",
      "   -1.8661107 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1962677   1.0726953  -0.5651732  ...  0.05417553 -0.2012027\n",
      "    0.5830511 ]\n",
      "  [-0.3551706  -0.22432351  0.972257   ...  1.0143263  -0.954879\n",
      "   -1.0930111 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.5243864  -0.0312314  -2.8277957  ...  0.91844904 -0.4297907\n",
      "   -1.426851  ]\n",
      "  [ 0.88340855  0.72498536  0.33427942 ... -1.1833489  -0.61788666\n",
      "   -0.2601884 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.5243864  -0.0312314  -2.8277957  ...  0.91844904 -0.4297907\n",
      "   -1.426851  ]\n",
      "  [ 0.88340855  0.72498536  0.33427942 ... -1.1833489  -0.61788666\n",
      "   -0.2601884 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.8462914   2.2137103  -1.8711588  ...  0.5062374  -1.4466088\n",
      "   -1.0939908 ]\n",
      "  [ 0.43222514  0.31054175  1.3833102  ...  0.7345028   1.3252805\n",
      "   -0.99360716]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.4300582   0.6059793  -1.401414   ... -1.6262146  -0.31266844\n",
      "   -1.3133528 ]\n",
      "  [-0.11546365  0.38310552 -0.2990445  ... -0.0906176  -0.594292\n",
      "   -1.4620684 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.0458581   0.25736797 -2.8742518  ... -1.0252775  -1.1697903\n",
      "   -1.0767971 ]\n",
      "  [ 0.49268714  2.6075718   0.9439084  ...  0.6479167  -1.1402559\n",
      "   -1.1480464 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.35928255  1.4174321  -0.45944798 ... -0.18500721  1.262367\n",
      "   -1.863315  ]\n",
      "  [ 0.48669556  2.660787    1.2034292  ... -0.8961088   0.88736576\n",
      "   -1.5665225 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.8757739  -0.46708667 -2.7836752  ... -0.6060165  -1.7539322\n",
      "    0.69260836]\n",
      "  [ 0.2626257   1.4724399   0.899666   ...  1.0092487   0.7626189\n",
      "   -1.0531205 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1388761  -0.42325485 -1.354343   ...  0.5569296   1.2645116\n",
      "   -0.20794964]\n",
      "  [-0.08862313 -0.3230455   1.4565206  ... -0.31642658  0.21422598\n",
      "   -0.96539855]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.1491766   0.76041794 -1.0515704  ...  0.09673318 -0.49343663\n",
      "   -1.6597726 ]\n",
      "  [-0.84267366  2.0109587   0.88065195 ...  1.1087124  -0.27437532\n",
      "   -2.2372708 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.1491766   0.76041794 -1.0515704  ...  0.09673318 -0.49343663\n",
      "   -1.6597726 ]\n",
      "  [-0.84267366  2.0109587   0.88065195 ...  1.1087124  -0.27437532\n",
      "   -2.2372708 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.1291852   0.22868001 -1.5330051  ... -0.998572   -0.13608974\n",
      "   -0.65144473]\n",
      "  [ 0.6996051   0.7118989   2.355276   ...  0.73057497 -0.74600846\n",
      "   -2.016583  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.80074984  2.1599474  -2.0545115  ...  0.7088984   0.48108155\n",
      "   -1.079648  ]\n",
      "  [ 0.1720771  -0.43091142  1.542229   ...  0.17132795  0.34485632\n",
      "   -0.2688132 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.80074984  2.1599474  -2.0545115  ...  0.7088984   0.48108155\n",
      "   -1.079648  ]\n",
      "  [ 0.1720771  -0.43091142  1.542229   ...  0.17132795  0.34485632\n",
      "   -0.2688132 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.633292    1.2999256  -2.0275483  ... -1.3679285  -0.6904084\n",
      "   -1.3927746 ]\n",
      "  [-0.17571309  1.064263    2.1675293  ...  0.037552    0.27334195\n",
      "   -0.43776774]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.633292    1.2999256  -2.0275483  ... -1.3679285  -0.6904084\n",
      "   -1.3927746 ]\n",
      "  [-0.17571309  1.064263    2.1675293  ...  0.037552    0.27334195\n",
      "   -0.43776774]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.1996408   2.3595161  -0.8711194  ... -0.77466977 -0.5523109\n",
      "    0.3455857 ]\n",
      "  [ 0.00714146  1.8653002  -0.599167   ...  0.964373    0.01518933\n",
      "   -1.9278886 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.2075398   0.7139542  -2.3203247  ... -0.35265422 -0.10824633\n",
      "    0.14828771]\n",
      "  [-0.2658733   1.1984328   2.0100343  ...  0.98883545  0.37429562\n",
      "    0.19777977]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.44099444  0.40402073 -2.2569635  ... -1.163117   -0.7003591\n",
      "   -0.1835342 ]\n",
      "  [-1.226092    0.7487802   1.1980201  ...  0.5707668  -0.38160437\n",
      "   -1.7157216 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.4874683   2.4199047   0.10019493 ...  0.5275972   0.428294\n",
      "   -0.47433695]\n",
      "  [ 0.01478584  0.00357366  0.3550452  ... -0.6310378   0.24757159\n",
      "   -0.26754218]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.7385604   2.0251107  -3.0004582  ...  0.5618291  -0.30413526\n",
      "   -1.0922992 ]\n",
      "  [ 0.01817317  1.7272714   1.3644956  ... -0.15121746 -1.470965\n",
      "   -1.1341    ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.018801    0.38802862 -1.5218627  ...  0.641662    1.0102739\n",
      "   -1.6526507 ]\n",
      "  [ 1.3494576   2.106286    0.8166913  ... -0.10297507  0.77942073\n",
      "   -1.0986819 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.8671285  -0.26500797 -1.985825   ... -0.24378006  0.4965992\n",
      "   -1.8458781 ]\n",
      "  [-0.09980999  1.1746004   1.8041419  ...  0.7446922  -0.10691692\n",
      "   -1.9574096 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.8423901  -0.13566852 -1.5662017  ... -0.6549607  -1.3281555\n",
      "   -0.5658342 ]\n",
      "  [ 0.5711119   1.5431697   2.1157594  ...  0.74965274  1.8378654\n",
      "   -2.2485862 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1061075   0.9385255  -0.40767813 ... -0.89710796 -0.30215636\n",
      "   -0.05249643]\n",
      "  [-0.8187022  -0.20788014  2.2146144  ...  0.7590414  -0.14017643\n",
      "   -1.9015987 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.0302044   1.0488628  -0.7710656  ... -0.18996772 -0.68241525\n",
      "   -1.5721383 ]\n",
      "  [ 0.48865756  1.4213152   0.89442885 ...  1.7094059   1.3251178\n",
      "   -0.03567708]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.0302044   1.0488628  -0.7710656  ... -0.18996772 -0.68241525\n",
      "   -1.5721383 ]\n",
      "  [ 0.48865756  1.4213152   0.89442885 ...  1.7094059   1.3251178\n",
      "   -0.03567708]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.10130012  1.8663907  -1.1391797  ...  0.696782    0.3413337\n",
      "   -0.48300183]\n",
      "  [-0.6746702   1.1975944   1.3662195  ...  1.3347968  -0.19759853\n",
      "   -0.5250213 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.22114617  0.1996203  -1.8732464  ...  0.6623317   0.5556644\n",
      "   -1.7887585 ]\n",
      "  [-0.42769077  1.0782872   1.2043521  ... -0.16101635 -1.305505\n",
      "   -1.1484828 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.8743069   1.9898679  -2.348189   ... -0.8231632  -1.475754\n",
      "   -0.45881844]\n",
      "  [-1.4681997   0.09451151  0.9000101  ...  1.3381816  -0.17491294\n",
      "   -1.0851365 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.3580852   0.9525497  -1.3708553  ... -1.0956763  -1.8810034\n",
      "   -0.7632114 ]\n",
      "  [ 1.1327009   0.72706056  0.4072885  ... -0.85340554 -0.25437188\n",
      "   -0.7224803 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.3985941  -0.03122604 -1.6751974  ...  0.4035217  -0.7504113\n",
      "   -0.6998653 ]\n",
      "  [ 1.1331427   1.9846053   0.43425834 ... -0.41729504 -1.3157153\n",
      "   -1.9173132 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.5133759   0.6228951  -1.8014019  ...  0.7064259  -1.1185405\n",
      "   -0.73822635]\n",
      "  [ 0.0565136   1.1251848   2.1245446  ... -0.6242482   0.47804803\n",
      "   -1.9435048 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.0357603   1.4046755  -2.2590823  ... -0.13435099 -1.3596032\n",
      "   -1.4985797 ]\n",
      "  [-0.7189342   0.78924274  0.49958366 ... -0.30949992  0.745025\n",
      "   -0.67149884]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.87388086  0.9994473  -0.45066285 ... -1.5589081  -0.09745032\n",
      "   -1.5582335 ]\n",
      "  [-1.3179872   1.8625869   0.03750169 ... -0.6150376   1.5270739\n",
      "   -0.26149476]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.285521    0.22277427 -0.4452455  ...  0.23328903 -0.6264912\n",
      "   -1.9311645 ]\n",
      "  [-0.07266527  1.6710925  -0.01317263 ...  1.9478157  -0.30153942\n",
      "   -2.6961555 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.2436721   1.8689485  -0.9991745  ... -1.3235364   0.02620688\n",
      "   -0.06634265]\n",
      "  [-1.5736314   2.073793    1.2796034  ...  0.8305301  -0.6903627\n",
      "   -1.454255  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.2436721   1.8689485  -0.9991745  ... -1.3235364   0.02620688\n",
      "   -0.06634265]\n",
      "  [-1.5736314   2.073793    1.2796034  ...  0.8305301  -0.6903627\n",
      "   -1.454255  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.6279194  -0.06670153 -1.3397039  ... -0.2798518   1.2631838\n",
      "   -0.15827477]\n",
      "  [ 0.21374726  0.6481176   1.5544168  ...  1.528913   -0.5977499\n",
      "    0.3283987 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.6256194   0.7262149  -0.7843522  ... -0.22788    -0.15360385\n",
      "   -0.07113367]\n",
      "  [-0.26170677  1.5472921   1.5389394  ...  0.890403   -0.29553056\n",
      "   -2.0068893 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.4588306   0.21949989 -1.2983824  ...  0.02601925  1.2219718\n",
      "   -1.4549096 ]\n",
      "  [ 1.055439    0.4993251   0.3038364  ...  1.0216898   1.068312\n",
      "   -1.4012147 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.21087497  1.5698798  -1.8150394  ...  0.08089027  0.05806863\n",
      "    0.05941433]\n",
      "  [ 0.15191324  1.3654529   1.0211544  ...  0.32625824 -0.01127049\n",
      "   -1.9688883 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.796682    1.2564366  -1.6184776  ... -0.56688184  0.16534835\n",
      "   -1.886595  ]\n",
      "  [-0.98263955  1.1288773   1.8059762  ... -0.65496284  0.4774313\n",
      "   -0.4619254 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.7546607   0.08814752 -2.3622246  ... -1.1222634   0.34434003\n",
      "   -0.1808638 ]\n",
      "  [-0.652382    0.741768    1.2591066  ...  1.5948477   1.1605135\n",
      "   -2.4679914 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.7075212   1.4372172  -0.88963526 ...  0.38859728 -0.26012927\n",
      "    0.9394492 ]\n",
      "  [-0.16155928  0.6387505   0.7944274  ...  0.6072178  -1.0434495\n",
      "   -0.7234827 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1068325   1.8612599  -1.2873558  ...  0.33472934 -1.4706538\n",
      "   -1.6124959 ]\n",
      "  [ 0.2448754   1.7408371   0.38103276 ...  1.5871837   0.9850854\n",
      "   -0.9208065 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.131266    0.5916102  -1.2919666  ... -0.64648837  1.0550494\n",
      "   -0.08658385]\n",
      "  [-1.5682169   1.0883858   1.3926891  ...  0.79699504 -0.13816719\n",
      "   -1.394155  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.41196376  0.36980957 -0.816436   ... -2.0780861  -1.8154907\n",
      "    0.24362719]\n",
      "  [-1.3531975   1.2533717   1.3381335  ...  0.10482523 -0.70190334\n",
      "   -1.435576  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.5079407   2.4805908  -1.0179965  ... -1.6543461  -0.86141545\n",
      "   -2.07661   ]\n",
      "  [-0.39862037  1.6504116   0.7033104  ... -1.2311635  -0.20797332\n",
      "   -0.9091935 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.01975584  2.0959013  -1.4808793  ...  0.13274875 -0.46611282\n",
      "   -0.96080184]\n",
      "  [ 0.30477506  0.85195243  1.7908553  ...  0.70677996  0.4218945\n",
      "   -0.45640498]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.8470821   0.65609634 -0.1766814  ... -0.72749376 -1.284437\n",
      "    0.19712955]\n",
      "  [-0.14170097  0.6250086  -0.13316739 ... -0.3818161   1.4512085\n",
      "   -1.331359  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.12263364  0.7252729  -2.256896   ...  0.17536196 -0.22514951\n",
      "   -0.41379282]\n",
      "  [-1.219884    1.2141751   1.674361   ... -0.23232192  0.07035956\n",
      "   -0.28194827]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.16679126  1.895693   -1.1625409  ...  0.53169715 -0.26854044\n",
      "   -1.114425  ]\n",
      "  [ 0.24217269  2.1180613   0.47947    ... -0.4492445  -0.40552938\n",
      "    0.02807951]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.9511638   0.40679783 -1.6066558  ... -1.2187393  -1.4957452\n",
      "    0.09809917]\n",
      "  [ 0.8654524  -0.3940742   0.60864466 ...  0.48567447 -0.5948997\n",
      "   -1.8369405 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.0720954   0.49927104 -2.708375   ... -1.316476    0.8757102\n",
      "   -0.9460877 ]\n",
      "  [-1.3179163   0.28459495  1.1380782  ...  0.6593099  -0.69882524\n",
      "   -0.67242086]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.75831735 -0.55664897 -1.0329784  ... -0.763332   -0.23064202\n",
      "    0.11645812]\n",
      "  [-1.1172165   2.5729747   1.9996052  ...  0.26977527  0.75666887\n",
      "   -1.5123336 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.2404773   2.7059026  -1.3592575  ... -0.6151367  -0.01120239\n",
      "   -0.9697458 ]\n",
      "  [-0.1980186   1.5423739   1.1334128  ...  0.58476186  1.8733042\n",
      "   -1.6545649 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.2404773   2.7059026  -1.3592575  ... -0.6151367  -0.01120239\n",
      "   -0.9697458 ]\n",
      "  [-0.1980186   1.5423739   1.1334128  ...  0.58476186  1.8733042\n",
      "   -1.6545649 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1284131   1.4166363  -1.4417946  ... -0.34989804  1.297806\n",
      "   -1.2692935 ]\n",
      "  [-0.17238608  1.7863796   1.2000974  ...  0.91314745  0.10557292\n",
      "   -1.0907927 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.29458869  0.8868762  -1.0268106  ...  1.2488691  -0.11685392\n",
      "   -1.1386735 ]\n",
      "  [-1.1892542   1.7896454   0.36831647 ...  0.596112   -1.4557158\n",
      "   -2.7698739 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.68315625  1.2898401  -2.4222634  ...  0.304544   -0.7692145\n",
      "   -1.9147379 ]\n",
      "  [-0.7545922   1.1838675   0.85446465 ...  1.176211   -0.1355154\n",
      "   -0.277412  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.3349111   1.4778144  -2.5885177  ... -0.33939165 -1.4269328\n",
      "   -1.6659306 ]\n",
      "  [-0.5182532   1.4413797   0.37359732 ...  0.42523313 -0.16186057\n",
      "   -0.17038572]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.93873686  0.12763244 -2.0382948  ...  0.59816086 -0.5934548\n",
      "   -1.5928955 ]\n",
      "  [-0.29526633  1.7450056   1.9043872  ...  0.7870548  -0.43042552\n",
      "   -1.098072  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.17093188  1.4867296  -1.8019873  ...  0.3598238  -0.6183015\n",
      "   -1.7193055 ]\n",
      "  [ 1.4529909   0.24212575 -0.08195221 ...  0.7791308  -0.77213734\n",
      "   -2.6626103 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.17093188  1.4867296  -1.8019873  ...  0.3598238  -0.6183015\n",
      "   -1.7193055 ]\n",
      "  [ 1.4529909   0.24212575 -0.08195221 ...  0.7791308  -0.77213734\n",
      "   -2.6626103 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.20868808  1.4097388  -1.9048383  ...  0.24505618 -0.72490585\n",
      "   -0.9503133 ]\n",
      "  [-0.8254597   0.90389544  0.36588222 ...  1.0721812  -0.46153837\n",
      "   -0.553519  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.7558541   0.7781579  -2.2093253  ...  0.13752136 -1.0370367\n",
      "   -0.1682477 ]\n",
      "  [ 0.52355003 -0.38571835 -0.5841088  ...  1.5548384  -0.37773132\n",
      "   -1.2471288 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.0466695   1.359307   -2.3952217  ... -0.4425242  -0.2710064\n",
      "    0.8736547 ]\n",
      "  [ 0.07080052  1.0198988  -0.15975952 ...  1.4704866   0.25242153\n",
      "   -1.9864572 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.9594705   2.3544986  -0.8790894  ... -1.5156189  -1.2473555\n",
      "   -1.4875886 ]\n",
      "  [ 0.27848727  2.836277    1.9179949  ...  2.006922    1.2197539\n",
      "   -1.9357544 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.374448    0.920573   -0.3976922  ...  0.12627038 -0.70748794\n",
      "   -1.1253053 ]\n",
      "  [ 0.55370235  1.9734571   1.646511   ...  0.04254547 -1.0533562\n",
      "   -2.6935759 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.111104    0.25855058 -2.9944057  ... -1.4372956  -1.6130168\n",
      "   -1.9902363 ]\n",
      "  [ 0.73010397  1.1439755   2.3994505  ...  0.0311645   0.4015867\n",
      "   -1.9884032 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1564139   0.92689127 -1.8151615  ... -0.88140833 -0.79407614\n",
      "    0.11636525]\n",
      "  [ 0.7088976   0.86724424  1.272379   ...  0.18801197  0.72085404\n",
      "   -1.0273551 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.41906315  1.9805484  -1.7585162  ... -1.037635    0.20392239\n",
      "   -0.71341056]\n",
      "  [-0.3827341   0.4421237   1.5085754  ...  1.2140014  -0.56070596\n",
      "   -1.2715313 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.0418852   2.0115273  -1.2839055  ...  0.79932857  0.7964093\n",
      "   -1.1100383 ]\n",
      "  [-1.2793756   2.0252225   0.45014626 ...  0.43174744  0.19141842\n",
      "    0.03758645]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1777709   1.7259603  -1.9101108  ...  0.6172203  -0.72892606\n",
      "    0.07532489]\n",
      "  [-0.67102504  1.6241602   1.5055728  ...  0.43168575  1.212208\n",
      "   -1.1286685 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-2.1331871e-01 -3.7896746e-01  9.8948789e-01 ... -1.4485674e+00\n",
      "    2.9604977e-01 -8.1329250e-01]\n",
      "  [-4.5958662e-01  9.9179840e-01 -1.6629918e+00 ... -8.6186999e-01\n",
      "   -1.1456344e+00  2.9120123e-01]\n",
      "  [-2.8403476e-04  8.9200079e-01  1.6031030e+00 ...  9.4650364e-01\n",
      "    5.7190543e-01  2.0744467e-01]\n",
      "  ...\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.1121495   1.0147957  -1.1881859  ... -1.462028    0.12487459\n",
      "   -0.31063208]\n",
      "  [-1.3100829   2.83164     1.21595    ...  0.31952327  0.56429595\n",
      "   -1.3550172 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.06605077 -0.22469425 -1.511427   ... -0.4958182   1.2179492\n",
      "   -2.4135432 ]\n",
      "  [-1.074219    1.2953411   1.465324   ...  1.3400283   0.6756226\n",
      "   -2.8762136 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.6718185   2.193747   -0.8114735  ... -0.59164023 -0.8666901\n",
      "   -0.5502302 ]\n",
      "  [ 0.79451334  1.708122    0.70056266 ...  1.243124   -0.2470163\n",
      "   -1.0648907 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.64577323  1.6669369  -1.4635764  ... -0.11405483  0.06869537\n",
      "    0.34658563]\n",
      "  [-0.23199028  1.9432456   1.1943079  ...  0.12202552  1.0744747\n",
      "   -0.3891309 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.52813876  0.88408697 -1.8102297  ...  0.84545064  0.1237793\n",
      "   -0.5523142 ]\n",
      "  [-0.08862313 -0.3230455   1.4565206  ... -0.31642658  0.21422598\n",
      "   -0.96539855]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.47753638  1.3789413  -1.9586847  ... -1.0241784   0.11828691\n",
      "   -0.45815393]\n",
      "  [-0.03983457  2.386941    2.1160522  ... -0.32351118  0.44921082\n",
      "   -1.4713231 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.072398    1.2892685  -2.7158499  ...  0.69113755 -0.08950311\n",
      "   -0.09171921]\n",
      "  [-0.42143133  0.8865982   0.9963555  ... -1.138015   -0.53515196\n",
      "   -0.7785318 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.0315762   0.48377687 -1.1108177  ... -1.6191732  -0.79665434\n",
      "   -2.1185007 ]\n",
      "  [ 0.14498322  0.62197673  0.72808325 ...  1.4183018   0.5875476\n",
      "   -2.439681  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.78541124  0.4962392  -1.8471242  ...  0.48364195  0.01204929\n",
      "   -2.05441   ]\n",
      "  [ 0.13640437  1.9168973   2.604076   ...  1.1087298   1.1187457\n",
      "   -0.34438998]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.0877952   1.2306701  -1.8390621  ... -1.6660607   0.1715796\n",
      "   -0.63053834]\n",
      "  [-1.7180297  -0.04467523  1.0990422  ...  0.2479956   0.49859542\n",
      "   -1.7035165 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.7789764   0.04765677 -1.5411286  ... -0.26767814 -0.10783198\n",
      "    0.8789511 ]\n",
      "  [ 0.29019272  1.1505592   0.6701113  ... -0.15503901 -0.01711538\n",
      "    0.53535795]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.4184382   1.3266121  -0.23672163 ...  0.18740067 -0.28378716\n",
      "    0.39501798]\n",
      "  [ 0.5000833   0.08994389  1.7804341  ...  0.2770195   1.4495382\n",
      "   -2.1815875 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.0023056   2.5373154  -0.69268245 ... -1.3767713  -2.0545259\n",
      "   -0.81151026]\n",
      "  [-1.0588373  -0.20102715  0.3874085  ... -0.77424926 -1.0026399\n",
      "   -1.0979198 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.43583447  2.3071103  -1.7416224  ... -0.27847788  0.7333415\n",
      "   -0.21256447]\n",
      "  [-0.11172854  1.3997725   1.4200183  ...  0.669844    0.05186298\n",
      "   -3.032608  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.3203473  -0.48621225 -0.7929351  ...  0.69512284 -0.24382442\n",
      "   -2.1855607 ]\n",
      "  [-0.5027504   1.5328215   0.5370226  ...  1.6120348  -0.697188\n",
      "   -0.7412243 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.35252362  0.30996764 -1.6748288  ...  0.21196964 -0.99793184\n",
      "    0.2855872 ]\n",
      "  [ 1.6736256   1.6967342   0.23340327 ... -0.35797483  0.13340452\n",
      "   -2.2318544 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.4724336   0.66600883 -2.156853   ... -0.72734904 -0.8073001\n",
      "   -1.0286175 ]\n",
      "  [ 0.71853817  1.4211829   1.8921278  ...  1.4772072   1.9309745\n",
      "   -2.8665948 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.24409759  1.7125937  -2.667441   ... -0.5499017  -0.11366597\n",
      "   -1.9799696 ]\n",
      "  [ 0.99413395  1.3585511   1.8724463  ...  0.88528204 -0.5959488\n",
      "   -1.0073215 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.24409759  1.7125937  -2.667441   ... -0.5499017  -0.11366597\n",
      "   -1.9799696 ]\n",
      "  [ 0.99413395  1.3585511   1.8724463  ...  0.88528204 -0.5959488\n",
      "   -1.0073215 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.4331448   1.407084   -2.038185   ...  0.67737496 -1.2726864\n",
      "   -0.35595295]\n",
      "  [-0.26220408  1.905554    1.5870376  ...  0.32761177 -0.12863685\n",
      "   -2.4928493 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.7432311   1.5709966  -2.3418043  ... -1.2926347  -0.4420938\n",
      "   -1.846583  ]\n",
      "  [ 0.30976027  1.6589559   1.4087696  ... -0.5029804  -0.01476781\n",
      "   -1.9971335 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.6206342   1.5332184  -1.1664379  ... -1.4376403  -0.5902661\n",
      "   -1.6118622 ]\n",
      "  [ 0.4212302   1.5842301   2.0879893  ...  0.45641187  0.45933473\n",
      "   -1.4391383 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.7784812   1.2397153  -1.5540531  ... -0.60840166 -0.5867688\n",
      "   -1.583617  ]\n",
      "  [ 1.0280409   2.4064627   2.1905098  ...  1.9083381  -0.92930394\n",
      "   -1.0980898 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.5572322   1.8407116  -1.4827278  ...  0.63593113  0.12896305\n",
      "   -0.6399185 ]\n",
      "  [ 0.14770629  2.5222692   1.2684144  ... -0.6655695  -0.03944863\n",
      "    0.3276441 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.13418531 -0.09671926 -0.7769476  ...  1.0630215  -0.13025418\n",
      "   -0.07946485]\n",
      "  [-0.36280024  0.5984058   0.9085282  ...  0.232535    1.0895855\n",
      "   -1.0744087 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.36470002  2.593964   -0.7149369  ...  0.04643282 -1.0692531\n",
      "   -0.20114964]\n",
      "  [-0.7863122   1.218217    0.36377233 ...  1.7483683   1.6603391\n",
      "   -2.120698  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.90823627  1.0544541  -1.7279809  ... -1.6994735  -0.09752274\n",
      "   -0.5092116 ]\n",
      "  [-0.7209406   0.8349407   1.9945788  ...  0.23979041  1.7509675\n",
      "   -1.0933138 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.02474433  1.8311305  -2.5741625  ... -0.35272244  0.02379119\n",
      "   -0.2943116 ]\n",
      "  [-0.7689208   1.1658573   0.53297865 ...  0.9298794   0.8700928\n",
      "   -1.509356  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.02474433  1.8311305  -2.5741625  ... -0.35272244  0.02379119\n",
      "   -0.2943116 ]\n",
      "  [-0.7689208   1.1658573   0.53297865 ...  0.9298794   0.8700928\n",
      "   -1.509356  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.42949122 -0.28583062 -2.5229518  ... -0.56270087 -0.36295652\n",
      "   -1.4803095 ]\n",
      "  [ 1.6286304   2.5691297   1.9911985  ...  1.0947622   0.7141935\n",
      "   -1.1568311 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.68066025  1.4930061  -1.2848587  ...  0.2906781   0.3220994\n",
      "   -0.39253983]\n",
      "  [-1.2183435   2.1633708   1.2407862  ... -0.27105498  0.06578945\n",
      "   -2.3813603 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.4787607   1.1924629  -2.2697637  ...  0.46127918 -0.02247456\n",
      "   -0.69631237]\n",
      "  [-0.8734398   1.370609    2.606556   ... -0.27500993  0.8219008\n",
      "   -1.3964548 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.45378095  0.1473298  -0.7897513  ...  0.61700964 -0.9427113\n",
      "   -0.6790749 ]\n",
      "  [ 1.1501708   1.3246047   2.0664945  ...  0.819082    0.04691144\n",
      "   -1.9289765 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.3533269   0.64406466 -0.7644971  ... -0.23615418 -1.0424993\n",
      "   -0.12100548]\n",
      "  [-0.9609833   1.2133118   1.9130409  ...  1.2030911  -1.0720582\n",
      "   -2.5446348 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.78227675  0.43613327 -2.5213614  ...  1.0999717  -0.4437775\n",
      "    0.10887092]\n",
      "  [ 0.30237487  0.8303045   1.2818083  ...  0.4198304  -0.8366931\n",
      "   -1.5972877 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.580773    0.7432859  -1.5984454  ...  0.12141636  0.08238128\n",
      "   -1.2278678 ]\n",
      "  [-0.4741132   2.1149511   0.4002905  ...  0.8234533   0.70796293\n",
      "   -1.4955101 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.580773    0.7432859  -1.5984454  ...  0.12141636  0.08238128\n",
      "   -1.2278678 ]\n",
      "  [-0.4741132   2.1149511   0.4002905  ...  0.8234533   0.70796293\n",
      "   -1.4955101 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1848967   1.3764949  -0.6390058  ... -1.3238994  -1.2768672\n",
      "   -1.3443682 ]\n",
      "  [-0.43128633  0.36665106  0.72330153 ...  1.6014023   0.78309244\n",
      "   -0.7264972 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.2436721   1.8689485  -0.9991745  ... -1.3235364   0.02620688\n",
      "   -0.06634265]\n",
      "  [-0.33345282  2.4711003   2.3345437  ...  0.07836986  1.4862641\n",
      "   -1.0514777 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.435966    0.50175005 -1.3314927  ... -0.169738    0.9960448\n",
      "   -0.5301491 ]\n",
      "  [ 0.0967941   1.441061    1.9898307  ...  1.983294   -1.1473639\n",
      "   -2.434067  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.9891698   1.6300383  -1.3626043  ...  0.41027948 -0.8546536\n",
      "   -0.27044833]\n",
      "  [ 0.988588    0.88947666  1.0287108  ...  0.52569133  0.5893219\n",
      "    0.22018266]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.84680665  0.63657856 -3.2138124  ...  0.30581376  0.09882313\n",
      "   -1.6447389 ]\n",
      "  [-0.85826874 -0.11072385  0.49186164 ...  0.27462035  1.2523857\n",
      "   -1.3477817 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.2638472   2.3453627  -0.24066389 ... -0.8562901   0.9107658\n",
      "   -0.66620636]\n",
      "  [-1.8825142   1.3562517   0.9784565  ...  0.7475637   0.46847117\n",
      "    0.24371171]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.8279967  -0.21086442 -1.9111161  ... -1.4615753  -0.8765526\n",
      "   -1.4722433 ]\n",
      "  [-0.8441665   0.13571084  1.6116471  ...  0.742592   -0.13247679\n",
      "   -2.9193187 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.91607827  0.1127708  -0.59902894 ...  0.7752682  -0.13765913\n",
      "   -0.07472271]\n",
      "  [-1.3518544   1.5644343   1.89662    ...  0.27168924  0.13965145\n",
      "   -1.478875  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.2553678   2.150585   -2.629173   ...  0.03714231 -1.6091516\n",
      "    0.6218395 ]\n",
      "  [ 0.77726257  2.0770211  -0.59991384 ...  0.28493673  0.45226264\n",
      "   -1.2485194 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.606293    0.00942135 -1.7712778  ... -1.2304676   1.2956018\n",
      "    0.6923646 ]\n",
      "  [ 1.2805973   0.31037295  0.8319508  ...  0.4190277  -0.49309093\n",
      "   -0.20533538]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.545474    0.54801184 -1.7867656  ... -2.0925326   0.36360598\n",
      "   -1.9087741 ]\n",
      "  [-1.330722    1.9451793   0.08382982 ... -0.30907065 -0.2598831\n",
      "   -2.1342583 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.14737165  1.1348819  -0.18330479 ... -0.69483703 -0.8576592\n",
      "   -0.6363036 ]\n",
      "  [-1.1377717   2.135611   -0.28993845 ... -0.16258729  0.6702845\n",
      "   -0.18329185]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.8965144  -0.41155028  0.1486044  ... -0.21994314 -0.09182444\n",
      "   -0.8638147 ]\n",
      "  [ 1.6040431   2.4732795   1.469671   ...  0.9146555   0.5292439\n",
      "   -1.1846138 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.26133347  1.9435151  -2.1884346  ... -0.66158503  0.9276845\n",
      "   -1.3124703 ]\n",
      "  [-0.6525184   2.242168   -0.5760515  ...  1.289295   -0.35717916\n",
      "   -0.698867  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.182192   -0.16543913 -2.4315863  ...  1.0464063   0.39431024\n",
      "   -1.5579034 ]\n",
      "  [ 1.1327009   0.72706056  0.4072885  ... -0.85340554 -0.25437188\n",
      "   -0.7224803 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.913034    1.0031397  -0.7692314  ... -1.5896227  -0.09806705\n",
      "   -0.07665408]\n",
      "  [-0.7496208   1.0790858   0.37425488 ... -0.74802667  0.68986195\n",
      "   -1.2732044 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.5020048   2.455905   -0.90865576 ... -1.1350127  -1.1175281\n",
      "   -0.60782   ]\n",
      "  [-0.67008567  2.175074    0.54228944 ...  0.1977574  -0.22037257\n",
      "   -1.2045821 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.285521    0.22277427 -0.4452455  ...  0.23328903 -0.6264912\n",
      "   -1.9311645 ]\n",
      "  [ 0.31643566  0.6412231   1.0663506  ...  0.15991905 -1.1149851\n",
      "   -1.9471096 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.0458581   0.25736797 -2.8742518  ... -1.0252775  -1.1697903\n",
      "   -1.0767971 ]\n",
      "  [-1.0137708   2.8025432   1.7189453  ...  1.4372809   1.1198264\n",
      "   -1.3721457 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.32475257  1.3552309  -1.8237346  ... -0.6386281   0.42512113\n",
      "    0.5482929 ]\n",
      "  [-1.3514948   1.9963785   0.2617895  ...  0.01201451  0.19002959\n",
      "   -0.31958652]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.4610945   0.5097193  -0.690351   ... -0.29128516  0.56228065\n",
      "   -0.4512961 ]\n",
      "  [-0.8240044   1.214724   -0.17624664 ... -0.78099424 -0.08684976\n",
      "   -2.2237773 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.4610945   0.5097193  -0.690351   ... -0.29128516  0.56228065\n",
      "   -0.4512961 ]\n",
      "  [-0.8240044   1.214724   -0.17624664 ... -0.78099424 -0.08684976\n",
      "   -2.2237773 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.9441652   2.6768057  -0.85626215 ...  0.50262094  0.54432815\n",
      "   -0.9868743 ]\n",
      "  [-0.9449303   1.2776655   0.2315951  ...  1.8773417   1.523257\n",
      "   -0.9871348 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.6319405   0.23591626 -1.1946948  ...  0.01870725 -0.13057342\n",
      "   -1.2367864 ]\n",
      "  [ 0.5545827   0.4044416   1.7322717  ...  1.5862153  -0.08422996\n",
      "   -0.433326  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.83121485  0.13245869 -1.2943081  ... -1.1223267  -1.3047545\n",
      "   -0.54627883]\n",
      "  [ 0.4258316   1.0554279   1.9304986  ...  1.9701834   1.4746724\n",
      "    0.26916862]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.13729912  0.33361387 -1.718294   ... -1.7356131  -0.4359595\n",
      "    0.404943  ]\n",
      "  [ 1.1241815   0.56806326 -0.5887747  ...  0.03153366  1.276017\n",
      "   -0.7390957 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.0950744   0.7184557  -2.3034873  ... -0.6927201   0.71206635\n",
      "   -2.4723365 ]\n",
      "  [ 0.19432007  2.2864141   1.472331   ... -0.03055525  0.12667136\n",
      "   -1.6889039 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.60952604  2.2049637  -0.98264986 ...  0.35615453 -0.8245189\n",
      "    0.46890032]\n",
      "  [-0.1409738  -0.2852205   1.2560757  ...  1.6514385   1.8429762\n",
      "   -0.36930215]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.60952604  2.2049637  -0.98264986 ...  0.35615453 -0.8245189\n",
      "    0.46890032]\n",
      "  [-0.1409738  -0.2852205   1.2560757  ...  1.6514385   1.8429762\n",
      "   -0.36930215]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.7360744   2.1606765  -1.1028764  ... -0.9652152  -0.44882697\n",
      "   -1.3036326 ]\n",
      "  [-0.3239699  -0.21551299 -0.00414407 ...  1.7544403   1.1543177\n",
      "    0.20686674]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.633292    1.2999256  -2.0275483  ... -1.3679285  -0.6904084\n",
      "   -1.3927746 ]\n",
      "  [-0.49671873  0.9245976   0.02359962 ... -0.37615645 -0.32172042\n",
      "   -0.5755199 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.35252362  0.30996764 -1.6748288  ...  0.21196964 -0.99793184\n",
      "    0.2855872 ]\n",
      "  [-1.4876997   2.5169048   0.7949552  ...  1.5165856   0.14452071\n",
      "   -0.77230287]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.669545   -0.0920285   0.10428786 ... -1.9563148  -0.24805224\n",
      "   -0.6265118 ]\n",
      "  [ 0.87031376  0.64289904  0.47591817 ...  0.25116304  0.26676023\n",
      "   -0.99255496]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.06008065  0.5171615  -2.0992894  ... -0.6834969  -0.30873808\n",
      "   -0.60728365]\n",
      "  [ 0.6873734   0.6222028   0.16053265 ...  1.2385197   1.678927\n",
      "   -1.3865094 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.64577323  1.6669369  -1.4635764  ... -0.11405483  0.06869537\n",
      "    0.34658563]\n",
      "  [ 0.15850644  1.1156409   1.4921412  ...  0.73354864  0.24547327\n",
      "   -0.45982438]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.6605109   1.7468584  -1.3737895  ...  0.73669755 -0.05509651\n",
      "   -0.25391924]\n",
      "  [ 0.29561624  1.3240933   0.02949512 ...  0.2639521   0.10691206\n",
      "   -0.63012064]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.390449    2.3469853  -1.3067516  ...  0.16432837  0.05294675\n",
      "   -1.2416191 ]\n",
      "  [ 0.10296082  0.754419    1.7097076  ...  1.1887066   0.01854405\n",
      "   -0.2765726 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.1162737   0.73501915 -0.85548586 ... -0.77456915 -0.17875975\n",
      "   -0.6586527 ]\n",
      "  [-0.42357403  2.713798    2.6683226  ...  1.7307916   0.3241297\n",
      "    0.0545218 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1523037   0.556073   -0.6315525  ...  0.22923872  0.65134495\n",
      "   -0.22004545]\n",
      "  [-1.4249034   1.5257006  -0.39269733 ...  0.504884   -0.35645664\n",
      "   -1.9804444 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.3539685   2.5880606   0.09311509 ...  0.7961316  -0.2513686\n",
      "    0.4397931 ]\n",
      "  [ 0.50090325 -0.16009307  0.05225563 ...  0.37195903  0.21254179\n",
      "   -1.8655808 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.8038261  -0.27999544 -1.8985426  ... -0.7606325   1.2419193\n",
      "   -1.4875635 ]\n",
      "  [ 0.45349732  0.76679707  2.2484798  ...  0.5315461  -0.87182266\n",
      "   -0.6370273 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.4075605  -0.04813242 -0.50509566 ...  0.80686915 -0.20435564\n",
      "   -2.0254927 ]\n",
      "  [-1.2131416   1.6379162   0.7910974  ... -0.6382229   0.1536754\n",
      "   -1.5028418 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.093753    1.1878862  -1.447545   ... -0.32609844 -1.3414564\n",
      "   -2.1323051 ]\n",
      "  [ 0.23452765  0.56375366  0.8343339  ...  0.5929419  -0.10691671\n",
      "   -1.7039449 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.9832657   0.2531926  -0.32743394 ...  0.93453956  0.05814987\n",
      "   -0.03522092]\n",
      "  [-0.04611746  1.3068746   1.4843808  ...  0.34850731 -1.3017231\n",
      "   -1.3342232 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.2809408   0.558302   -1.6875497  ... -1.820945    0.89798874\n",
      "   -0.92548585]\n",
      "  [ 0.93432975  1.8338132   0.05543876 ... -0.33690947  0.30538756\n",
      "   -0.6395581 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.23306435  0.89122665 -2.6750863  ... -0.7904607  -1.8228657\n",
      "   -0.4552966 ]\n",
      "  [ 0.5955608   1.0522517   0.65496755 ...  1.3905139   0.12381604\n",
      "   -1.1869348 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.23306435  0.89122665 -2.6750863  ... -0.7904607  -1.8228657\n",
      "   -0.4552966 ]\n",
      "  [ 0.5955608   1.0522517   0.65496755 ...  1.3905139   0.12381604\n",
      "   -1.1869348 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.21087497  1.5698798  -1.8150394  ...  0.08089027  0.05806863\n",
      "    0.05941433]\n",
      "  [ 0.7526816   1.0229712   1.0591826  ...  0.4405899  -0.19063942\n",
      "   -0.7772796 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.8700514   0.37221056 -0.14369178 ... -0.66557616 -1.6065485\n",
      "    0.22371024]\n",
      "  [ 0.84756434  0.8028963   0.4500972  ...  0.7200328   0.29535434\n",
      "   -1.2933755 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.5331196   1.4113605  -2.2152073  ...  0.16735753 -0.81919837\n",
      "   -1.516671  ]\n",
      "  [-0.627048    0.39792114  0.25950533 ... -1.065176   -0.44876224\n",
      "   -0.12729359]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.3590936  -0.44179332 -2.2538505  ...  0.62530327 -0.5655128\n",
      "   -0.3250085 ]\n",
      "  [ 0.4250993   0.71104264  1.3295656  ... -1.0167983  -0.8517812\n",
      "   -0.9931153 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.6500366   1.943799   -0.36625063 ... -0.55533665  0.01163483\n",
      "   -1.3136632 ]\n",
      "  [-0.03014392  1.3751063   1.117948   ...  1.1682276   0.2957583\n",
      "   -2.1052914 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-2.1331871e-01 -3.7896746e-01  9.8948789e-01 ... -1.4485674e+00\n",
      "    2.9604977e-01 -8.1329250e-01]\n",
      "  [-1.0671020e-03  7.4536681e-02 -2.8040113e+00 ...  9.0095311e-02\n",
      "    5.9825212e-01 -7.3436576e-01]\n",
      "  [-2.4221373e-01  1.0288486e+00 -7.4026823e-02 ...  1.9208944e-01\n",
      "    1.2299958e+00 -1.1779600e+00]\n",
      "  ...\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.4509094   0.59260905 -2.330467   ... -0.959877   -0.4009319\n",
      "   -2.434504  ]\n",
      "  [ 0.81011343  1.5219574   1.0526865  ...  0.19219258  0.75517017\n",
      "   -0.8571418 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.18853277  2.1455553  -1.9525582  ...  0.23193797  0.4925124\n",
      "   -1.0748085 ]\n",
      "  [ 0.32808292  2.56598     2.57088    ... -0.71821874  0.42200565\n",
      "   -0.636637  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.2075398   0.7139542  -2.3203247  ... -0.35265422 -0.10824633\n",
      "    0.14828771]\n",
      "  [-1.0757644   2.0585072  -0.31847215 ... -0.19683635 -1.4257903\n",
      "    0.10674214]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.1531319   1.9512835  -3.1751213  ... -0.6497232  -0.12323567\n",
      "   -0.86324805]\n",
      "  [-0.9453795  -0.34134912 -0.20846784 ...  0.3286434  -1.178434\n",
      "    0.30733705]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.1531319   1.9512835  -3.1751213  ... -0.6497232  -0.12323567\n",
      "   -0.86324805]\n",
      "  [-0.9453795  -0.34134912 -0.20846784 ...  0.3286434  -1.178434\n",
      "    0.30733705]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.7179079   1.0785053  -1.823919   ... -0.57063186  0.12604374\n",
      "   -1.8170714 ]\n",
      "  [ 0.47075477  1.4420719   0.20994097 ... -0.23544955  0.1787094\n",
      "   -0.77687395]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.6023115   2.4402423  -0.00432765 ... -1.6528786  -0.15349266\n",
      "   -0.25136566]\n",
      "  [-0.07341564  1.4319122   0.9111254  ... -0.1621579   1.0969514\n",
      "   -0.81280947]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.49007434  0.44252044 -0.12594771 ... -1.4587936   1.2794263\n",
      "   -1.0515089 ]\n",
      "  [-0.25712886  1.4821857   0.12122899 ...  0.7198851   0.496485\n",
      "   -0.4173506 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.92788434  1.2683313  -0.68015486 ... -0.2687195   0.23135036\n",
      "    0.13498712]\n",
      "  [ 1.5388798   0.5364087   2.2984495  ...  1.5884681   0.7932602\n",
      "   -1.5863571 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.9421595   1.4877875  -1.1743609  ... -0.90970063  0.32743323\n",
      "   -1.8547674 ]\n",
      "  [-1.8825142   1.3562517   0.9784565  ...  0.7475637   0.46847117\n",
      "    0.24371171]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.09287047  0.94313455 -1.3774965  ... -0.19213814 -0.8632314\n",
      "   -0.8656148 ]\n",
      "  [-1.3167217   0.26715922  1.6800435  ... -0.8782039  -0.37885785\n",
      "   -0.7478808 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.24064773  0.68652457 -1.2296436  ... -0.83904576 -0.06763449\n",
      "   -0.998672  ]\n",
      "  [ 0.05881735  0.0402211   1.9938713  ...  1.4394516  -0.78434676\n",
      "   -1.3451566 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.4720542   1.8388788  -1.6889069  ... -0.1686567  -0.51365685\n",
      "   -1.200192  ]\n",
      "  [ 0.60641754 -0.05065894  2.0577176  ...  0.06622404 -0.2608469\n",
      "    0.0130862 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1854396   1.1130106  -1.7142383  ... -1.2080842   0.90504915\n",
      "   -0.5729875 ]\n",
      "  [ 0.5641488   1.800845   -0.34578645 ... -0.39220738 -0.38082302\n",
      "   -1.3017132 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.89282906  0.58322245 -0.95378953 ...  0.6487341   0.06630707\n",
      "   -1.7185167 ]\n",
      "  [ 0.3186798   1.1891314   2.273543   ... -0.22833467  0.5440137\n",
      "   -0.27423918]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.06277198  1.6015835  -0.01060236 ... -0.2987244   0.14663237\n",
      "   -2.241435  ]\n",
      "  [ 1.555185    2.007347    1.4774456  ... -0.3052255   1.0281223\n",
      "   -1.2151469 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.39709204 -0.48048186 -2.797773   ... -0.1959166  -1.268215\n",
      "   -1.6934137 ]\n",
      "  [ 0.8343704   0.8543717   1.1670878  ...  0.01743889 -1.1919131\n",
      "    0.5628265 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.6015762   1.0011854  -1.8211086  ... -0.11810407  0.13787764\n",
      "   -0.6689674 ]\n",
      "  [-0.01998247  2.3333983   1.753625   ...  1.5392163   0.18055995\n",
      "   -1.4685031 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.09958875  0.5074274  -2.2130675  ... -0.33310345 -0.12338728\n",
      "   -2.1779437 ]\n",
      "  [-0.8650447   1.3317153   1.2322166  ...  0.9822327   1.2980027\n",
      "    0.05709207]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.5350475   1.6641903  -1.8741888  ... -0.6592587  -0.875487\n",
      "   -0.68011636]\n",
      "  [ 0.3761494   1.2375736   0.95547014 ...  2.2091975   0.5825646\n",
      "   -0.34914076]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.143536    1.5121787  -1.7841101  ... -1.5728828  -0.42182294\n",
      "   -1.1175705 ]\n",
      "  [ 1.1327009   0.72706056  0.4072885  ... -0.85340554 -0.25437188\n",
      "   -0.7224803 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1501979   1.4778557  -1.1700957  ... -0.5835425  -0.25838566\n",
      "    0.01521248]\n",
      "  [ 0.67483187  1.2145015  -0.4143839  ...  1.591675    0.19445828\n",
      "   -0.16524297]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.0543165   1.8943349  -0.8947369  ... -1.7603467   0.3270024\n",
      "    0.11836046]\n",
      "  [ 1.3583133   2.1374702  -0.46032023 ... -0.32233858 -1.3631467\n",
      "   -0.94740266]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.7954392   1.2059779  -1.3429909  ...  0.04757276  0.72250444\n",
      "    0.44236338]\n",
      "  [-0.2490879   1.9190166  -0.0376929  ...  1.7553921   0.84508044\n",
      "   -0.4964767 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.5498348   1.5605791  -2.7551298  ... -2.143257    0.8313903\n",
      "   -1.4691545 ]\n",
      "  [-1.4876997   2.5169048   0.7949552  ...  1.5165856   0.14452071\n",
      "   -0.77230287]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.5498348   1.5605791  -2.7551298  ... -2.143257    0.8313903\n",
      "   -1.4691545 ]\n",
      "  [-1.4876997   2.5169048   0.7949552  ...  1.5165856   0.14452071\n",
      "   -0.77230287]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1794823   1.7932789  -2.6129003  ...  0.82073224  0.2695821\n",
      "   -0.1112054 ]\n",
      "  [-0.10931226  1.2254813   1.324341   ...  0.633578    1.0092921\n",
      "   -0.47654706]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.89280516  1.69964    -0.66042805 ... -1.324016    0.05276012\n",
      "   -0.6021409 ]\n",
      "  [-0.3834982   0.43588156  1.3780184  ...  0.3630334  -0.532826\n",
      "   -1.5273914 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.33853424 -0.27923298 -1.6555469  ... -0.4591189  -0.53571737\n",
      "   -0.45733592]\n",
      "  [-0.9726013   2.3915024   0.1307283  ... -0.15333915 -0.7783196\n",
      "   -0.42740571]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.48570916  2.5625153  -1.7999982  ...  0.0277057  -0.18224391\n",
      "   -0.61347437]\n",
      "  [ 0.78198946  1.3627846   0.66035116 ... -0.87007457 -1.1380732\n",
      "   -1.226162  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.25606644  1.3996208  -1.7053341  ... -2.071867   -0.7836291\n",
      "   -0.5051407 ]\n",
      "  [ 0.08358781  0.7623161  -0.6386049  ...  1.2404737   0.6743215\n",
      "   -2.0300102 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.83121485  0.13245869 -1.2943081  ... -1.1223267  -1.3047545\n",
      "   -0.54627883]\n",
      "  [-0.9911872   0.10243487  0.57833403 ...  1.1472634  -0.3808037\n",
      "   -0.33030796]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.2638472   2.3453627  -0.24066389 ... -0.8562901   0.9107658\n",
      "   -0.66620636]\n",
      "  [-0.23199028  1.9432456   1.1943079  ...  0.12202552  1.0744747\n",
      "   -0.3891309 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.12028104  1.3962197  -1.522521   ... -0.74246734  0.17967182\n",
      "   -0.47187048]\n",
      "  [ 0.33740163  1.5857122   1.511451   ...  1.9936547   0.5590978\n",
      "    0.03335714]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.9215816  -0.02330267 -1.9968734  ...  0.21260348 -0.95630205\n",
      "    0.05496335]\n",
      "  [-1.2034763   1.7952402  -0.10556185 ...  1.471041    0.12742229\n",
      "   -1.08629   ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.9215816  -0.02330267 -1.9968734  ...  0.21260348 -0.95630205\n",
      "    0.05496335]\n",
      "  [-1.2034763   1.7952402  -0.10556185 ...  1.471041    0.12742229\n",
      "   -1.08629   ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.32475257  1.3552309  -1.8237346  ... -0.6386281   0.42512113\n",
      "    0.5482929 ]\n",
      "  [ 0.9107244   0.9661612   1.3400984  ...  0.33298525 -1.0571066\n",
      "   -0.4765013 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.8416291   2.2135892  -1.1342722  ... -0.652004   -0.30715603\n",
      "   -0.01003039]\n",
      "  [-0.2998011   2.0892816   0.9564776  ... -0.31015944  0.37248778\n",
      "   -0.76375365]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.81703     0.30285937 -1.3862302  ... -0.5313797  -0.66087866\n",
      "   -0.07396054]\n",
      "  [-1.6985077  -0.2589767   1.0450362  ... -0.4741791  -0.49611145\n",
      "    0.18087852]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.7490966  -0.3336177  -0.36059296 ... -0.1756185  -0.71567476\n",
      "   -1.5163274 ]\n",
      "  [ 0.42653486  1.4356488  -0.29183102 ...  0.15534702  0.42682403\n",
      "   -2.5172057 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.9200764   0.9984705  -1.5115541  ... -1.1291267  -0.9231526\n",
      "    0.39929998]\n",
      "  [-0.14246857  0.84710157  1.0985892  ...  1.3793063   1.1238524\n",
      "   -1.431736  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.4621294   0.38657314 -1.9333811  ... -0.9957994  -0.6766939\n",
      "   -1.878553  ]\n",
      "  [-0.8312695   2.1203985   0.4235502  ...  1.0506434  -0.83179426\n",
      "   -1.5631855 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.048182    0.38523388 -1.2240767  ...  0.87202656  0.94436806\n",
      "   -1.282588  ]\n",
      "  [ 0.0565136   1.1251848   2.1245446  ... -0.6242482   0.47804803\n",
      "   -1.9435048 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.5849097   1.1647278  -2.5718985  ... -0.4695575  -0.86179507\n",
      "   -0.89218175]\n",
      "  [ 1.4870628   0.849543    0.52493954 ...  0.28019685 -0.8596855\n",
      "   -0.92038345]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.6400597   0.9245281  -1.3081465  ... -1.2255234   1.0655165\n",
      "   -0.17779732]\n",
      "  [ 1.2708008   0.8480514   0.9293885  ... -0.01995206  0.5185126\n",
      "   -1.4117765 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.8829278   1.758365   -0.73621476 ... -0.6972388  -0.19956149\n",
      "   -1.2030927 ]\n",
      "  [ 0.8594235   0.02840555  0.3377306  ...  0.18762302 -0.52263886\n",
      "    0.11749864]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1259804   0.5986456  -1.0831177  ... -1.2646767  -0.6452825\n",
      "   -0.12932277]\n",
      "  [-1.6644144   0.8760283   1.2884351  ...  0.40250313 -0.0670353\n",
      "   -2.633429  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.6903698   2.0532408  -1.3811728  ... -0.78539884  0.5139229\n",
      "   -2.0190313 ]\n",
      "  [ 1.3414669   1.5424733   1.9235163  ... -0.9089499   1.1395205\n",
      "   -2.8293252 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.7620127   1.8713634  -0.960784   ...  0.5074247  -0.78297555\n",
      "   -0.17097086]\n",
      "  [ 0.80840814  1.4661199   2.038922   ...  1.0280943  -0.2853908\n",
      "   -1.1471341 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.2330632   0.9582115  -1.4212182  ... -0.7350773  -0.3520619\n",
      "   -1.4282902 ]\n",
      "  [ 0.15529086  1.3714528   0.58830523 ...  0.9275551  -1.2076445\n",
      "    0.18472767]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.4345505   0.8869528  -1.8251774  ...  0.21187893 -1.100601\n",
      "   -0.09333992]\n",
      "  [ 0.265849    0.48002613  2.7162657  ...  0.86539894  0.586775\n",
      "   -2.012748  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.4767917   1.5070765  -0.56483734 ...  0.32770607  0.94905037\n",
      "    0.20318735]\n",
      "  [ 1.6172069   1.846673    1.9826853  ...  1.0687749   1.2191842\n",
      "   -2.5300078 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.7857562   0.61936796 -2.1467645  ... -0.3249663   0.2737519\n",
      "   -0.6545106 ]\n",
      "  [-1.4877006   1.041086    1.1235842  ...  0.7405058  -0.9298981\n",
      "   -0.16473371]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1520025   0.6391219  -1.1476946  ... -0.47276896 -0.49418652\n",
      "   -0.50531334]\n",
      "  [-0.47542298  1.713321    2.5978594  ...  1.0578512   0.861444\n",
      "   -1.6806331 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.0713682  -0.41095805 -1.3191317  ...  0.7167785   1.267478\n",
      "    0.01596916]\n",
      "  [ 0.44550517  2.209977    2.3726842  ...  0.82853854  0.08072609\n",
      "   -0.7932075 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.06008065  0.5171615  -2.0992894  ... -0.6834969  -0.30873808\n",
      "   -0.60728365]\n",
      "  [-0.7076088   1.1386286   2.209683   ... -0.3156175   0.46927685\n",
      "   -1.1882645 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.3279214   0.6833203  -1.201567   ...  0.26728466 -0.15507424\n",
      "   -0.8291584 ]\n",
      "  [ 0.11574771  1.0134912  -0.2881627  ...  1.616137    0.6328132\n",
      "   -1.5986683 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.351202    0.13549721 -0.34989786 ...  1.0574899   1.0586474\n",
      "   -0.88262105]\n",
      "  [ 0.228495    1.9027953   2.2277808  ...  0.631765    0.76184344\n",
      "   -0.6268529 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.3443792   1.5602484  -2.672244   ...  0.0752798   0.17019463\n",
      "   -0.08817738]\n",
      "  [ 1.0210556   1.7016258   1.7246716  ... -1.1022444   0.3423319\n",
      "   -0.30002993]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.25399917 -0.2988565  -1.9980986  ... -0.44622657 -0.69665074\n",
      "   -1.1644181 ]\n",
      "  [ 0.77659214 -0.09450686  0.21541315 ...  1.1025889   0.20085533\n",
      "   -1.0017862 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.2311087   0.20641434 -2.0160623  ... -1.4509065  -0.0352619\n",
      "   -0.23018241]\n",
      "  [-0.3304543   0.5779553   1.0252846  ...  0.19973263  0.8204148\n",
      "   -0.97767234]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.94604623  0.9178891  -2.2715716  ...  0.04648826 -0.5907319\n",
      "   -1.2898242 ]\n",
      "  [-0.47774914  0.46428156  1.4214007  ...  0.9706917  -0.28373414\n",
      "   -0.7693037 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.429028    2.8391376  -0.88891053 ... -0.6270634   0.65218896\n",
      "   -1.9748825 ]\n",
      "  [-1.1987908   0.35441756  1.0422024  ... -0.06391209  0.4394086\n",
      "   -1.0367161 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.00448817  1.4840063  -1.1840457  ... -1.5301113  -0.67730385\n",
      "   -0.5914035 ]\n",
      "  [-0.808166    2.1508482  -0.42525065 ...  1.496489    0.27136305\n",
      "   -1.4775705 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.0082967   0.5439159  -2.090367   ... -0.327434   -0.6029156\n",
      "   -1.1680968 ]\n",
      "  [-0.30499178  2.0464718   1.3746401  ...  1.353848   -0.9215464\n",
      "   -1.727902  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.4828385   0.9543062  -1.5614675  ...  0.84490097 -0.55079997\n",
      "   -1.0060029 ]\n",
      "  [-0.7816086   0.06419909  1.4211211  ... -1.0971603   1.5539323\n",
      "    0.02931559]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.3308015  -0.23327172 -1.2461008  ...  0.08699611  1.0656264\n",
      "   -0.72515124]\n",
      "  [ 0.7215258   0.73390627  2.279045   ...  0.12584624  1.3881062\n",
      "   -0.9214028 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.81703     0.30285937 -1.3862302  ... -0.5313797  -0.66087866\n",
      "   -0.07396054]\n",
      "  [ 0.2437773   1.3308742   1.4415925  ...  0.29866785 -0.23107256\n",
      "   -0.73047566]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1818677   1.4239049  -2.077435   ... -1.355005   -1.041579\n",
      "   -0.560326  ]\n",
      "  [-0.23485975  0.12797391  1.2391455  ...  0.9586429  -1.1558732\n",
      "   -0.9581263 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.36323553  0.05316901 -1.9341573  ... -0.6886062   0.6209188\n",
      "    0.4674989 ]\n",
      "  [ 0.46365508  0.18034899  0.92815626 ...  1.4516594  -0.75125474\n",
      "   -1.3624333 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.2598445   1.0868793  -1.5289384  ... -1.5813406  -1.0609883\n",
      "   -0.95829254]\n",
      "  [ 0.46929994  0.63545686  1.8848565  ...  0.6433748   1.137779\n",
      "   -0.8365674 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.9441652   2.6768057  -0.85626215 ...  0.50262094  0.54432815\n",
      "   -0.9868743 ]\n",
      "  [ 0.14811772  0.5618708   0.053846   ...  2.0346315   0.13172083\n",
      "   -0.2764004 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.95201075  0.49585724 -2.3220527  ... -1.2706834   0.3353523\n",
      "   -0.3654989 ]\n",
      "  [-1.1412474   2.2213173   1.6423527  ...  0.8104124  -0.1670893\n",
      "   -2.1888332 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.24316531  0.49295014 -1.1191328  ... -0.34459713 -0.67409146\n",
      "   -0.8738417 ]\n",
      "  [ 0.42705625  1.0407299   0.5884192  ... -1.006671   -0.82922715\n",
      "   -0.14940071]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.6310153  -0.59729815 -0.01082301 ...  0.20054206 -1.2928946\n",
      "   -0.89523196]\n",
      "  [-0.61507964  0.6737494   0.78844184 ... -1.1578727   0.9391043\n",
      "   -2.2940454 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.374448    0.920573   -0.3976922  ...  0.12627038 -0.70748794\n",
      "   -1.1253053 ]\n",
      "  [-1.1482135  -0.5645399   1.5452334  ...  0.664771    1.2723666\n",
      "   -0.50462997]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.29224366  1.5490358  -0.5006502  ... -1.0432392   0.3460251\n",
      "   -0.7572136 ]\n",
      "  [ 0.30477506  0.85195243  1.7908553  ...  0.70677996  0.4218945\n",
      "   -0.45640498]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.25695974  0.4371932  -0.25038552 ... -2.1624143  -0.4396373\n",
      "   -1.9146537 ]\n",
      "  [-1.0884678   1.2532078   0.27724403 ...  0.6011219  -1.4677651\n",
      "   -0.98946095]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.71263593  0.82836235 -2.1398518  ...  0.33367363 -1.0372456\n",
      "   -2.256278  ]\n",
      "  [-0.8734398   1.370609    2.606556   ... -0.27500993  0.8219008\n",
      "   -1.3964548 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.16254854  0.7212434  -2.2689445  ...  0.2146934  -0.087093\n",
      "   -1.5076315 ]\n",
      "  [-0.24221373  1.0288486  -0.07402682 ...  0.19208944  1.2299958\n",
      "   -1.17796   ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.5038596   1.3099113  -2.8670385  ... -0.7793129  -0.14867428\n",
      "   -2.1319344 ]\n",
      "  [ 1.1331297   1.2120557   0.3297655  ...  0.73335564 -0.22867809\n",
      "   -0.71156645]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.3899252   1.1703434  -1.5074335  ... -0.2575838  -0.24257515\n",
      "   -0.08379853]\n",
      "  [ 0.38333893  2.7279224   1.270637   ... -0.5370066   0.69326764\n",
      "   -2.3923566 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.5225964   0.1163882  -2.6571598  ... -0.15552914 -1.3476357\n",
      "   -2.277339  ]\n",
      "  [-1.1429087   0.18823993 -0.22682357 ...  0.2445129  -0.30981427\n",
      "   -1.5916998 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.83904064  0.8835893  -1.8261364  ... -0.6903619   0.27062392\n",
      "   -1.0733308 ]\n",
      "  [ 0.61091435  2.0243852   1.8489122  ...  0.02101701 -0.8909343\n",
      "   -0.8574419 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.0729383   2.7051423  -0.9978869  ... -0.57868505 -1.2154434\n",
      "   -0.597315  ]\n",
      "  [ 0.33110508 -0.04045212  1.7475424  ...  0.35476178  1.0458238\n",
      "   -1.3001091 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.40601474  0.31349862 -1.5054513  ... -0.97335505 -1.1788666\n",
      "   -1.0413504 ]\n",
      "  [-0.5047802   0.32044888  1.120076   ... -0.49583906 -0.5916883\n",
      "   -1.3940599 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.4351746   0.19471133 -1.4551315  ... -1.430499   -1.1671866\n",
      "   -1.0087886 ]\n",
      "  [-0.35109395  0.51629305  1.188955   ...  1.5738451  -1.20697\n",
      "   -0.0705471 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1145569   1.5049366  -0.90245616 ... -1.3344526  -1.9562261\n",
      "   -0.03070569]\n",
      "  [-0.20990628  1.3874171   0.13649124 ...  0.14702696 -0.28178692\n",
      "   -2.0101051 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.6646614   1.0251188  -1.7456967  ... -1.1727239  -0.7617538\n",
      "   -1.2717986 ]\n",
      "  [-0.46159238  1.8005918   1.2669811  ...  0.64235497 -0.2925527\n",
      "   -1.1852788 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.44124895  2.6826737  -3.1966672  ... -0.9595181  -0.4194582\n",
      "    0.9791353 ]\n",
      "  [ 0.3861436   0.8082108   1.0324001  ... -0.00948423 -0.8629053\n",
      "   -2.4730475 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.54425085  0.68247324 -1.5428073  ... -0.9441441  -1.4384036\n",
      "   -2.0877762 ]\n",
      "  [-0.04198601  1.2469609   0.35679436 ... -0.5679657  -1.1388832\n",
      "   -1.3739754 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.09804708 -0.3245045  -1.4504691  ... -0.2383766   0.07042813\n",
      "    0.3938607 ]\n",
      "  [-1.3013197   1.1845529  -0.6511191  ...  0.57374257 -0.25062692\n",
      "   -2.4156804 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.6286018   0.20654213 -3.1319115  ... -0.5557269  -0.30522326\n",
      "    0.15702033]\n",
      "  [-0.78368366  2.33367     2.6399927  ...  2.1211367  -0.8383401\n",
      "   -0.60294133]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.2256608   1.6192682  -0.6708203  ... -0.14760512 -1.0059239\n",
      "   -0.7128008 ]\n",
      "  [-0.15182906  0.37161553  1.8657194  ...  1.5916004   0.4877417\n",
      "   -0.7266209 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.9288256   1.5391717  -2.546751   ... -0.5809146   0.8074457\n",
      "   -0.27783075]\n",
      "  [ 1.3275114   1.4189377  -0.4896084  ...  0.67777896  1.0457877\n",
      "   -2.0964599 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.8211132   1.5959482  -0.75979185 ...  0.08450267  0.01858833\n",
      "    0.01962906]\n",
      "  [-1.2819275   1.047023   -0.08538544 ...  0.33869255 -0.21877648\n",
      "   -1.3107469 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.6157277   1.3939383  -2.6321201  ... -0.37275484 -0.9245231\n",
      "   -1.1992044 ]\n",
      "  [-0.82157636  0.10637367  0.3322665  ...  0.28287184  1.0916102\n",
      "   -1.4413517 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.7363379   1.123785   -1.1397525  ... -0.47919506 -1.450501\n",
      "   -0.70598036]\n",
      "  [-1.074219    1.2953411   1.465324   ...  1.3400283   0.6756226\n",
      "   -2.8762136 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.5855396   0.93015915 -2.1517422  ... -1.5194725  -1.4151914\n",
      "   -0.7027238 ]\n",
      "  [ 0.08331236  0.7818339   2.3985262  ...  0.20716615 -0.70893866\n",
      "   -0.18814176]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.0382179   0.3059246  -1.2461164  ...  1.0980058  -1.0150018\n",
      "   -0.13335198]\n",
      "  [ 1.2071002   1.759662    2.6137218  ...  0.2341979   1.8183193\n",
      "   -0.570515  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.5308767   2.3080792  -1.325128   ...  0.94432366 -0.27506\n",
      "    0.71146154]\n",
      "  [ 1.1146892   0.56274825 -0.14431584 ...  1.906313   -1.2094209\n",
      "   -2.120716  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.9363377  -0.54111314 -0.930726   ... -0.69055784  0.4944085\n",
      "   -0.45564523]\n",
      "  [-1.0143081   1.2468681   0.16029745 ...  0.1500411  -0.8783149\n",
      "   -0.5256049 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.57573754 -0.11075675 -3.0241113  ... -1.4431823  -0.82337695\n",
      "    0.17697251]\n",
      "  [-0.81649506  0.96701825  1.6285517  ...  1.5594428  -0.7915914\n",
      "   -1.0711432 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.3068826   0.6107192  -1.5434728  ...  0.30397764 -0.6313722\n",
      "   -0.51170826]\n",
      "  [ 1.4188148   1.9925025   1.951638   ...  1.0846734   0.8779806\n",
      "   -1.5920811 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.046533    2.1347127  -2.133356   ... -0.35447422 -1.496525\n",
      "   -0.9230401 ]\n",
      "  [-0.10208771  1.7341068   2.5773678  ...  0.11257178 -0.1837884\n",
      "   -0.21234351]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.6472616   0.85717547 -1.3267624  ... -0.17273563 -0.11822033\n",
      "   -1.959537  ]\n",
      "  [ 1.3713889   0.5297583   0.31824404 ... -0.2284571  -0.12486075\n",
      "   -0.5688055 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.44099444  0.40402073 -2.2569635  ... -1.163117   -0.7003591\n",
      "   -0.1835342 ]\n",
      "  [ 0.08523171  0.7918008  -0.28593707 ...  0.77980745  0.15856351\n",
      "    0.43951762]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.8680204  -0.06603801 -1.8374312  ... -1.2220001   0.49369174\n",
      "    0.2633263 ]\n",
      "  [-0.9439126   2.1367736  -0.64658105 ...  1.5168264  -0.49561423\n",
      "   -2.0683446 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.45861858  0.7840199  -1.9304965  ... -0.86288077  0.2658561\n",
      "   -1.2081716 ]\n",
      "  [ 1.3414669   1.5424733   1.9235163  ... -0.9089499   1.1395205\n",
      "   -2.8293252 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1726081   0.9031111  -2.6492343  ... -0.7425705   0.65449756\n",
      "   -0.79268867]\n",
      "  [-0.71588266  1.3943387   1.8446655  ...  0.31049442 -1.3714601\n",
      "   -1.5035803 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.2116611   2.3747787  -1.9683754  ... -1.1119314   0.15614283\n",
      "   -0.2104075 ]\n",
      "  [-0.09409375  0.574238    1.6272593  ...  1.5445607   1.278553\n",
      "   -0.9036218 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.9129583  -0.49717295 -0.9544703  ...  0.15830132  0.17137522\n",
      "    0.53878343]\n",
      "  [-1.2784982   0.59058696  0.2702853  ...  0.56251055  0.33560607\n",
      "   -1.1841869 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.4588306   0.21949989 -1.2983824  ...  0.02601925  1.2219718\n",
      "   -1.4549096 ]\n",
      "  [-0.7272513   0.28904593  1.1705177  ...  0.6686614  -0.53078663\n",
      "   -0.868523  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.4588306   0.21949989 -1.2983824  ...  0.02601925  1.2219718\n",
      "   -1.4549096 ]\n",
      "  [-0.7272513   0.28904593  1.1705177  ...  0.6686614  -0.53078663\n",
      "   -0.868523  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.0244882   0.44850045 -0.94794816 ...  0.60990083  0.7030547\n",
      "   -0.5183505 ]\n",
      "  [ 0.69930613  1.0904396   0.25429088 ...  0.6914332   0.889542\n",
      "   -1.1572174 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.5920537   0.27060217  0.00931787 ... -0.45758268 -0.24889284\n",
      "   -0.94266784]\n",
      "  [-0.7310085   1.409466    1.0826389  ...  1.301842    0.69399995\n",
      "   -1.6174334 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.9051051   0.0523057  -1.4268836  ... -1.076449   -0.97262496\n",
      "   -0.3457773 ]\n",
      "  [ 0.53580177  1.3997686   1.0847795  ...  0.4265641  -0.5268529\n",
      "   -0.8872788 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.661403    1.2837285  -1.4925686  ...  0.3671821   0.1185016\n",
      "   -1.2321621 ]\n",
      "  [-0.54185534 -0.09402549  2.6076107  ... -0.02446881  1.38839\n",
      "   -0.32296747]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.9137963   0.30822277 -1.7676792  ... -0.79835856  0.27955055\n",
      "   -0.8902319 ]\n",
      "  [ 1.5388798   0.5364087   2.2984495  ...  1.5884681   0.7932602\n",
      "   -1.5863571 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.6084853   0.41067117 -0.27675796 ...  0.6538081   0.21776187\n",
      "   -1.2010858 ]\n",
      "  [-1.21384     1.6853728   0.9158708  ... -0.11008495  0.44731605\n",
      "   -0.8634527 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.3016844   1.5524503  -1.290663   ... -0.88926923  0.09393543\n",
      "   -1.3991864 ]\n",
      "  [ 1.3287874   0.7718388   1.4657518  ...  1.0876088   0.34210664\n",
      "   -1.3252206 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.6861916   1.3083011  -1.0496516  ... -0.7348109  -0.5865766\n",
      "   -1.4454691 ]\n",
      "  [-1.6009622   1.2943373   1.301879   ...  0.5526805   0.16606997\n",
      "   -0.5364772 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.5565062   0.50908947 -1.9200822  ...  0.48079082 -0.6874196\n",
      "   -1.5265172 ]\n",
      "  [-0.35231206  0.79323244  1.2655587  ...  0.6611202   1.1005036\n",
      "   -1.0279295 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.6958668   0.43801612 -1.7408736  ... -0.34171808 -0.682415\n",
      "   -1.3186736 ]\n",
      "  [ 0.11861269  0.43558425  1.3363793  ... -0.31051528 -0.07005519\n",
      "   -1.1569777 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.4600205   1.79478    -0.75403523 ... -1.0906864   0.17518783\n",
      "   -0.8911976 ]\n",
      "  [-0.40256798  1.9448137   0.67448103 ... -0.16719222 -1.0377561\n",
      "   -0.44399714]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.2000968   0.6892631  -0.46008897 ... -1.788742   -2.0514946\n",
      "   -0.42624545]\n",
      "  [ 0.19650103  2.1809466   1.6620234  ...  0.91189635  0.17640236\n",
      "    0.03281891]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.7338934   2.0552092  -0.91318405 ... -0.02276361 -0.39909598\n",
      "    0.41809022]\n",
      "  [-0.16470481  1.1042997   1.5597835  ...  1.0493451   0.5315422\n",
      "   -0.04135132]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.27174848  0.44699496 -0.4008     ... -0.15336475 -1.044591\n",
      "   -1.740385  ]\n",
      "  [-0.4706651   1.8055396   0.93926436 ... -0.5367152   0.08154887\n",
      "   -1.2067487 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.4010595   1.6798022  -1.6359432  ... -1.4713751  -0.49394947\n",
      "   -0.8214774 ]\n",
      "  [-1.549352   -0.19296992  2.08181    ...  0.13903919  0.8853408\n",
      "   -1.0747123 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.4002373   0.04268348 -1.5229299  ... -0.4456627  -0.08063087\n",
      "   -0.4294855 ]\n",
      "  [ 0.0751778   1.599208    0.65781283 ... -0.4214689   1.5112648\n",
      "   -2.4692411 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.5150284   0.4015888  -0.08812308 ... -1.5128053  -0.5194754\n",
      "   -1.0204457 ]\n",
      "  [-0.1854709   0.9318242   1.5280215  ... -0.59502536  1.843247\n",
      "   -2.0611708 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.2819594  -0.2853204  -1.0784163  ... -1.6747147   0.20427126\n",
      "   -0.96243227]\n",
      "  [-0.06834968  0.75427794  2.6897192  ...  0.15165463  0.41452146\n",
      "   -0.5465071 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.0046134   1.1696036  -1.1098834  ...  0.40536842  0.10012424\n",
      "   -2.490942  ]\n",
      "  [-0.67102504  1.6241602   1.5055728  ...  0.43168575  1.212208\n",
      "   -1.1286685 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.15182596  2.328302   -0.6151045  ... -0.6056583   0.77466613\n",
      "   -1.99396   ]\n",
      "  [ 0.7222539   1.7659252   0.86274207 ... -0.6807347   0.482821\n",
      "   -2.3091574 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.3224079   0.7689022  -1.5659053  ... -0.41797605  0.13539433\n",
      "    0.78625715]\n",
      "  [-1.0757644   2.0585072  -0.31847215 ... -0.19683635 -1.4257903\n",
      "    0.10674214]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.2128597   0.7387712  -2.4821343  ...  0.43439332  0.71976155\n",
      "   -0.9600447 ]\n",
      "  [-0.54185534 -0.09402549  2.6076107  ... -0.02446881  1.38839\n",
      "   -0.32296747]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.4090228   0.9997071  -1.5552009  ... -1.3857107   0.15974694\n",
      "   -2.187756  ]\n",
      "  [-0.8772119   0.77853096  1.8370786  ...  0.08216822 -0.59643525\n",
      "   -1.4324054 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.9129583  -0.49717295 -0.9544703  ...  0.15830132  0.17137522\n",
      "    0.53878343]\n",
      "  [ 0.77577543  1.3642466   1.1400062  ...  0.644825    0.11916455\n",
      "   -1.0889868 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.5406134   0.8979061  -1.3041133  ... -0.6678113   0.52613\n",
      "   -1.4165685 ]\n",
      "  [ 0.48360577  2.6457858   0.50827867 ...  0.76883596  0.14569941\n",
      "   -1.2268223 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.91440326  2.3757823  -0.26616597 ... -0.42534626 -0.3968538\n",
      "   -1.9142405 ]\n",
      "  [ 0.17010838  1.1005222   1.0273744  ...  0.9406871  -0.73784\n",
      "   -2.4904318 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.91440326  2.3757823  -0.26616597 ... -0.42534626 -0.3968538\n",
      "   -1.9142405 ]\n",
      "  [ 0.17010838  1.1005222   1.0273744  ...  0.9406871  -0.73784\n",
      "   -2.4904318 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.8442595   1.5584489   0.0537591  ... -0.873923    0.983239\n",
      "   -0.5693107 ]\n",
      "  [-0.19798157  1.3891399   1.2777225  ... -1.101016   -1.165718\n",
      "   -1.0592105 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.504403    1.9239597  -1.4228979  ... -0.50663584 -0.37895986\n",
      "   -0.7225469 ]\n",
      "  [-1.123922    2.0200725   1.6804706  ... -0.82568675  0.90250075\n",
      "   -0.26691085]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.624901    0.9399748  -0.66521806 ...  0.5988885  -0.15620676\n",
      "    0.04862499]\n",
      "  [ 1.5466081   0.20870626  1.9095376  ...  0.23967882 -0.62652874\n",
      "   -2.1300337 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.8913777   1.0875742  -0.6621666  ...  0.26843116 -1.6475565\n",
      "   -2.1593637 ]\n",
      "  [ 1.0473737   2.3703146  -0.06844366 ...  0.3353719   0.5116541\n",
      "   -2.0213294 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.1196487   1.6639078  -2.206891   ... -0.33854792 -2.031214\n",
      "   -2.3846025 ]\n",
      "  [ 0.15191324  1.3654529   1.0211544  ...  0.32625824 -0.01127049\n",
      "   -1.9688883 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.8593448   0.80480015 -1.7352973  ... -1.4076579  -1.7324345\n",
      "    0.1024664 ]\n",
      "  [ 0.47330877  1.8796898   0.2764933  ...  0.45418185  0.8970908\n",
      "   -0.44276232]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.7730776  -0.47073853 -1.5389476  ...  0.37004057 -1.5187368\n",
      "   -0.09436411]\n",
      "  [-0.3130621   1.0456146   0.45463568 ...  0.66744816  0.28650284\n",
      "   -0.28083372]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.421725    0.6894951  -2.5278459  ... -0.36876255 -2.1301925\n",
      "   -0.53034735]\n",
      "  [-0.48977152  1.6528594   1.1381749  ...  0.5201691   1.5833408\n",
      "   -1.3914798 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.5682846   1.9247111  -1.6506243  ... -1.0042921  -1.8024018\n",
      "    0.7134646 ]\n",
      "  [-0.26331308  2.2548318   2.302652   ...  1.3006647   0.444368\n",
      "   -0.93723476]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.794747   -0.4223082  -2.163202   ... -0.7760645  -0.8008862\n",
      "   -0.00272244]\n",
      "  [ 1.1146892   0.56274825 -0.14431584 ...  1.906313   -1.2094209\n",
      "   -2.120716  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.60952604  2.2049637  -0.98264986 ...  0.35615453 -0.8245189\n",
      "    0.46890032]\n",
      "  [ 0.67309046  2.4457786   1.6432025  ...  0.05407503 -0.4287082\n",
      "   -1.6462818 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.74943703  1.7568867  -2.3476574  ... -0.02955049 -1.545502\n",
      "   -1.1711853 ]\n",
      "  [-1.0207694   0.5325354   0.9685516  ... -0.2840793  -0.9202469\n",
      "   -0.28717214]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.4314153   0.9995614  -1.6186763  ... -0.6448247  -0.9520687\n",
      "   -0.7552044 ]\n",
      "  [ 1.214534    1.0495775   1.8127134  ...  1.2668626  -0.10817496\n",
      "   -1.561218  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.6050646   1.0718569  -1.208988   ...  0.40013686 -0.77309686\n",
      "   -0.13975   ]\n",
      "  [ 0.4802837   1.0126882   1.6370995  ...  1.4630744   0.546858\n",
      "   -0.64164084]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.9511638   0.40679783 -1.6066558  ... -1.2187393  -1.4957452\n",
      "    0.09809917]\n",
      "  [ 0.19432007  2.2864141   1.472331   ... -0.03055525  0.12667136\n",
      "   -1.6889039 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.22114617  0.1996203  -1.8732464  ...  0.6623317   0.5556644\n",
      "   -1.7887585 ]\n",
      "  [ 0.49638298  0.37158388  0.64489716 ...  0.00699019  1.7297652\n",
      "   -1.8007071 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.4448006   0.25128514 -1.4095477  ...  0.01831988 -1.0678375\n",
      "   -1.0085704 ]\n",
      "  [ 1.1422387   1.2235445   0.6711674  ...  1.4632363   0.5741752\n",
      "   -0.8911217 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.253388    0.28027648 -0.08841896 ... -1.137845   -0.40212965\n",
      "   -1.3444366 ]\n",
      "  [-0.67119217  1.6169336   2.7706347  ...  0.16619632 -0.27898204\n",
      "   -0.8195669 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.43401146  0.24584633 -1.9303102  ... -0.92766976  1.1542668\n",
      "   -1.4154358 ]\n",
      "  [ 0.22943744  0.2632652   1.5760069  ... -0.62811047 -1.2167643\n",
      "   -1.2026722 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.43401146  0.24584633 -1.9303102  ... -0.92766976  1.1542668\n",
      "   -1.4154358 ]\n",
      "  [ 0.22943744  0.2632652   1.5760069  ... -0.62811047 -1.2167643\n",
      "   -1.2026722 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-2.1331871e-01 -3.7896746e-01  9.8948789e-01 ... -1.4485674e+00\n",
      "    2.9604977e-01 -8.1329250e-01]\n",
      "  [ 2.1184427e-01  1.0978069e+00 -1.9040401e+00 ...  5.2857649e-01\n",
      "   -1.3231635e-03 -5.0585037e-01]\n",
      "  [ 1.3287874e+00  7.7183878e-01  1.4657518e+00 ...  1.0876088e+00\n",
      "    3.4210664e-01 -1.3252206e+00]\n",
      "  ...\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]]]\n",
      "Before attention blocks: [[[-2.1331871e-01 -3.7896746e-01  9.8948789e-01 ... -1.4485674e+00\n",
      "    2.9604977e-01 -8.1329250e-01]\n",
      "  [-6.8822175e-01  1.9923236e+00 -2.0957375e+00 ... -1.3839045e+00\n",
      "   -9.8102772e-01  4.1335082e-01]\n",
      "  [ 5.1016080e-01  6.5208977e-01 -7.2797334e-01 ...  2.7499795e-01\n",
      "   -1.5128553e-03 -1.8959339e+00]\n",
      "  ...\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.1698063  -0.13460946 -2.68575    ... -0.9391497  -0.4318571\n",
      "   -1.5957048 ]\n",
      "  [ 0.3223106   1.6995387   1.4941387  ...  0.06994471 -0.31342757\n",
      "   -0.6765629 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.26424664  1.6911789  -1.896446   ... -0.11827129 -1.3192858\n",
      "   -0.06710744]\n",
      "  [ 1.0010328   1.7226822   1.4168321  ...  1.5697861  -0.63033473\n",
      "   -2.1780195 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1564139   0.92689127 -1.8151615  ... -0.88140833 -0.79407614\n",
      "    0.11636525]\n",
      "  [ 0.94718266  0.7929945   1.1224743  ...  0.1293216   0.07903968\n",
      "   -0.5299954 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.21889567  0.8731592  -1.9690747  ...  0.54735875  0.2571184\n",
      "   -1.8290092 ]\n",
      "  [ 0.15191324  1.3654529   1.0211544  ...  0.32625824 -0.01127049\n",
      "   -1.9688883 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.6800153   0.9533483  -2.2009525  ... -1.6826866   0.11436361\n",
      "   -0.88793314]\n",
      "  [-0.8377589   2.341586    1.8399501  ...  0.3172524  -1.2628148\n",
      "   -0.73075795]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.30649805 -0.17822325 -1.1547174  ... -0.84645677 -0.4332368\n",
      "   -0.18155855]\n",
      "  [ 0.8968376   1.5336461   0.29008973 ... -0.5424099   0.36682\n",
      "    0.21819198]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.7681534   2.2158484  -0.7352573  ... -0.6174075  -1.8383131\n",
      "   -0.34548667]\n",
      "  [-0.02298271  1.3337494   1.0681438  ... -0.2486965   1.3334246\n",
      "   -1.8818139 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.7950878   1.0364792  -2.3349848  ... -0.3485062  -1.2269368\n",
      "    0.5453689 ]\n",
      "  [-0.36860523  1.0545053   0.16393465 ... -0.37004328  1.7001027\n",
      "   -1.181744  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.7950878   1.0364792  -2.3349848  ... -0.3485062  -1.2269368\n",
      "    0.5453689 ]\n",
      "  [-0.36860523  1.0545053   0.16393465 ... -0.37004328  1.7001027\n",
      "   -1.181744  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.7519708  -0.01936388 -2.242941   ... -0.6517881   0.5161119\n",
      "   -1.0560803 ]\n",
      "  [ 0.86902714  2.3926249   1.8052742  ...  0.9303906  -0.85644025\n",
      "   -1.6534059 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.3632041   1.159139   -1.2439244  ...  0.6469029  -1.300461\n",
      "   -0.1379714 ]\n",
      "  [ 0.04937894  0.6436221   1.5755955  ... -0.92274374  0.88452077\n",
      "    0.3817469 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.0888317   1.5852485  -2.3344269  ... -1.1513186   0.12724131\n",
      "   -1.2790711 ]\n",
      "  [-0.94815457  1.5365747   1.4826405  ...  1.1670835   1.1930108\n",
      "   -0.60097533]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.128376    1.2634023  -1.297485   ... -2.035676   -1.7412162\n",
      "   -0.6739392 ]\n",
      "  [-1.0696658   2.6909218   1.9929621  ...  1.3829513   0.43249863\n",
      "   -1.0436376 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-2.1331871e-01 -3.7896746e-01  9.8948789e-01 ... -1.4485674e+00\n",
      "    2.9604977e-01 -8.1329250e-01]\n",
      "  [-2.0052443e+00  1.1098938e+00 -1.3179862e+00 ...  1.9522905e-03\n",
      "   -9.7769636e-01 -1.3305724e+00]\n",
      "  [ 1.1694315e-01  2.2098978e+00  1.6316193e+00 ... -2.3713613e-01\n",
      "    4.7998053e-01 -6.6602200e-01]\n",
      "  ...\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.878549    1.4108372  -1.092567   ...  0.23242351  0.6175125\n",
      "   -0.21570402]\n",
      "  [-0.75821507  1.6826178  -0.53010726 ... -0.23371315  0.12664276\n",
      "   -1.9823035 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.2820624   1.3866854  -0.4698236  ... -0.10824427 -0.60827035\n",
      "   -0.03054303]\n",
      "  [-1.1453912   0.37925774  0.92416215 ...  0.6386744  -0.17032777\n",
      "   -0.977721  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.9518857  -0.0992167  -0.75652725 ... -0.27768958 -1.1371521\n",
      "    0.203996  ]\n",
      "  [-0.79387116  0.67634267  1.1053029  ...  0.5349197  -0.42306328\n",
      "   -1.4070253 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.9037907   2.547631   -0.9385283  ...  0.0920752  -0.25863308\n",
      "   -1.1224762 ]\n",
      "  [-0.98968196  1.1242081   1.0636533  ... -0.19446677 -0.34765428\n",
      "    0.01402867]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.4486476   1.3156421  -2.20161    ... -0.5094268  -0.7373589\n",
      "    0.21488559]\n",
      "  [-0.61507964  0.6737494   0.78844184 ... -1.1578727   0.9391043\n",
      "   -2.2940454 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.41196376  0.36980957 -0.816436   ... -2.0780861  -1.8154907\n",
      "    0.24362719]\n",
      "  [-0.872795   -0.1358012   1.4600945  ...  1.5199578   0.09959999\n",
      "   -1.5540906 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.4887323   1.2807539  -2.29594    ... -1.2653062  -0.86904603\n",
      "   -1.4960842 ]\n",
      "  [-0.845649    1.3444498   0.14071536 ...  0.52799433  0.2005986\n",
      "   -2.3819003 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.7262899   1.149599   -1.89146    ... -0.878176   -1.3564286\n",
      "    0.02963257]\n",
      "  [-0.7293335  -0.1795727   0.9349     ... -0.16197073  0.48952985\n",
      "    0.1651907 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.30166698  1.0340217  -2.0976253  ...  0.41461226  0.12287587\n",
      "   -1.2471526 ]\n",
      "  [-0.21699858 -0.00286365  0.69106627 ...  1.5768883   0.5583608\n",
      "   -2.2355533 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.9189944   1.3213416  -2.377948   ... -1.3776078   0.7321549\n",
      "    0.11243862]\n",
      "  [-0.43868908 -0.21400487  1.6092479  ...  0.8651569   0.98576784\n",
      "   -0.32920873]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.09806508  1.001151   -1.4545085  ... -1.9128953  -0.37033308\n",
      "    0.37327027]\n",
      "  [ 0.83300555  1.0498644   0.6307304  ... -0.13256365  1.5815649\n",
      "   -1.1348553 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1369063   0.6411948  -0.32791865 ... -2.1922195   0.754782\n",
      "   -0.37203458]\n",
      "  [-1.0280024   1.8256898  -0.709002   ...  0.23866922  1.2335769\n",
      "    0.11575699]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.73986596  2.0187907  -0.626717   ... -0.66185766  0.10128123\n",
      "    0.00704408]\n",
      "  [-0.7689208   1.1658573   0.53297865 ...  0.9298794   0.8700928\n",
      "   -1.509356  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.3919868   1.6748543  -1.3082263  ... -0.29230493 -0.86805105\n",
      "   -0.80000746]\n",
      "  [ 0.23739219  1.4646381  -0.0677557  ...  0.9071131   1.079121\n",
      "   -1.8739135 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.30007654  2.2467875  -2.0763001  ...  0.6504024  -0.71135193\n",
      "   -0.65935063]\n",
      "  [ 0.21155128  0.5440264   1.3272622  ...  0.04555821 -0.4312554\n",
      "   -1.6236515 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.0602746   0.20528346 -2.5298634  ... -0.44640863 -0.44762388\n",
      "   -1.9492724 ]\n",
      "  [-0.44483823  2.7452745   0.4173739  ... -0.32194197 -0.61652577\n",
      "   -2.0844646 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.8837373   1.1385083  -0.66219753 ... -0.9342916  -0.887246\n",
      "   -2.0433722 ]\n",
      "  [-1.0360636   2.0800292   0.865493   ...  1.4128911   0.18928781\n",
      "   -1.584339  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.3526767   2.2110667  -1.0802615  ... -0.3547151  -0.3479737\n",
      "   -1.3930826 ]\n",
      "  [ 0.13689525  2.538507    1.8756542  ... -0.47977865  0.5365115\n",
      "   -2.821024  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.697933    0.8112533  -1.3479085  ...  0.2650012  -0.10626698\n",
      "    0.5011742 ]\n",
      "  [-0.5041561   1.0126904   0.75003004 ...  1.1465389  -0.5251026\n",
      "   -0.47861123]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.0057569   0.5218593  -1.4927702  ... -1.3023528  -1.658634\n",
      "    0.91668284]\n",
      "  [ 0.8654524  -0.3940742   0.60864466 ...  0.48567447 -0.5948997\n",
      "   -1.8369405 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.5133759   0.6228951  -1.8014019  ...  0.7064259  -1.1185405\n",
      "   -0.73822635]\n",
      "  [ 0.8395406   1.7472744   0.2912144  ...  1.3835754   1.6135507\n",
      "   -0.48311853]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1124915   0.7541325  -2.0103397  ... -0.38428476 -0.30769292\n",
      "   -1.9817203 ]\n",
      "  [-0.2490879   1.9190166  -0.0376929  ...  1.7553921   0.84508044\n",
      "   -0.4964767 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.2931947   0.47266823 -1.6666793  ... -0.70212495  0.51408726\n",
      "   -0.6891373 ]\n",
      "  [ 0.8968376   1.5336461   0.29008973 ... -0.5424099   0.36682\n",
      "    0.21819198]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.4184808   2.1783457  -1.6427736  ...  0.6811702  -1.0823364\n",
      "   -0.56020147]\n",
      "  [-0.10952392  2.7291088   1.3656394  ...  0.76737374  1.3430033\n",
      "   -0.47613293]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.6836393   1.3612347  -0.17077899 ... -0.9688221  -1.5572526\n",
      "    0.03397131]\n",
      "  [ 0.16555347  0.31870818  0.3770697  ...  0.09166849  0.23972172\n",
      "    0.34473515]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.9016163   0.96748054 -1.4915031  ... -0.72474074 -1.2306575\n",
      "   -1.1382805 ]\n",
      "  [ 0.70178497  2.2448864   2.6795678  ...  0.5212167  -1.2544725\n",
      "   -2.142998  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.2306267   1.1879376  -2.1401825  ...  0.83403015 -1.3057823\n",
      "   -0.37848723]\n",
      "  [-0.77397823  1.8346689   1.0749726  ...  1.237868    0.84550506\n",
      "   -1.1597879 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.545642    1.9482341  -0.632038   ... -0.47648025 -1.744034\n",
      "   -1.1497879 ]\n",
      "  [-0.216094    0.1368128   0.984681   ...  0.9277818  -0.3446622\n",
      "   -1.8077999 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.04317158  1.5806998  -2.5292706  ...  0.02328888  0.5698227\n",
      "    0.5926137 ]\n",
      "  [-0.47087044  2.9473805   1.3821641  ...  0.78861177 -0.80013734\n",
      "   -0.7002563 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.51183534  2.6762533  -2.0597703  ... -1.7429539  -1.3779392\n",
      "   -1.2395147 ]\n",
      "  [-0.18749346  2.8518963   1.6104465  ... -0.19335711  0.54586184\n",
      "   -0.9489596 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.7713766   0.43276513 -1.2863172  ... -1.9203417  -0.12686515\n",
      "    0.7927226 ]\n",
      "  [ 0.72943294  0.68981344  1.5655515  ...  1.23019     1.1444641\n",
      "   -1.0737073 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1656928   1.4174801  -2.947278   ...  1.2385243  -0.32029942\n",
      "   -1.000786  ]\n",
      "  [-1.0369462   1.2652658   0.4122727  ...  0.13958895 -0.40302587\n",
      "   -1.0012863 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.32384056  1.5455079  -2.1035886  ... -0.0378907  -0.37101474\n",
      "   -1.0536028 ]\n",
      "  [ 0.4412404   0.16570067 -0.06610024 ...  0.10398528 -0.74066603\n",
      "   -0.6983148 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.6824603   2.5091746  -0.9387149  ... -0.43790168 -0.81243306\n",
      "   -0.86671937]\n",
      "  [-0.7538644   0.6253838   1.3891761  ... -0.44450706 -0.33643478\n",
      "   -0.11674881]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.9744976   2.1039913  -2.4801788  ... -1.1025273  -1.2984326\n",
      "   -2.07728   ]\n",
      "  [-0.953024    1.0931638   1.4371142  ...  2.0020585  -0.23971649\n",
      "   -0.62384665]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.1749399   1.5120075  -3.0705104  ...  0.10940453 -1.0828037\n",
      "   -1.0312072 ]\n",
      "  [-0.07773171  1.0454979   0.84578776 ...  0.03245491  1.8175923\n",
      "   -2.1653118 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.5171827   0.8335135  -1.2822436  ... -1.0541984  -0.5942951\n",
      "   -0.5726529 ]\n",
      "  [-0.03962185  1.6909558   1.0101184  ...  1.0714489   1.2666076\n",
      "   -0.24941641]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.4384728   0.34916413 -1.9297336  ... -0.7297145   0.95176727\n",
      "    0.11085421]\n",
      "  [-0.7180816   1.957717    1.1562198  ... -1.0682256   0.08641954\n",
      "   -0.76203656]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.0960241   0.7286342  -1.4081197  ... -0.91722107 -1.7674115\n",
      "    0.9480978 ]\n",
      "  [-0.13103455  1.5612745   0.73688745 ...  1.0679151  -0.21608908\n",
      "   -2.039133  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.17921251  1.4094412  -1.5424901  ...  1.1624049  -1.6035349\n",
      "   -0.3775877 ]\n",
      "  [-0.1055228   0.528854    0.23301375 ... -0.3184092   0.7754443\n",
      "   -0.8636831 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.182192   -0.16543913 -2.4315863  ...  1.0464063   0.39431024\n",
      "   -1.5579034 ]\n",
      "  [-0.02411697  0.28193796  2.208663   ...  0.7214571   1.290494\n",
      "   -1.6624296 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1442113   2.1470187  -1.0770223  ... -0.11936015 -1.8548927\n",
      "   -1.9025513 ]\n",
      "  [-0.43280968  1.2848766   1.3312831  ...  1.5815628  -0.7249627\n",
      "   -0.5232427 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1442113   2.1470187  -1.0770223  ... -0.11936015 -1.8548927\n",
      "   -1.9025513 ]\n",
      "  [-0.43280968  1.2848766   1.3312831  ...  1.5815628  -0.7249627\n",
      "   -0.5232427 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1061075   0.9385255  -0.40767813 ... -0.89710796 -0.30215636\n",
      "   -0.05249643]\n",
      "  [ 0.20745093  1.0801942   1.0713795  ... -0.77636343 -1.520687\n",
      "   -1.7594242 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.2856936   0.6512911  -0.6139794  ... -0.5604043   1.4228723\n",
      "   -0.11973733]\n",
      "  [-0.5041561   1.0126904   0.75003004 ...  1.1465389  -0.5251026\n",
      "   -0.47861123]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.80622995  0.29892927 -2.6459377  ...  1.1806242  -0.47545686\n",
      "   -1.3648256 ]\n",
      "  [-0.09343134  2.5753403  -0.2871933  ... -0.5640728  -0.32693624\n",
      "   -2.1459508 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1726081   0.9031111  -2.6492343  ... -0.7425705   0.65449756\n",
      "   -0.79268867]\n",
      "  [ 0.80885446  1.3523399   1.1724664  ...  1.1159737   0.66864324\n",
      "   -2.5492437 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.0649455   0.21351528 -1.1054938  ... -1.7322075  -0.35505712\n",
      "   -0.5367521 ]\n",
      "  [ 1.5346445   1.1494303   1.6167127  ...  0.5437246  -1.0406836\n",
      "   -1.216463  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.9020754   0.5026786  -1.2470014  ...  0.4454548  -0.8637587\n",
      "   -0.7106949 ]\n",
      "  [ 0.7426317   1.991127    1.5810297  ...  1.6150095   0.844406\n",
      "   -1.1335152 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.5661422   0.6435633  -1.1314033  ... -1.7820657  -0.86535436\n",
      "    0.42579317]\n",
      "  [ 1.6837438   0.17634249  0.6583725  ... -0.7581137   0.7614918\n",
      "   -1.5404699 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.02492934  1.8785334  -1.8355806  ... -1.2416487  -1.8961995\n",
      "   -0.94770277]\n",
      "  [ 0.8350327   1.7987368   1.9091165  ...  1.5689104   0.5933826\n",
      "   -0.15420842]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.4588306   0.21949989 -1.2983824  ...  0.02601925  1.2219718\n",
      "   -1.4549096 ]\n",
      "  [-0.6546278   1.3441323   2.0483358  ... -0.33082736  0.40710163\n",
      "   -1.6224166 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.6905382   0.38094097 -1.8836529  ... -1.3636814   1.0556712\n",
      "    0.4165883 ]\n",
      "  [ 0.7426317   1.991127    1.5810297  ...  1.6150095   0.844406\n",
      "   -1.1335152 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.573066    2.816772   -0.3647802  ...  0.19112727 -1.0193145\n",
      "   -1.7332714 ]\n",
      "  [-1.0424082   1.5141885   1.2139107  ... -0.82657176  0.40266442\n",
      "   -0.43439472]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.5205354   1.1721747  -1.518796   ... -0.25239545 -0.6797434\n",
      "   -1.4737141 ]\n",
      "  [ 1.4214506   1.1293024  -0.1054641  ...  0.89611244 -0.6239411\n",
      "   -1.2370939 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.3831188   0.6707677  -1.7048776  ... -0.8912716   0.15811163\n",
      "   -1.3548408 ]\n",
      "  [-1.1611183   2.138949    1.0340765  ...  0.7720563  -1.2860805\n",
      "   -1.2693071 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.17157328  0.46603137 -3.2394733  ... -0.48163065 -1.3357258\n",
      "   -1.3314108 ]\n",
      "  [ 0.672717    1.2550932   2.0850124  ... -0.23132038  0.49461585\n",
      "   -2.4523025 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.384498    1.181803   -0.73792326 ... -1.3860606  -0.06843689\n",
      "   -0.20126742]\n",
      "  [-0.2945022   1.8735511   2.6071057  ...  0.446319   -1.0473665\n",
      "   -1.5052782 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.02522963 -0.57513523 -2.1471171  ... -0.8763199  -1.1828873\n",
      "   -0.8936583 ]\n",
      "  [ 0.91882646  0.5355137   0.80343056 ... -1.0429206   0.02285035\n",
      "   -0.2446565 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.13660765  0.6817484  -1.9706434  ...  0.6142384  -0.39955276\n",
      "    0.5480714 ]\n",
      "  [-1.4009202   1.2532356   0.40187925 ... -0.4497491  -0.14310162\n",
      "    0.23136926]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.9890853   2.2027574  -0.8418343  ... -1.4692633  -0.03899658\n",
      "   -2.0706463 ]\n",
      "  [-0.31905004  0.9779781   0.07188928 ...  0.649313    0.5702592\n",
      "   -0.3842399 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.4147851   1.2703955  -0.26212    ... -0.61453116 -1.733088\n",
      "   -0.91428655]\n",
      "  [-0.79172826  1.6030941   2.298717   ...  0.9791592   0.06565941\n",
      "   -1.0495521 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.6629714   0.7987036  -1.2909119  ... -0.8545277  -0.7563246\n",
      "   -0.80875033]\n",
      "  [-0.872795   -0.1358012   1.4600945  ...  1.5199578   0.09959999\n",
      "   -1.5540906 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.8031895  -0.26153874 -1.115113   ...  0.58529794 -0.47589833\n",
      "   -1.1688193 ]\n",
      "  [ 0.59139407  1.7898057   1.7293036  ...  1.2388453   1.413188\n",
      "   -1.8329548 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.771888    0.9899033  -1.0830662  ... -0.20111132 -0.33002508\n",
      "   -0.07455307]\n",
      "  [ 0.14451927  2.1011121   0.40834218 ...  0.03229707 -0.02201289\n",
      "   -2.8076856 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.6307079   2.4997687  -2.3310814  ... -0.7371911  -0.1116907\n",
      "   -1.716907  ]\n",
      "  [-0.44376534  1.3491266   1.3704044  ...  1.3284112   0.34356818\n",
      "   -0.6321545 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.20096153  0.5640759  -1.009656   ...  0.29553017  0.56896585\n",
      "   -0.6884361 ]\n",
      "  [-1.0184317   0.17536497  0.46176147 ...  0.32641807  0.69013005\n",
      "   -1.0262825 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.2809408   0.558302   -1.6875497  ... -1.820945    0.89798874\n",
      "   -0.92548585]\n",
      "  [-0.39880133  1.581695    1.8597548  ...  1.1694467  -0.54315656\n",
      "   -0.65167165]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.3272681   1.1768622  -0.9140274  ...  0.8264104  -2.1233528\n",
      "   -0.76146424]\n",
      "  [ 1.1585991   2.5336006  -0.5316218  ...  0.78789026  0.20711146\n",
      "   -1.6381667 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.21103758 -0.3802811  -1.6640239  ... -1.5161119   0.1802336\n",
      "   -1.5443203 ]\n",
      "  [-0.38483205 -0.25965846  0.94825184 ...  0.87077624  0.23095497\n",
      "   -1.1343154 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.5313346   1.4939077  -1.7879744  ... -1.039386    0.24964291\n",
      "   -0.30016044]\n",
      "  [ 0.44292614  2.5456424   2.6754024  ...  1.4622571   1.0037923\n",
      "   -0.8596083 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.41107243  1.4167356  -0.6516912  ... -1.8436098   0.56402224\n",
      "   -2.4440536 ]\n",
      "  [ 0.58558834  2.1512656   1.136901   ...  0.0124473  -0.9477722\n",
      "   -1.7026823 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.41107243  1.4167356  -0.6516912  ... -1.8436098   0.56402224\n",
      "   -2.4440536 ]\n",
      "  [ 0.58558834  2.1512656   1.136901   ...  0.0124473  -0.9477722\n",
      "   -1.7026823 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-2.1331871e-01 -3.7896746e-01  9.8948789e-01 ... -1.4485674e+00\n",
      "    2.9604977e-01 -8.1329250e-01]\n",
      "  [ 2.1184427e-01  1.0978069e+00 -1.9040401e+00 ...  5.2857649e-01\n",
      "   -1.3231635e-03 -5.0585037e-01]\n",
      "  [-2.3240188e-01  1.6101134e+00  8.3345652e-02 ... -6.6386801e-01\n",
      "   -7.2859627e-01 -2.7492762e+00]\n",
      "  ...\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.34480613  2.025528   -1.4383065  ... -0.9222126  -1.5232706\n",
      "   -1.317411  ]\n",
      "  [ 0.16555347  0.31870818  0.3770697  ...  0.09166849  0.23972172\n",
      "    0.34473515]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.5901202   0.26694041 -1.3151528  ... -1.208102   -0.49738485\n",
      "   -1.8462479 ]\n",
      "  [-0.3005284   2.2001822   0.7355876  ...  0.7739935   0.4842416\n",
      "   -0.10541451]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1550347   0.39655286 -0.08009636 ... -0.9508967  -1.2908843\n",
      "   -0.09215689]\n",
      "  [-1.2758346   0.17930865 -0.23612976 ...  0.32204998 -0.88775903\n",
      "   -1.4336019 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.81703     0.30285937 -1.3862302  ... -0.5313797  -0.66087866\n",
      "   -0.07396054]\n",
      "  [-0.03962185  1.6909558   1.0101184  ...  1.0714489   1.2666076\n",
      "   -0.24941641]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.5296357   0.6739199  -2.6429982  ... -0.31611055 -0.5168528\n",
      "   -1.945298  ]\n",
      "  [ 0.6854216   1.2237451   1.828321   ... -0.5122847  -0.58330494\n",
      "   -1.8150002 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.2481582   0.6149502  -2.3600397  ... -1.1962475  -1.1658633\n",
      "    0.45112062]\n",
      "  [-0.33527976  2.6358056   1.8199457  ... -0.6141836  -0.5595647\n",
      "   -0.41715252]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.4509094   0.59260905 -2.330467   ... -0.959877   -0.4009319\n",
      "   -2.434504  ]\n",
      "  [ 0.12806858  1.4044423   1.1607797  ...  1.1403654  -0.3253094\n",
      "   -1.3371856 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.201808    1.2674782  -1.740149   ... -2.0220325   1.0444505\n",
      "   -0.6523555 ]\n",
      "  [-0.2219092   0.68181056  1.943655   ...  1.1638987   1.2268432\n",
      "   -0.60531676]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.5669284   2.0158203  -2.1749084  ...  0.56246233 -1.3188229\n",
      "   -1.5480878 ]\n",
      "  [ 0.89781976  1.2299796   1.7804098  ...  1.7835364   0.60495114\n",
      "   -2.4246368 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.03257465  1.1042421  -0.79479766 ...  0.8488765   0.02945283\n",
      "   -2.0393655 ]\n",
      "  [-0.27311048  0.44944972  0.7014791  ...  1.9222281   0.940578\n",
      "   -0.58533454]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.3152266  -0.385396   -1.6269556  ... -0.06388369 -0.34454334\n",
      "   -0.74904406]\n",
      "  [-1.2083708   1.8646679   1.1410826  ...  0.78565836  0.37563217\n",
      "    0.5061016 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.98675287  0.815186   -2.3351667  ... -0.7525246  -1.0352969\n",
      "   -1.441807  ]\n",
      "  [-0.4231983  -0.11431587 -0.07971346 ...  0.7302487  -0.37247086\n",
      "   -1.2702371 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.8300816   1.9487889  -0.8961555  ...  0.04726073 -0.09930542\n",
      "   -0.25637937]\n",
      "  [ 1.6693127   0.7836261   0.30062908 ...  0.7130648  -0.4652449\n",
      "   -0.9258568 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1977357   1.1871156  -0.85634476 ...  0.4908261  -1.4115338\n",
      "   -0.9980287 ]\n",
      "  [ 0.86626434  1.9214053   0.5180546  ...  1.7216375  -0.34042346\n",
      "   -2.3461013 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.7939901   1.7911599   0.02886832 ...  0.17406985  0.54324734\n",
      "    0.04088134]\n",
      "  [-0.32945007  1.2126168   1.046269   ... -0.64668065 -0.48548996\n",
      "   -1.3435639 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.16749936  1.4589272  -2.8181195  ... -1.0084081  -0.2659229\n",
      "   -0.10653239]\n",
      "  [ 0.6358057   1.0126138   1.5483968  ...  2.183529    0.4586444\n",
      "   -1.5239449 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.9183834   1.1084502  -2.0838633  ...  0.18202499 -0.2254856\n",
      "   -0.9061354 ]\n",
      "  [ 0.62562466  0.5171454   1.4748194  ...  0.34205428  0.7803112\n",
      "   -1.1198726 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.8026357   0.99094987 -2.0340052  ... -0.34335607  1.2079117\n",
      "   -1.9075553 ]\n",
      "  [ 0.03798622  1.5577182   1.9842093  ... -0.4647922  -0.09993584\n",
      "   -2.731533  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.0757856   0.2535202  -1.6510453  ... -0.29598552 -0.7458261\n",
      "   -0.59244967]\n",
      "  [-0.28461352  0.04906988  1.9740514  ... -0.6136269   1.2971896\n",
      "   -0.84105456]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.6220476   0.46197808 -2.6039407  ... -1.1063521  -0.35358316\n",
      "   -1.7976193 ]\n",
      "  [-1.2838985   0.5348448   0.8964659  ...  0.54307795 -1.201669\n",
      "   -1.0015087 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.5944262   2.5207596  -2.7085214  ...  0.37479773 -0.6417303\n",
      "    0.36342645]\n",
      "  [-1.7207029   0.54936606 -0.21571481 ... -0.5082107  -0.30097234\n",
      "   -1.8522198 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.9215816  -0.02330267 -1.9968734  ...  0.21260348 -0.95630205\n",
      "    0.05496335]\n",
      "  [-0.54138255  1.3190241  -0.58194876 ...  0.7677423  -0.70127887\n",
      "   -1.6677794 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.0791504   1.738002   -0.77262187 ... -0.92907333  0.382636\n",
      "   -2.1160495 ]\n",
      "  [-0.09748384  1.9579624   0.8631101  ...  0.1997479   1.369116\n",
      "   -0.69453037]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.07453132  1.171399   -0.6948027  ... -1.662906   -1.6620574\n",
      "   -2.1794748 ]\n",
      "  [ 0.25464702  1.38327     0.69504774 ...  1.4877391   0.01228967\n",
      "    0.00308132]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.5089901   1.1756151  -2.2312768  ... -0.96652985  0.05352339\n",
      "   -0.7273849 ]\n",
      "  [ 0.2880254   0.00886321  1.7837634  ...  0.89370006 -0.2916383\n",
      "   -1.5015415 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.64236903 -0.11687434 -0.79144406 ... -0.04095986 -0.8671366\n",
      "   -1.1162702 ]\n",
      "  [ 0.54614913  0.77196014  0.12708151 ... -0.7748199  -0.14897604\n",
      "   -0.35441005]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.02212745  1.2323723  -2.3005283  ... -1.1037451  -0.6023718\n",
      "    0.5797584 ]\n",
      "  [-0.27311048  0.44944972  0.7014791  ...  1.9222281   0.940578\n",
      "   -0.58533454]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.6484241  -0.17041278 -1.4761653  ... -0.68666434 -0.07690293\n",
      "   -1.3182452 ]\n",
      "  [-0.81649506  0.96701825  1.6285517  ...  1.5594428  -0.7915914\n",
      "   -1.0711432 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.1169167   0.21153617 -2.195067   ... -0.45594198 -0.520287\n",
      "    0.19070059]\n",
      "  [-0.48808628  2.304083    0.93243384 ...  1.6158302  -0.50683814\n",
      "   -0.9454728 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.5789733   0.86524737 -0.47244668 ... -1.1077225  -0.34421915\n",
      "   -1.134606  ]\n",
      "  [ 0.40410584  0.3074997   1.5892836  ...  0.28636056 -0.0290184\n",
      "   -0.32307303]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.17275095  1.3644006  -2.0849266  ... -0.16016525  0.9726867\n",
      "   -0.2795249 ]\n",
      "  [-1.636534    2.1415577   0.40029907 ...  1.4971223  -0.7433246\n",
      "   -1.9333591 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.5955721   0.08071077 -1.7821352  ...  1.2613065  -0.6202675\n",
      "   -0.66207504]\n",
      "  [ 0.23617125  1.9424956   0.7590202  ...  0.9740267   0.03113122\n",
      "   -2.4153886 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.471777    1.1932865  -3.1571562  ... -0.16691762 -1.2767773\n",
      "   -1.2825081 ]\n",
      "  [-0.12300399  2.147784    1.0987694  ... -0.300808   -0.14047496\n",
      "   -1.0364171 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.889809    0.5447798   0.01566124 ...  0.29386398  0.0320136\n",
      "    0.2143361 ]\n",
      "  [ 0.9105029  -0.40165484  1.411061   ...  1.0567784   0.64422727\n",
      "   -0.78007984]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.8210369   0.64022714 -0.6157563  ... -0.7872625  -1.4754386\n",
      "   -0.58240783]\n",
      "  [ 0.71853817  1.4211829   1.8921278  ...  1.4772072   1.9309745\n",
      "   -2.8665948 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.5496596   1.4495143  -0.0475198  ... -1.7109298  -0.04258412\n",
      "    0.19818234]\n",
      "  [-0.54935277  2.5992303   0.24354452 ... -0.6194046   0.01558103\n",
      "   -0.48688352]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.83840406  1.2664553  -0.8550973  ... -1.5973951  -0.08213821\n",
      "   -0.24105024]\n",
      "  [-0.7596712   1.6126362   2.144856   ...  0.30623865  0.5661278\n",
      "   -1.7986059 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.504403    1.9239597  -1.4228979  ... -0.50663584 -0.37895986\n",
      "   -0.7225469 ]\n",
      "  [ 1.131637   -0.4843812   1.0361274  ... -0.6299018   0.5851396\n",
      "   -0.7795522 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.504403    1.9239597  -1.4228979  ... -0.50663584 -0.37895986\n",
      "   -0.7225469 ]\n",
      "  [ 1.131637   -0.4843812   1.0361274  ... -0.6299018   0.5851396\n",
      "   -0.7795522 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.8113239   0.46824253  0.04474628 ... -1.0281287   0.10629249\n",
      "   -1.1125641 ]\n",
      "  [-0.10782343  0.43166214  1.3290911  ...  2.0326657  -0.43950343\n",
      "   -0.5186233 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.90823114  1.4538162  -2.009945   ... -1.6225722  -0.1552796\n",
      "   -0.06620568]\n",
      "  [-1.2838985   0.5348448   0.8964659  ...  0.54307795 -1.201669\n",
      "   -1.0015087 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.176573   -0.05223608 -3.157485   ... -0.7647095   0.48629528\n",
      "   -0.2265898 ]\n",
      "  [ 0.54627025  0.78971386  1.0036517  ...  0.11869755  0.19319247\n",
      "   -1.6072919 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.9647606   0.7141076  -0.18139172 ... -0.79255843 -1.7720432\n",
      "   -0.00211728]\n",
      "  [-1.1257263   0.2598881  -0.10454535 ...  1.5776677   0.30753493\n",
      "   -1.4423299 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.033942    1.8970867  -1.7625393  ...  0.47866908 -0.02770758\n",
      "   -2.427098  ]\n",
      "  [ 1.3874953   1.8595631   0.02694058 ...  1.085931   -0.5354709\n",
      "   -1.5492988 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.07411557  1.9466851  -0.01746523 ...  1.0259619  -0.09880257\n",
      "   -0.38326448]\n",
      "  [-1.0584954   1.7379211   0.6161231  ... -0.40144795 -0.7579276\n",
      "   -0.65217215]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.2356093  -0.02356839 -3.1356297  ... -0.27819043 -0.02565891\n",
      "    0.22398067]\n",
      "  [ 0.44081485  1.1590453   0.42024785 ...  0.63899827  0.83735\n",
      "   -0.21275848]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.0681663   2.0098734  -2.865146   ... -1.0972472   0.09478617\n",
      "    0.20197946]\n",
      "  [ 0.6491798   2.0153067   2.6815484  ...  0.61137855 -1.0919552\n",
      "   -1.7957354 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.6185105   0.5707581  -1.4558182  ... -1.4413292  -0.44761205\n",
      "   -1.1674392 ]\n",
      "  [ 1.1242348   1.6505644   0.06427073 ... -0.62531775  0.34217566\n",
      "   -1.0725194 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.71411     0.135777   -1.1599543  ... -0.58615154 -1.723069\n",
      "   -1.6918299 ]\n",
      "  [-0.37586847  0.45562303  0.65082645 ...  0.20625451  0.05746195\n",
      "   -0.660994  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.9447025   1.1211306  -2.41491    ... -0.78461885 -1.4538133\n",
      "   -0.1403336 ]\n",
      "  [ 0.97506166  1.2094728   0.4306112  ...  0.34817895  0.99395776\n",
      "   -0.41475236]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.3394362  -0.09285295 -2.373384   ...  0.15593585  0.86559695\n",
      "    0.33207893]\n",
      "  [ 0.24746373  0.234353    1.8937359  ...  1.4628139   0.71816456\n",
      "   -1.1918315 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.7428991   1.29648    -1.2106036  ...  0.4821907  -0.5437837\n",
      "   -1.0571227 ]\n",
      "  [-0.7317518   1.0387948  -0.36473346 ... -0.7786284   0.7283521\n",
      "   -0.2594095 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.0261192   0.87493145 -1.6738955  ... -0.09241626 -0.39893097\n",
      "   -1.594315  ]\n",
      "  [ 0.5347165   1.4131262   0.6467709  ...  0.08299646  0.24420376\n",
      "   -0.62537223]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.1515975   2.7886314  -2.5645316  ... -0.29955488 -1.2898432\n",
      "   -1.8848566 ]\n",
      "  [ 0.21034825  0.11761975  0.7989826  ...  0.6243549  -0.20884247\n",
      "   -1.7273358 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.8221178   0.8007367  -1.3238285  ...  0.00730574 -1.4711826\n",
      "   -1.6795232 ]\n",
      "  [ 0.39108038  2.1917138  -0.2714193  ... -0.5200631  -1.2745945\n",
      "   -0.26470643]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.22860944  2.1191487   0.10436046 ... -0.41344327 -1.8299708\n",
      "   -1.7577267 ]\n",
      "  [-0.3239699  -0.21551299 -0.00414407 ...  1.7544403   1.1543177\n",
      "    0.20686674]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.08278173  2.1533365  -1.817603   ... -0.56915337 -0.80766964\n",
      "    0.51727843]\n",
      "  [ 0.64497745  1.475647    0.40285945 ...  0.13956872 -1.2502162\n",
      "   -1.0173466 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.5280774   0.22299945 -1.8977956  ... -0.8186183  -0.99177814\n",
      "   -0.6043196 ]\n",
      "  [ 0.4400924   0.5076476   0.29658324 ...  0.6542966   0.6791532\n",
      "   -1.2463654 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.0949308   1.2376467  -2.049753   ... -0.03047848  0.3847152\n",
      "    0.22192669]\n",
      "  [-0.97573173  1.5835942   0.98458886 ...  0.35921597  0.70650053\n",
      "   -0.67348313]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.6341598  -0.2284931  -2.091419   ... -0.17702854 -0.6192197\n",
      "    0.27937424]\n",
      "  [ 0.2619679   2.6941319   1.6241302  ...  0.03828135  0.79808176\n",
      "   -0.46629345]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.0776198   2.170223   -1.2957292  ... -0.5012435   0.81622416\n",
      "   -0.25485784]\n",
      "  [-0.09551762  0.29285455  0.12451512 ...  1.2023416  -0.0482942\n",
      "   -1.1894797 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-2.1331871e-01 -3.7896746e-01  9.8948789e-01 ... -1.4485674e+00\n",
      "    2.9604977e-01 -8.1329250e-01]\n",
      "  [-1.1318926e+00  6.4028263e-01 -3.0456517e+00 ... -1.9580246e+00\n",
      "    2.4517053e-01 -1.2342091e+00]\n",
      "  [ 5.8203936e-04  7.7018267e-01  1.5899866e+00 ...  1.7771059e-01\n",
      "   -8.2796347e-01 -1.8724320e+00]\n",
      "  ...\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.4436771   0.90375435 -2.1371205  ... -2.1391878  -1.2040441\n",
      "    0.9613037 ]\n",
      "  [ 0.39356118  0.88491416  0.7100907  ...  0.15478042  0.1112992\n",
      "   -1.7193283 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.4333162   1.3889562   0.18953335 ...  0.67460215 -2.0545003\n",
      "   -1.571048  ]\n",
      "  [ 1.5466081   0.20870626  1.9095376  ...  0.23967882 -0.62652874\n",
      "   -2.1300337 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.2390141   0.87513006 -2.5657492  ... -1.1985112  -0.9049356\n",
      "   -1.3139374 ]\n",
      "  [-0.2025057   1.7243578   0.3279937  ...  1.4699895   1.460967\n",
      "   -1.4007562 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.464118    1.5085168  -1.5891827  ...  0.08984151  0.4802568\n",
      "   -0.20861256]\n",
      "  [-1.5736314   2.073793    1.2796034  ...  0.8305301  -0.6903627\n",
      "   -1.454255  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.31300586  1.3189334  -1.65447    ... -1.4347073  -1.3142927\n",
      "   -2.2915487 ]\n",
      "  [ 0.02888428  1.7250092   1.2173151  ...  0.7456095  -0.48347896\n",
      "   -1.7871466 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.018801    0.38802862 -1.5218627  ...  0.641662    1.0102739\n",
      "   -1.6526507 ]\n",
      "  [-0.67589843  0.1351589   0.8039297  ... -0.29580766  1.8711002\n",
      "    0.30709326]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.247116    0.14142168 -0.895164   ... -1.8128638  -0.9543562\n",
      "   -0.36260948]\n",
      "  [-0.77667046  0.79411554  1.2175279  ...  0.60327744  0.6835846\n",
      "   -0.5957683 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.0078996   0.68928003 -1.7918563  ... -1.0654589  -0.9027201\n",
      "   -1.4477446 ]\n",
      "  [-1.2700068   1.2559484   2.3721495  ...  1.4215376  -0.5408983\n",
      "    0.20980787]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.67364866  2.347542   -1.1055365  ... -0.02000442 -0.04625443\n",
      "   -0.7993426 ]\n",
      "  [-0.6485429   1.0352787   2.2663093  ... -0.36975646 -0.6082754\n",
      "   -1.6402192 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.4724336   0.66600883 -2.156853   ... -0.72734904 -0.8073001\n",
      "   -1.0286175 ]\n",
      "  [-0.25420326  0.08345771  0.76739746 ... -0.6311714   0.9700639\n",
      "   -1.5479777 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.6813269   0.2915421  -1.4581859  ... -1.0143192  -0.11072493\n",
      "   -1.4997686 ]\n",
      "  [-0.15317449  0.68145233 -0.14416265 ...  0.37954748 -0.84471947\n",
      "   -1.0957999 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.36058527  1.8736433  -1.2345798  ... -0.35541195  0.30849254\n",
      "   -0.04167652]\n",
      "  [-0.01271034  1.0694885   0.11395538 ... -0.02830017  1.358714\n",
      "   -2.231663  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.5732338   1.4113684  -1.0134916  ... -1.2131093  -0.20407562\n",
      "   -2.1518216 ]\n",
      "  [-1.2331823  -0.09377611  1.9924767  ...  0.7449777  -0.9004448\n",
      "   -1.1280735 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.4037151   2.0736427  -0.4602841  ...  1.0975723  -1.7530499\n",
      "   -1.4187362 ]\n",
      "  [ 0.46165076  1.749445    1.7033393  ... -1.1036925   1.1185647\n",
      "   -1.4778558 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.46874368  1.6237075  -0.87186813 ... -2.0383525   0.5430664\n",
      "   -1.0925845 ]\n",
      "  [ 0.4095866   1.2812656  -0.18370318 ...  0.32644376 -0.24536552\n",
      "   -1.8174119 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.79004574  0.8048582  -0.93142474 ... -0.44292367 -1.1773758\n",
      "   -0.4772532 ]\n",
      "  [-0.09551762  0.29285455  0.12451512 ...  1.2023416  -0.0482942\n",
      "   -1.1894797 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.27936137  1.195689   -3.128048   ... -0.22682792  0.94638723\n",
      "   -1.0208722 ]\n",
      "  [ 1.036861    0.80974495  1.4958962  ...  1.2401755   0.27049848\n",
      "   -1.0129995 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.18798476  1.8739154  -1.5539088  ... -0.5587678  -1.609652\n",
      "   -2.1062877 ]\n",
      "  [ 0.96265733  0.4280153   0.33124977 ...  0.08767524  0.01168765\n",
      "   -0.70130193]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.0088701   1.39398    -1.5469891  ... -0.9868101   0.46180278\n",
      "   -0.746433  ]\n",
      "  [ 0.57787085  0.43570518  0.9003787  ...  1.1466296  -0.4224335\n",
      "   -0.09968412]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.6800153   0.9533483  -2.2009525  ... -1.6826866   0.11436361\n",
      "   -0.88793314]\n",
      "  [-0.22407015  2.6642785   1.0045191  ...  0.9373604  -0.4469332\n",
      "   -1.4908998 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.15182596  2.328302   -0.6151045  ... -0.6056583   0.77466613\n",
      "   -1.99396   ]\n",
      "  [-0.8772119   0.77853096  1.8370786  ...  0.08216822 -0.59643525\n",
      "   -1.4324054 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.09865767  1.2820166  -1.9949334  ...  1.133806   -0.91774637\n",
      "    0.22217369]\n",
      "  [ 0.36222926  1.0804597   1.1205344  ...  0.5847548   0.60117054\n",
      "   -2.2415006 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.60323274  0.04906654 -1.2429657  ... -1.4382493   1.1640425\n",
      "   -1.5585306 ]\n",
      "  [-0.20867223  2.2727675   0.68259573 ...  0.5648098   1.86273\n",
      "   -2.2951097 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.3255399  -0.11488605 -1.9301054  ...  0.7093848   0.72318226\n",
      "   -1.7207735 ]\n",
      "  [ 0.15346298  1.8857934   0.4747855  ... -0.16723502  0.32730672\n",
      "   -1.1743737 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.30222106  0.8185985  -1.9485468  ...  0.21419027 -0.60734504\n",
      "   -0.924075  ]\n",
      "  [ 0.5122256   0.6599174   1.4787903  ...  0.3974445   0.42733002\n",
      "   -2.7267795 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.05566621  2.2095547  -0.60200226 ... -1.4514236  -0.8338176\n",
      "   -0.8100675 ]\n",
      "  [ 0.7148944   2.631575    0.96218944 ...  1.4732547  -0.02307494\n",
      "   -2.193375  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.2543454   1.0683625  -3.031645   ...  0.12532184  0.6350755\n",
      "   -2.203071  ]\n",
      "  [ 1.0435001   2.1308465   2.0833626  ...  0.8902539  -0.47608763\n",
      "   -0.8324023 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.9871471   1.6526558  -1.315464   ... -0.5721312   0.9715511\n",
      "   -0.51050586]\n",
      "  [ 1.0536093   1.1776563   1.9088148  ...  1.0831573  -0.76842195\n",
      "   -1.3950313 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.60604095  1.6174805  -1.1376207  ... -0.08906984 -1.4415479\n",
      "   -1.2308149 ]\n",
      "  [ 0.32435346  1.7432181   1.4375868  ...  0.8455901  -0.86604947\n",
      "   -1.6160862 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.8970936   1.5362982  -2.0721285  ... -1.8187861  -0.83618814\n",
      "   -0.6300417 ]\n",
      "  [ 0.49023208  0.49778092  0.5066698  ...  0.13404444  0.16744138\n",
      "   -1.5678244 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.15182596  2.328302   -0.6151045  ... -0.6056583   0.77466613\n",
      "   -1.99396   ]\n",
      "  [-1.0233952   1.7498918   0.01243269 ...  1.291441   -0.31520873\n",
      "   -1.1562694 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.37959     1.7000697  -1.0801378  ... -0.26032853 -0.8170806\n",
      "   -0.6881235 ]\n",
      "  [ 0.23898679  2.3012323   0.56415313 ... -0.70014447 -0.25164926\n",
      "   -1.9044936 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.47213417  1.6881429  -1.4605955  ... -1.3048886  -1.4953182\n",
      "   -0.08968836]\n",
      "  [-0.15317449  0.68145233 -0.14416265 ...  0.37954748 -0.84471947\n",
      "   -1.0957999 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.4669414   1.7294104  -1.2096552  ... -0.6058103  -0.8672181\n",
      "   -1.8499925 ]\n",
      "  [ 1.465185    0.86811405  1.3979436  ...  1.4672103  -0.7830871\n",
      "   -0.9125046 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.59176624  2.7038975  -1.4336294  ... -1.1523821   0.07210076\n",
      "   -1.3581688 ]\n",
      "  [ 0.4250993   0.71104264  1.3295656  ... -1.0167983  -0.8517812\n",
      "   -0.9931153 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1340848   2.0198684  -2.1479087  ... -0.686191    0.86921436\n",
      "   -0.21278256]\n",
      "  [ 0.02570783  0.313581    0.06686616 ... -0.42922217  0.13496432\n",
      "   -0.11166573]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.557312    1.9512906  -1.9646823  ... -1.8990709  -0.7697123\n",
      "   -0.9589468 ]\n",
      "  [ 0.29873666  1.2842437   1.2174405  ... -0.41589695  1.0080571\n",
      "   -0.20823503]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.06634682  2.0295458  -1.0067883  ... -0.49725497 -1.6149602\n",
      "   -1.8728821 ]\n",
      "  [-0.15860772  0.6143984   1.5492325  ...  1.940103    0.9480266\n",
      "   -1.9360979 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.7567117   1.4685197  -2.39773    ... -0.08301803  1.2325821\n",
      "   -1.5676467 ]\n",
      "  [ 0.352129    0.46608323  0.33880365 ...  0.578465   -0.6578559\n",
      "   -0.519656  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.6316578   1.1585062  -1.357767   ... -1.3505569   0.43255877\n",
      "    0.17703629]\n",
      "  [-0.7293335  -0.1795727   0.9349     ... -0.16197073  0.48952985\n",
      "    0.1651907 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.6086572   0.85905755 -0.1712836  ...  0.16612896  0.58024055\n",
      "   -1.4108868 ]\n",
      "  [ 1.0210556   1.7016258   1.7246716  ... -1.1022444   0.3423319\n",
      "   -0.30002993]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.8470821   0.65609634 -0.1766814  ... -0.72749376 -1.284437\n",
      "    0.19712955]\n",
      "  [ 1.333379    2.1629765   0.67017734 ...  1.1590605   0.6475485\n",
      "   -1.6479785 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-2.1331871e-01 -3.7896746e-01  9.8948789e-01 ... -1.4485674e+00\n",
      "    2.9604977e-01 -8.1329250e-01]\n",
      "  [-1.0728122e+00  1.4498538e+00 -2.9325502e+00 ... -2.3232162e-01\n",
      "   -9.4159597e-01 -1.4672623e+00]\n",
      "  [-2.1699858e-01 -2.8636456e-03  6.9106627e-01 ...  1.5768883e+00\n",
      "    5.5836082e-01 -2.2355533e+00]\n",
      "  ...\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.45455664  1.3508635  -1.4625999  ...  0.66258085 -0.5781737\n",
      "    0.3163799 ]\n",
      "  [-1.4654087   2.0073133  -0.01617026 ...  0.9100679   1.8216547\n",
      "   -1.1616398 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.799506    0.09615457 -2.4722226  ... -1.4522668  -0.45191485\n",
      "   -0.14799035]\n",
      "  [ 1.7224581   0.26939273  0.8986318  ... -0.75280267  0.7507661\n",
      "   -2.055903  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.9168663   2.1797419  -1.531074   ... -1.3559976  -1.4334459\n",
      "    0.21515018]\n",
      "  [-0.49932733  0.70539236  1.8040049  ... -0.59085804 -0.2064458\n",
      "   -1.8928701 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.3919868   1.6748543  -1.3082263  ... -0.29230493 -0.86805105\n",
      "   -0.80000746]\n",
      "  [-0.22639215  1.2842681   1.0291824  ...  0.4610096   0.06687257\n",
      "   -2.0817459 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.8552166   1.4734704  -1.9173946  ... -1.3561288   0.9357665\n",
      "   -2.0839696 ]\n",
      "  [-0.3293223   0.01919794  0.16520137 ...  2.2229145   1.6413465\n",
      "   -0.21992344]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.06978208  2.4035466  -1.1276438  ... -1.6456453  -1.2144096\n",
      "   -0.21097529]\n",
      "  [ 0.5609963   1.392101    0.7405205  ... -0.71764547 -0.7388969\n",
      "   -2.3996696 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.5812676   1.2042316  -1.4917998  ...  0.34281877 -0.35972214\n",
      "   -0.74758744]\n",
      "  [ 0.26348895  0.5492385   1.1402447  ...  0.74771214 -0.9983172\n",
      "    0.05410099]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.21798813  0.5755595  -1.8697433  ...  0.8005668  -0.8537883\n",
      "   -0.59002125]\n",
      "  [ 1.3557818   1.2625103   0.89624584 ...  0.04578725  1.5941827\n",
      "   -1.1818771 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.8002039   1.0275931  -1.5731214  ... -1.4254581  -1.2735407\n",
      "   -1.1735991 ]\n",
      "  [-0.9598093   0.76489747  1.2589383  ...  1.3400079   0.02252106\n",
      "   -1.9108498 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.33483362  0.92651415 -1.9202399  ...  0.45585397 -0.45168227\n",
      "   -0.80166346]\n",
      "  [-0.2855371   1.8813791   0.72888005 ... -0.26555318  1.1386297\n",
      "   -0.81184405]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.6587446   0.62446785 -1.927347   ... -0.5817749  -0.07169664\n",
      "   -1.1416066 ]\n",
      "  [ 0.31825018  2.3250363   0.483113   ...  0.43906593  0.78135467\n",
      "   -0.45340025]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.436759    1.0734053  -0.15506303 ... -0.47321406 -0.36162412\n",
      "   -0.88843644]\n",
      "  [-0.9449303   1.2776655   0.2315951  ...  1.8773417   1.523257\n",
      "   -0.9871348 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.5032804   1.1600275  -1.584367   ...  0.34427163 -0.74050444\n",
      "    0.388425  ]\n",
      "  [-0.11172854  1.3997725   1.4200183  ...  0.669844    0.05186298\n",
      "   -3.032608  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.4377073   2.4818344  -1.6312991  ... -0.28674322 -1.7157543\n",
      "   -0.762775  ]\n",
      "  [-0.7293335  -0.1795727   0.9349     ... -0.16197073  0.48952985\n",
      "    0.1651907 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.664263    0.85164523 -1.6221316  ... -1.1930693   0.1209659\n",
      "    0.61198056]\n",
      "  [-0.6303556   1.1039629   0.26616216 ...  0.89032793  0.6111486\n",
      "   -0.13016939]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.7641363   2.0641537  -1.8725826  ... -0.37930354  0.6742316\n",
      "   -0.9303336 ]\n",
      "  [-0.14875595  1.8637395   1.8025856  ...  0.00558662  0.95813435\n",
      "   -2.5013208 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.043168    0.5916088  -1.2594091  ... -0.33295682 -0.19470105\n",
      "   -0.3048776 ]\n",
      "  [-0.18376552  2.2820945   0.3789829  ...  1.4222383  -0.27118462\n",
      "   -0.8437499 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.4448006   0.25128514 -1.4095477  ...  0.01831988 -1.0678375\n",
      "   -1.0085704 ]\n",
      "  [ 0.46165076  1.749445    1.7033393  ... -1.1036925   1.1185647\n",
      "   -1.4778558 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.6472616   0.85717547 -1.3267624  ... -0.17273563 -0.11822033\n",
      "   -1.959537  ]\n",
      "  [-0.0965137   1.7066419   2.645688   ...  0.3604448   0.37970334\n",
      "   -1.410692  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.9405168   2.2277725  -1.0307689  ... -1.2446191  -1.172065\n",
      "   -0.70205945]\n",
      "  [-0.25950328  2.7263546   1.8076413  ...  1.5762894  -0.18833403\n",
      "   -1.400356  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.285521    0.22277427 -0.4452455  ...  0.23328903 -0.6264912\n",
      "   -1.9311645 ]\n",
      "  [ 1.3275114   1.4189377  -0.4896084  ...  0.67777896  1.0457877\n",
      "   -2.0964599 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.22050697 -0.10816669 -0.57283974 ...  0.7252208  -0.10154098\n",
      "   -2.289012  ]\n",
      "  [ 0.85935557  1.9119561  -0.05594122 ...  1.3516576   0.07517821\n",
      "   -0.9341631 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.81703     0.30285937 -1.3862302  ... -0.5313797  -0.66087866\n",
      "   -0.07396054]\n",
      "  [ 0.5955608   1.0522517   0.65496755 ...  1.3905139   0.12381604\n",
      "   -1.1869348 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.0663013   0.9499075  -2.8936992  ... -1.1131152  -0.6583781\n",
      "   -1.6711217 ]\n",
      "  [-0.28246534  0.86450875  0.09307325 ...  1.3690532   1.2952598\n",
      "   -1.345316  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.8129413   0.44398797 -2.011082   ... -0.9547458  -0.61195457\n",
      "   -0.19257575]\n",
      "  [-0.04165952  0.17850792 -0.07021511 ... -0.83134025  0.23935157\n",
      "   -0.76489085]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.2365978   2.248652   -1.8818545  ... -0.7067533  -1.5425897\n",
      "   -1.525275  ]\n",
      "  [ 0.8654524  -0.3940742   0.60864466 ...  0.48567447 -0.5948997\n",
      "   -1.8369405 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.0533984   2.0220463  -1.476438   ... -1.2354679  -0.71597326\n",
      "   -0.6511458 ]\n",
      "  [ 0.07080052  1.0198988  -0.15975952 ...  1.4704866   0.25242153\n",
      "   -1.9864572 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.7710431   1.1753896  -2.550491   ... -0.87069577 -0.77675223\n",
      "   -0.63326883]\n",
      "  [ 0.50983286  1.7148495  -0.1962049  ...  1.2581977   0.32666582\n",
      "   -1.0109327 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.44369888  2.5350497  -1.3717782  ... -1.8307687   0.31186742\n",
      "   -1.1812512 ]\n",
      "  [-1.4986335   2.9648752   1.6862969  ...  0.30759656  1.2276872\n",
      "   -2.3601537 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.7641363   2.0641537  -1.8725826  ... -0.37930354  0.6742316\n",
      "   -0.9303336 ]\n",
      "  [-0.36860523  1.0545053   0.16393465 ... -0.37004328  1.7001027\n",
      "   -1.181744  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.5696806   0.15592188 -1.8755857  ... -0.74831355  0.4821821\n",
      "   -2.0617151 ]\n",
      "  [-0.36860523  1.0545053   0.16393465 ... -0.37004328  1.7001027\n",
      "   -1.181744  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.37581176  0.27870405 -0.84293574 ...  0.6515554  -0.6597283\n",
      "   -0.0480547 ]\n",
      "  [-0.7380228   0.47410923 -0.6538098  ...  1.6943436   0.33751822\n",
      "   -1.6956258 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.12372905  2.097402   -1.7598511  ... -0.96615887 -0.60279787\n",
      "    0.11267108]\n",
      "  [-0.10048048 -0.37517703  0.13669473 ...  1.2850523  -0.4668194\n",
      "   -0.04771185]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.7857562   0.61936796 -2.1467645  ... -0.3249663   0.2737519\n",
      "   -0.6545106 ]\n",
      "  [ 0.6358057   1.0126138   1.5483968  ...  2.183529    0.4586444\n",
      "   -1.5239449 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.3016043   2.8050156  -2.3591363  ... -0.61968905  0.01474524\n",
      "   -0.990502  ]\n",
      "  [-0.65949464  1.7246192  -0.3850547  ...  0.9934138  -0.04237644\n",
      "   -0.73948395]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.0563464   2.400428   -1.4543251  ... -0.96554714  0.16655374\n",
      "   -0.507928  ]\n",
      "  [-0.5581534   1.2283463   0.32025528 ... -1.0892568   1.7334313\n",
      "   -2.4821281 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.4010595   1.6798022  -1.6359432  ... -1.4713751  -0.49394947\n",
      "   -0.8214774 ]\n",
      "  [ 0.14616181  1.9108872   1.418808   ... -0.26722962 -0.9071055\n",
      "   -0.7242271 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.23757118  1.4370532  -1.2490548  ... -0.67552054 -0.41620377\n",
      "   -2.1820002 ]\n",
      "  [-0.18626384  0.8471719  -0.21879554 ...  1.2608787   0.3746369\n",
      "   -2.1505175 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.8383721   0.72239614 -1.938633   ... -1.7457287  -1.2332625\n",
      "   -0.93165535]\n",
      "  [-0.855605    1.6106641   1.4291519  ...  1.7649448  -1.3860595\n",
      "   -1.4382793 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.45625287  0.30147713 -2.3914766  ... -1.3299311  -0.97038984\n",
      "    0.2607317 ]\n",
      "  [ 0.73288095  0.5901951   0.7898532  ... -0.20512515  0.19515343\n",
      "   -1.7807677 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.8470821   0.65609634 -0.1766814  ... -0.72749376 -1.284437\n",
      "    0.19712955]\n",
      "  [ 0.6818942  -0.04802394  2.4000392  ...  0.9110058   0.7146777\n",
      "   -0.15427184]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.2543454   1.0683625  -3.031645   ...  0.12532184  0.6350755\n",
      "   -2.203071  ]\n",
      "  [-0.29885963  1.8257335   1.8913772  ... -0.0372799  -0.6743233\n",
      "    0.09385324]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.0195312   2.642386   -1.9115565  ... -1.2064073  -1.6608515\n",
      "   -1.061588  ]\n",
      "  [ 1.3505545   1.6264181   1.6768562  ...  0.56138307  0.589967\n",
      "   -0.32183862]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.0705742   1.9122645  -1.3257608  ... -0.6148347   0.38065487\n",
      "    0.21899438]\n",
      "  [-0.8006573   1.3905792  -0.39171743 ...  0.6602659  -1.2898535\n",
      "   -2.4885542 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.1012278   0.0242126  -1.4317975  ...  0.46714428  1.2373931\n",
      "   -0.40129703]\n",
      "  [ 0.5186541   1.5708381   1.261816   ...  0.06830019  1.8418057\n",
      "   -1.7008138 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.28870368  1.9353721  -2.2109258  ... -0.21933553 -1.0110655\n",
      "   -0.26708668]\n",
      "  [-0.9726013   2.3915024   0.1307283  ... -0.15333915 -0.7783196\n",
      "   -0.42740571]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.91393614  1.8179417  -0.58531684 ... -1.64057    -0.48518646\n",
      "   -1.0129509 ]\n",
      "  [ 1.2477338   1.0523498   0.5000715  ...  0.3378397   1.8413937\n",
      "   -2.3544838 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.0242151   1.5196342  -1.3742554  ... -0.23810668 -0.3687783\n",
      "   -1.0549304 ]\n",
      "  [-0.182097    0.87987006  0.5648676  ...  0.55037516  0.26780543\n",
      "   -2.3669915 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.9216524   0.38255018 -2.3411791  ... -1.1374003   0.37110317\n",
      "    0.20514   ]\n",
      "  [ 0.03882968  1.3061533   1.7626808  ... -0.49900907 -1.0813961\n",
      "    0.5668216 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.0396633   0.6037002  -1.4908904  ... -2.1026783   0.8465912\n",
      "   -1.9204639 ]\n",
      "  [ 0.60564184  1.4809685   0.75147283 ...  0.2960318   1.0006194\n",
      "    0.16302156]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.8462914   2.2137103  -1.8711588  ...  0.5062374  -1.4466088\n",
      "   -1.0939908 ]\n",
      "  [ 0.10867687  1.9945186   0.9624474  ...  2.1373963  -1.0708756\n",
      "   -1.232206  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.889809    0.5447798   0.01566124 ...  0.29386398  0.0320136\n",
      "    0.2143361 ]\n",
      "  [ 0.13641432  2.358839    0.28447783 ...  0.25704408 -0.07562646\n",
      "   -0.9020304 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.27670568  1.6339245   0.03851426 ... -0.70046204  1.242821\n",
      "   -0.18524367]\n",
      "  [ 1.6841779   0.78130245  0.9057177  ...  1.2126594   0.7286948\n",
      "   -1.3666596 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.2060069  -0.59847903 -1.1137421  ...  0.5953536  -0.9490599\n",
      "   -0.3980492 ]\n",
      "  [ 1.5100791   1.7298467   1.7622778  ...  0.7633461   0.19606912\n",
      "   -0.19525337]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.5296357   0.6739199  -2.6429982  ... -0.31611055 -0.5168528\n",
      "   -1.945298  ]\n",
      "  [ 0.3575081   1.6233828  -0.5827968  ... -0.47850722  0.4324925\n",
      "   -0.7432927 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.6280196   0.70456696 -1.2933992  ... -0.5148295  -1.4121914\n",
      "   -1.2120163 ]\n",
      "  [-0.724211    0.7915058   0.05697703 ... -0.68337184  1.5171312\n",
      "   -2.3562305 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.1564865   0.62304264 -1.3771874  ... -0.3638931  -0.9571027\n",
      "   -1.3304503 ]\n",
      "  [-0.7520659   2.634912    1.6364925  ...  0.49675825 -0.23693474\n",
      "   -1.2519907 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.7657407  -0.4114504  -2.311164   ... -0.21282646  0.9183689\n",
      "   -1.0684366 ]\n",
      "  [ 0.5994785   2.0652988   1.529364   ...  0.58929694 -0.12419932\n",
      "   -0.3280838 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.71068543  0.7820308  -2.2862806  ... -1.2797124   0.49050528\n",
      "   -0.9414596 ]\n",
      "  [-0.9911872   0.10243487  0.57833403 ...  1.1472634  -0.3808037\n",
      "   -0.33030796]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.6959129   1.391699   -1.6918015  ...  0.34136894 -0.7387372\n",
      "    0.24148285]\n",
      "  [-0.792349    1.7465041   0.474335   ...  0.52446383 -0.31391597\n",
      "   -2.5749474 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.4049911   0.34982055 -1.2452     ... -0.59956366  0.8087351\n",
      "   -2.4848602 ]\n",
      "  [ 0.6116319   0.5260995   0.52882415 ...  0.2678331   0.3213352\n",
      "   -1.2448461 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.4511838   1.7129693  -1.1173403  ... -0.34616268  0.2251774\n",
      "   -0.20582002]\n",
      "  [ 0.08230169  0.47380656  2.8484297  ...  0.7280959  -0.23109724\n",
      "   -1.251974  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.2116611   2.3747787  -1.9683754  ... -1.1119314   0.15614283\n",
      "   -0.2104075 ]\n",
      "  [-0.724211    0.7915058   0.05697703 ... -0.68337184  1.5171312\n",
      "   -2.3562305 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.24876219 -0.15536666 -2.5527287  ...  0.3167934  -1.5005028\n",
      "   -0.8376647 ]\n",
      "  [-0.36860523  1.0545053   0.16393465 ... -0.37004328  1.7001027\n",
      "   -1.181744  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-2.1331871e-01 -3.7896746e-01  9.8948789e-01 ... -1.4485674e+00\n",
      "    2.9604977e-01 -8.1329250e-01]\n",
      "  [-1.1753578e+00  5.8965838e-01 -2.6125793e+00 ... -1.7484725e-03\n",
      "   -3.0857667e-01  8.0049026e-01]\n",
      "  [-7.2421098e-01  7.9150581e-01  5.6977034e-02 ... -6.8337184e-01\n",
      "    1.5171312e+00 -2.3562305e+00]\n",
      "  ...\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]\n",
      "  [ 1.7484777e+00 -2.3501392e-01  1.5967557e+00 ...  1.4864607e+00\n",
      "   -1.9107313e+00 -2.3773763e-01]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.5035245   0.9680867  -1.5350449  ...  0.5574378  -1.0535247\n",
      "   -2.0805686 ]\n",
      "  [-0.22601947  1.0526289   0.760046   ...  0.05325156 -0.21857782\n",
      "   -0.26890606]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.25146884  1.62032    -0.9822385  ...  0.51268506 -1.4701786\n",
      "   -1.6532083 ]\n",
      "  [ 1.0479639   0.37310976  0.764333   ... -0.7434706   0.53038734\n",
      "   -1.5633157 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.27426368  1.056677    0.0918169  ... -0.05003583 -0.52738994\n",
      "   -1.2242796 ]\n",
      "  [ 0.05033012  1.6735457   2.5091176  ...  0.6496137  -0.39590043\n",
      "   -0.6981876 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.0705742   1.9122645  -1.3257608  ... -0.6148347   0.38065487\n",
      "    0.21899438]\n",
      "  [-0.26970237  0.81500065  2.1151185  ... -0.85408205 -1.4759963\n",
      "   -0.81151676]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.3319428   0.6221596   0.14513719 ...  1.0912313  -1.9110847\n",
      "   -0.13656932]\n",
      "  [ 1.1791567  -0.02962911  0.02247882 ...  1.2514534  -0.92500454\n",
      "   -1.222936  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.12153995  1.2266023  -1.4027411  ...  0.18131378  0.09314489\n",
      "   -2.1639724 ]\n",
      "  [-0.02298271  1.3337494   1.0681438  ... -0.2486965   1.3334246\n",
      "   -1.8818139 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.9993098   2.2924035  -2.1194644  ...  1.0753628   0.07614493\n",
      "   -0.4977299 ]\n",
      "  [-1.3013197   1.1845529  -0.6511191  ...  0.57374257 -0.25062692\n",
      "   -2.4156804 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.6366389   2.1549544  -0.827367   ...  0.9233551  -1.0242859\n",
      "   -2.1258266 ]\n",
      "  [-0.07847571  1.5197176   1.0282184  ... -0.05215019  1.0373011\n",
      "   -1.1317043 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.9029928   0.4983517  -1.4926152  ... -1.330502   -1.2749181\n",
      "   -0.34807286]\n",
      "  [ 1.6226717   2.1112227   1.1085639  ... -0.05332136  0.14607404\n",
      "    0.5419476 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.5648963   0.6828049  -1.7643542  ...  0.33177045  0.04060698\n",
      "   -1.2326497 ]\n",
      "  [-1.0207694   0.5325354   0.9685516  ... -0.2840793  -0.9202469\n",
      "   -0.28717214]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1483778   0.11149734 -1.1400812  ... -1.0724281  -0.51728445\n",
      "   -1.1918671 ]\n",
      "  [ 0.74285555 -0.35284877 -0.1097908  ...  0.38281825 -1.1037501\n",
      "   -1.3985677 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.35768676  0.2713487  -1.3216113  ... -1.3639818   0.3940459\n",
      "    0.5958078 ]\n",
      "  [-0.10048048 -0.37517703  0.13669473 ...  1.2850523  -0.4668194\n",
      "   -0.04771185]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.5696806   0.15592188 -1.8755857  ... -0.74831355  0.4821821\n",
      "   -2.0617151 ]\n",
      "  [-1.2692462   2.4852538   1.7040881  ...  0.15999016  0.02318746\n",
      "   -0.03968561]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1166967   1.1320816  -1.2827475  ... -1.1440693  -0.1352872\n",
      "   -1.7897625 ]\n",
      "  [ 0.14278217  1.4589925   1.4032092  ... -0.629657   -0.4348175\n",
      "   -1.203126  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.7833173   1.5513369  -1.7205971  ... -0.41575134  0.20965111\n",
      "   -0.639687  ]\n",
      "  [-0.2649419   1.2610226   0.1105758  ... -0.13677824  1.9371525\n",
      "   -1.8796496 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.9179395   0.04034662 -2.1682615  ...  0.98202777 -1.3765444\n",
      "    0.17711604]\n",
      "  [ 1.0676936   0.45935142  0.85691345 ... -0.8009532   0.13953884\n",
      "    0.01967168]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.04226571  1.0897628  -2.2678118  ... -0.8362947   0.46657795\n",
      "   -1.0683794 ]\n",
      "  [ 0.09250441  1.2057211   0.8412549  ...  0.7564367  -0.49127674\n",
      "   -1.9270189 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.7201079   0.795623    0.01144743 ... -2.1893737   0.25850648\n",
      "   -1.4961123 ]\n",
      "  [-0.25712886  1.4821857   0.12122899 ...  0.7198851   0.496485\n",
      "   -0.4173506 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.6228998   1.201248   -0.30888402 ...  0.03840861  0.01829875\n",
      "   -0.02948296]\n",
      "  [-0.51868415  2.8402917   0.08179712 ... -0.29252934 -0.00875045\n",
      "   -1.1946889 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.36761135  1.755094   -2.059727   ... -0.87093174  0.01483399\n",
      "   -0.29477695]\n",
      "  [-1.0062157   1.443697    0.6261416  ...  1.0925263   0.37965226\n",
      "   -2.7518764 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.2376284   0.9306767  -1.4879154  ... -1.7824624  -1.5534227\n",
      "   -0.9610566 ]\n",
      "  [ 1.5291055   1.9864385   0.35148597 ...  1.07568    -0.46402913\n",
      "   -0.54864496]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.00595224  1.9542283  -2.1683009  ... -1.2545336   1.2336323\n",
      "   -1.568845  ]\n",
      "  [-0.17238608  1.7863796   1.2000974  ...  0.91314745  0.10557292\n",
      "   -1.0907927 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.16281968  1.16411    -1.7457137  ... -0.17175236  0.22115201\n",
      "   -0.7408011 ]\n",
      "  [ 0.03589983  1.2615383   2.64039    ...  0.56546533  1.3017095\n",
      "   -1.8437986 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.69227725  1.9854852  -1.4666436  ... -0.9879813  -0.4294243\n",
      "    0.9272189 ]\n",
      "  [ 1.5346445   1.1494303   1.6167127  ...  0.5437246  -1.0406836\n",
      "   -1.216463  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.0190176  -0.44878304 -1.1186869  ... -1.2510865  -0.36127236\n",
      "   -0.58012724]\n",
      "  [ 0.46929994  0.63545686  1.8848565  ...  0.6433748   1.137779\n",
      "   -0.8365674 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.6546054   0.66576827 -2.5182304  ... -1.6180317   0.9416329\n",
      "   -1.9709592 ]\n",
      "  [-1.5859948   0.46492153  1.1804284  ... -0.3975541   1.6114968\n",
      "   -0.55693245]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.8552166   1.4734704  -1.9173946  ... -1.3561288   0.9357665\n",
      "   -2.0839696 ]\n",
      "  [ 0.21155128  0.5440264   1.3272622  ...  0.04555821 -0.4312554\n",
      "   -1.6236515 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.3055837   1.5529969  -1.8686609  ... -0.60094523 -0.16813503\n",
      "   -0.1674636 ]\n",
      "  [ 0.41360083  0.8006533   0.91857004 ...  0.28886437  0.64771444\n",
      "   -0.473037  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.04236227  1.1157625  -1.7852294  ... -0.6256511  -0.63861674\n",
      "    0.43271136]\n",
      "  [ 0.14931197  0.55871105  1.5679083  ... -0.0998314   0.5900559\n",
      "   -0.8598535 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.6289022  -0.38471425 -1.5301713  ... -1.408839   -1.0716097\n",
      "    0.56614983]\n",
      "  [ 0.13667756  0.7062829  -0.30933988 ... -0.3580228  -0.03561336\n",
      "   -2.2203114 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.71884316  0.41828883 -1.2479453  ... -0.88910174 -1.0067537\n",
      "   -1.2383802 ]\n",
      "  [ 0.38631678  0.92141706  1.2161914  ...  0.7207135  -1.1729428\n",
      "   -0.6751375 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.63214     1.7885333  -2.7897565  ... -0.15969273 -0.6468934\n",
      "   -0.67855245]\n",
      "  [ 0.02215818  1.1801916   0.8472267  ... -0.7648136   0.4779756\n",
      "   -0.8944829 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.40206838  0.5940455  -2.0916493  ... -0.8048933  -0.5122776\n",
      "   -0.18122882]\n",
      "  [ 0.17348741  0.5117331   1.1704009  ... -0.52503735 -0.20542233\n",
      "   -1.8835261 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.80745745  1.9019818  -1.3898327  ...  0.21096429 -1.3983794\n",
      "   -2.3657506 ]\n",
      "  [ 0.84075224  2.0758767   1.6720924  ...  0.6884724   1.3108554\n",
      "   -0.49372685]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.8214931   1.7747126  -0.92118704 ... -0.39564276 -1.4677577\n",
      "   -0.9497871 ]\n",
      "  [-0.24221373  1.0288486  -0.07402682 ...  0.19208944  1.2299958\n",
      "   -1.17796   ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.9411794   0.46338052 -2.596344   ...  0.22651944 -0.8068285\n",
      "   -1.0192428 ]\n",
      "  [-1.2780931   1.9952242   1.9740306  ...  0.91938305  1.1633247\n",
      "   -1.1792327 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.9411794   0.46338052 -2.596344   ...  0.22651944 -0.8068285\n",
      "   -1.0192428 ]\n",
      "  [-1.2780931   1.9952242   1.9740306  ...  0.91938305  1.1633247\n",
      "   -1.1792327 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.23944497  1.6776043  -2.2313495  ... -0.653211   -0.88510144\n",
      "   -0.64972216]\n",
      "  [ 0.47411558  1.614202    1.5748451  ...  0.36228892  0.4383797\n",
      "   -2.3760996 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.45627886  1.4884645  -1.0003624  ... -0.572371   -0.13711864\n",
      "   -1.9908282 ]\n",
      "  [-0.01430534  0.21263433  2.063632   ...  1.4893222  -0.53963435\n",
      "   -1.0519968 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.2819594  -0.2853204  -1.0784163  ... -1.6747147   0.20427126\n",
      "   -0.96243227]\n",
      "  [ 1.2477338   1.0523498   0.5000715  ...  0.3378397   1.8413937\n",
      "   -2.3544838 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.7242656   0.5506051  -1.4699045  ... -0.39974025 -0.9985616\n",
      "   -1.021754  ]\n",
      "  [-1.0101224   2.35351     1.5444386  ... -0.30995917 -0.5965666\n",
      "   -1.0873308 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.5296357   0.6739199  -2.6429982  ... -0.31611055 -0.5168528\n",
      "   -1.945298  ]\n",
      "  [ 0.26348895  0.5492385   1.1402447  ...  0.74771214 -0.9983172\n",
      "    0.05410099]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.34040636  0.7223139  -1.645819   ... -0.954612   -0.05698571\n",
      "   -1.0265052 ]\n",
      "  [ 0.93488264  1.6097438   1.3911618  ... -0.5954514  -0.10180552\n",
      "   -0.9766748 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.55509883  0.5006708  -0.8777329  ... -0.31794745  0.2047944\n",
      "   -0.9311654 ]\n",
      "  [-0.6304356   2.3322158   1.9343833  ...  1.1005654   0.16909525\n",
      "    0.1735233 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.9908763   1.0247571  -3.0612442  ... -1.6997342  -1.1916478\n",
      "   -0.77492106]\n",
      "  [-0.6982074   0.33227968 -0.55670404 ...  0.37893307  0.2702751\n",
      "   -0.22825098]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.5097749   2.0326982  -2.1961207  ...  0.11183426  0.6436725\n",
      "   -1.891705  ]\n",
      "  [ 0.4546846   1.8666707   0.60431707 ...  0.17566219  1.1657679\n",
      "   -1.8853004 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.1141124   1.7585766  -1.8032187  ...  0.9845964  -0.3205548\n",
      "   -0.67324275]\n",
      "  [ 1.1390826   1.5354763   0.6703691  ...  1.1797161  -0.14940749\n",
      "   -1.3355846 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.362706    1.0439205  -0.47092748 ... -1.1201689   0.28303266\n",
      "   -0.08875489]\n",
      "  [ 0.841535    1.0595286   1.0827429  ...  0.55540496  0.7357464\n",
      "   -0.6684059 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.2014284   1.3547994  -0.3483007  ... -0.75853276 -1.2327805\n",
      "   -2.2643852 ]\n",
      "  [ 1.1517247   0.6807567   0.49433553 ...  0.58685434 -1.2679527\n",
      "   -1.4414612 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.1873164   0.5594995  -0.7884394  ... -1.3930011  -0.37585568\n",
      "   -0.6060088 ]\n",
      "  [ 1.5667285   2.1249733   0.93828773 ...  0.60598636  1.8869011\n",
      "   -2.2109094 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.712003   -0.06153846 -1.1540864  ... -2.0318203   0.978434\n",
      "    0.4145869 ]\n",
      "  [ 0.76622534  0.795663    2.0467303  ...  1.0962064   0.97038144\n",
      "   -0.6687515 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.2819594  -0.2853204  -1.0784163  ... -1.6747147   0.20427126\n",
      "   -0.96243227]\n",
      "  [-0.738389    1.7934808   2.338314   ...  0.30175346  1.2332093\n",
      "   -0.8811189 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.89179754  1.7626144  -1.5092169  ... -1.6269903  -1.3059485\n",
      "   -0.3164961 ]\n",
      "  [ 0.6177926   0.13991356  1.2874353  ...  0.42348024 -0.43121684\n",
      "   -0.8184685 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.5515571   1.3841505  -2.065411   ... -0.7941779  -0.6253767\n",
      "   -2.187746  ]\n",
      "  [ 0.6116613   0.7396544  -0.38492763 ...  1.2713153   1.307508\n",
      "   -1.7433028 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.78224427  1.0942742  -1.9129049  ... -0.03905886 -1.9805686\n",
      "   -1.7552316 ]\n",
      "  [-0.16453639  1.3633842   0.52545446 ...  0.9041815   0.96021354\n",
      "   -0.16334462]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.6718185   2.193747   -0.8114735  ... -0.59164023 -0.8666901\n",
      "   -0.5502302 ]\n",
      "  [-0.2658733   1.1984328   2.0100343  ...  0.98883545  0.37429562\n",
      "    0.19777977]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.5237026  -0.04019654 -1.4745862  ... -0.41239604 -0.28129542\n",
      "   -0.20263857]\n",
      "  [ 0.04937894  0.6436221   1.5755955  ... -0.92274374  0.88452077\n",
      "    0.3817469 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.8193265   0.98982    -2.2018433  ... -1.7721958  -1.6845794\n",
      "   -0.9189373 ]\n",
      "  [-1.219884    1.2141751   1.674361   ... -0.23232192  0.07035956\n",
      "   -0.28194827]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.0844616   1.9326258  -1.8192775  ... -0.83181286 -0.77592283\n",
      "   -0.11490792]\n",
      "  [ 0.05757476  0.47275943  0.5044638  ...  0.2212651   1.3090805\n",
      "   -1.7930083 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.41353709  0.72794116 -2.5118077  ...  0.5785612  -1.9835558\n",
      "   -1.1417767 ]\n",
      "  [-0.25179756 -0.03970158  0.1436212  ...  1.9810662   0.9698086\n",
      "   -1.9431747 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.913034    1.0031397  -0.7692314  ... -1.5896227  -0.09806705\n",
      "   -0.07665408]\n",
      "  [-1.3518544   1.5644343   1.89662    ...  0.27168924  0.13965145\n",
      "   -1.478875  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.5429379   1.6772318  -2.8950183  ...  1.2663261  -1.3347411\n",
      "   -1.4811983 ]\n",
      "  [ 0.6267444   1.3361778   1.9879816  ...  1.5809293   0.11443922\n",
      "   -1.407628  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.0397067   1.0997437  -1.2508664  ... -0.30108196  0.43379378\n",
      "   -0.09127575]\n",
      "  [-1.1246092   1.1385003   2.7136068  ... -0.46657944 -0.25956088\n",
      "   -2.1408641 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.0141783   1.6957362  -1.6110778  ... -1.4587731  -1.4620886\n",
      "   -0.46972033]\n",
      "  [ 1.0332161   0.52338046  1.755708   ... -0.16290241  0.68988603\n",
      "   -1.3070565 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.0821366   0.01554418 -2.2999253  ...  0.07792214  0.20598471\n",
      "   -1.3449866 ]\n",
      "  [ 1.1943228   2.3967352   0.7107054  ... -0.2820592   0.12311353\n",
      "   -0.40751612]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.7177689   1.1866255  -1.3470763  ... -0.70656073 -0.9057251\n",
      "    0.89250207]\n",
      "  [-0.7863122   1.218217    0.36377233 ...  1.7483683   1.6603391\n",
      "   -2.120698  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.6836393   1.3612347  -0.17077899 ... -0.9688221  -1.5572526\n",
      "    0.03397131]\n",
      "  [-0.71191156  1.40514    -0.27334988 ... -0.5805897   0.29297036\n",
      "   -1.2207177 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.5428221   0.45033103 -0.73786265 ... -0.61298347  0.23510706\n",
      "   -0.9332183 ]\n",
      "  [-0.21961944  0.07983816 -0.06172085 ...  1.5840218   1.0049145\n",
      "   -2.1271932 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.1264435   2.7481222   0.08668101 ... -1.3455135  -1.3291365\n",
      "   -0.35951552]\n",
      "  [ 0.11223914  1.6516798   1.4603916  ...  0.19285831  0.30160537\n",
      "   -1.6129019 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.5097749   2.0326982  -2.1961207  ...  0.11183426  0.6436725\n",
      "   -1.891705  ]\n",
      "  [-0.3130621   1.0456146   0.45463568 ...  0.66744816  0.28650284\n",
      "   -0.28083372]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.212322    0.92128557 -2.660593   ... -0.59596735 -0.7942748\n",
      "   -0.9254756 ]\n",
      "  [ 0.77726257  2.0770211  -0.59991384 ...  0.28493673  0.45226264\n",
      "   -1.2485194 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.35928255  1.4174321  -0.45944798 ... -0.18500721  1.262367\n",
      "   -1.863315  ]\n",
      "  [ 0.1337137   1.2034756  -0.29330242 ...  0.55503696 -0.52141833\n",
      "   -0.20558745]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.7490966  -0.3336177  -0.36059296 ... -0.1756185  -0.71567476\n",
      "   -1.5163274 ]\n",
      "  [ 0.40410584  0.3074997   1.5892836  ...  0.28636056 -0.0290184\n",
      "   -0.32307303]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.6597279  -0.30531025 -1.6403074  ... -1.0966307  -0.08596849\n",
      "    0.550462  ]\n",
      "  [ 0.09307817  1.5905746   1.5841551  ... -0.06454396  0.8866067\n",
      "   -2.4499698 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.6493286   0.6635052  -2.0756238  ... -1.2441598   0.16952664\n",
      "   -0.28622752]\n",
      "  [-0.19474506  1.2508441   1.5291973  ...  0.8778765   0.31419092\n",
      "   -1.556049  ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.076313    1.4490285  -2.6023421  ... -1.0605006  -0.66271806\n",
      "   -0.9577645 ]\n",
      "  [ 0.41503727  1.1584023   0.48446524 ... -0.07543373  0.9677992\n",
      "   -0.55242777]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.5807848   2.1748102  -1.7286481  ...  0.05253199 -0.84616536\n",
      "   -0.7787209 ]\n",
      "  [-0.4628193   0.23914522  0.03698397 ... -0.25110006  1.37388\n",
      "    0.03396392]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.3421196   0.24566317 -0.25987446 ... -1.9683893  -1.635658\n",
      "   -0.42592403]\n",
      "  [ 0.04257978  1.1904218   0.17776579 ... -0.6500053   0.59786576\n",
      "   -0.03534639]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.4709342   2.811421   -1.9611722  ... -0.0811561  -1.56658\n",
      "   -1.2661469 ]\n",
      "  [-0.14017972  2.038002    1.2494466  ...  0.31982523  0.9561532\n",
      "   -0.16627693]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.45733654  1.9009659  -1.9167721  ... -0.02753392  1.0513384\n",
      "   -1.9367844 ]\n",
      "  [-0.61345756  1.0830277   2.2220707  ...  1.21194     0.99794585\n",
      "   -0.97548366]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.97205395  0.05277038 -2.6454225  ... -1.7660002  -0.33614677\n",
      "   -0.37961954]\n",
      "  [-0.09409375  0.574238    1.6272593  ...  1.5445607   1.278553\n",
      "   -0.9036218 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-2.212322    0.92128557 -2.660593   ... -0.59596735 -0.7942748\n",
      "   -0.9254756 ]\n",
      "  [-0.97259843  0.62408924  1.0825922  ... -0.39584213 -0.6994197\n",
      "   -0.7333442 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.03355688  1.4079086  -2.2851176  ... -1.4770699  -0.20867832\n",
      "    0.6034633 ]\n",
      "  [-1.1051141   1.9074137   0.534825   ...  0.08599398 -0.75893474\n",
      "   -2.6127887 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.39709204 -0.48048186 -2.797773   ... -0.1959166  -1.268215\n",
      "   -1.6934137 ]\n",
      "  [ 0.79660666  1.4910395   0.7468635  ...  1.1227634   1.0144224\n",
      "   -1.4552583 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.50635374  1.71583    -0.54859006 ... -0.48382685  0.81256634\n",
      "   -0.4953967 ]\n",
      "  [ 0.93021333  2.3926299   1.7309332  ...  0.47198084 -0.11961143\n",
      "    0.38676655]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.9441652   2.6768057  -0.85626215 ...  0.50262094  0.54432815\n",
      "   -0.9868743 ]\n",
      "  [-0.6975249   0.05903602  1.2355036  ...  0.65480816  1.8386822\n",
      "   -0.5435461 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [ 0.02719563  1.5256965  -0.67582774 ...  0.8162757  -0.9673364\n",
      "   -0.68992335]\n",
      "  [ 0.13640437  1.9168973   2.604076   ...  1.1087298   1.1187457\n",
      "   -0.34438998]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.0767085   2.221941   -2.132567   ...  0.03442433 -0.5213206\n",
      "   -1.3262212 ]\n",
      "  [-0.8529228   1.6770744   0.85461026 ...  0.51890856  0.78514946\n",
      "   -1.0249584 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-1.2159315   1.7556415  -1.8463274  ... -1.2002131   0.5631314\n",
      "   -0.42657274]\n",
      "  [ 0.11839642  1.4743845   1.4717317  ... -0.50327027 -0.3231231\n",
      "   -0.42158085]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.30166698  1.0340217  -2.0976253  ...  0.41461226  0.12287587\n",
      "   -1.2471526 ]\n",
      "  [-0.06319826  0.92077935  0.7513152  ...  1.374084    1.1686678\n",
      "   -2.04256   ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n",
      "Before attention blocks: [[[-0.2133187  -0.37896746  0.9894879  ... -1.4485674   0.29604977\n",
      "   -0.8132925 ]\n",
      "  [-0.59830236  0.60691214 -2.4852147  ... -2.1042798   0.76493067\n",
      "   -0.9431468 ]\n",
      "  [-0.17704654  1.235351   -0.0950973  ... -0.9161504   1.0547316\n",
      "   -0.7396488 ]\n",
      "  ...\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]\n",
      "  [ 1.7484777  -0.23501392  1.5967557  ...  1.4864607  -1.9107313\n",
      "   -0.23773763]]]\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "import shutil\n",
    "\n",
    "path_to_set=\"my_hf_datasets/20240805/noncoding_embeddings_nt\"\n",
    "# Remove the dataset directory and its contents\n",
    "try:\n",
    "    shutil.rmtree(path_to_set)\n",
    "    print(f\"Dataset at {path_to_set} has been removed.\")\n",
    "except Exception:\n",
    "    # Silently catch the exception and do nothing\n",
    "    pass\n",
    "    \n",
    "# df_top = df.head(35)\n",
    "output2HFset(df, path_to_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "de9f8387-895d-4fc9-92c8-4959034c95f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the dataset: 100\n",
      "             0           1             2          3           4           5  \\\n",
      "0    0.3425328  15.1762085      3.742885   10.84721  -13.183308   5.8762307   \n",
      "1    2.2413845    16.28364     1.1982976   13.38946   -8.391577   3.2226942   \n",
      "2  0.007799506   15.692189  -0.080317944  12.313036   -9.821446    4.898007   \n",
      "3   -3.9048407   18.416681     2.7687669  12.427805  -7.1248636   3.6083035   \n",
      "4   -3.9048407   18.416681     2.7687669  12.427805  -7.1248636   3.6083035   \n",
      "5    -3.960939   15.503376     3.5796745  10.260619  -11.944419   4.7667713   \n",
      "6   -1.0721788   14.264588     1.6912824   6.378506  -9.9909525   5.1121697   \n",
      "7  -0.88614994   16.601736     3.6419735  8.6276045   -9.367528   5.9740396   \n",
      "8   -0.8275161   15.347647  -0.032351688   9.915802  -10.042259  0.66772527   \n",
      "9     4.567762   14.807265      2.861988   9.738922   -9.466186   5.9770746   \n",
      "\n",
      "            6          7          8          9  ...        1275        1276  \\\n",
      "0  -7.8857293  11.040478  5.2128897  36.853607  ...   -6.409525  -23.164848   \n",
      "1  -4.6560173  10.153633   4.811969   40.39728  ...   -3.107812  -20.822523   \n",
      "2    -8.20082   9.963928   2.859816  41.271027  ...  -6.3865585  -17.312077   \n",
      "3  -7.6756334   9.596698   7.528942   39.63939  ...   -2.688621  -18.135508   \n",
      "4  -7.6756334   9.596698   7.528942   39.63939  ...   -2.688621  -18.135508   \n",
      "5  -9.5531435  11.441084   3.390839  40.660202  ...  -5.5787992  -17.596203   \n",
      "6  -6.2480664  12.009638  3.1032872   39.98763  ...  -5.2258377  -23.314867   \n",
      "7   -3.174615  12.736058   6.082915   40.57684  ...   -6.542309  -24.137232   \n",
      "8   -4.508032  10.297563  5.5657988  37.600464  ...  -6.5931125  -20.680748   \n",
      "9    -9.49132  12.366289  4.7722044  38.815235  ...   -5.005286  -19.742197   \n",
      "\n",
      "          1277        1278        1279 CHROM REF ALT SIZE  y  \n",
      "0   -13.073088    -4.50166  -1.4731869     1   C   T  128  0  \n",
      "1    -16.94583  -0.9103235   2.2261982     1   C   T  128  0  \n",
      "2   -13.130859  -4.4665117   2.4564865     1   C   G  128  0  \n",
      "3   -10.937962  -6.7324505   4.1582246     1   C   T  128  0  \n",
      "4   -10.937962  -6.7324505   4.1582246     1   C   G  128  0  \n",
      "5   -16.047039   -5.659486    1.787206     1   G   T  128  0  \n",
      "6    -11.55464  -3.3438392    1.685201     1   C   T  128  0  \n",
      "7    -8.772387  -1.5766748  0.19068432     1   C   T  128  0  \n",
      "8    -7.614282  -4.7031803   2.2844536     1   A   G  128  0  \n",
      "9  -12.8281355  -3.2785966  0.23609209     1   G   T  128  0  \n",
      "\n",
      "[10 rows x 1285 columns]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset from disk\n",
    "path_to_set=\"my_hf_datasets/20240805/noncoding_embeddings_nt\"\n",
    "\n",
    "\n",
    "hf_dataset = load_from_disk(path_to_set)\n",
    "\n",
    "num_rows = len(hf_dataset)\n",
    "\n",
    "\n",
    "print(f\"Number of rows in the dataset: {num_rows}\")\n",
    "\n",
    "# Convert the first 10 rows to a pandas DataFrame\n",
    "df_top_10 = pd.DataFrame(hf_dataset[:10])\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df_top_10)\n",
    "\n",
    "# Convert the Hugging Face dataset to a pandas DataFrame\n",
    "# df_20 = hf_dataset.to_pandas()\n",
    "# df_20\n",
    "# df_top=df.head(2)\n",
    "# df_top\n",
    "# df_20.iloc[0]['embeddings']\n",
    "# df_20['embeddings'][0]\n",
    "# print(df_20.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c49569f-f31f-4d41-9409-6d61942dc5bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa35ecb-9691-416c-8b87-829b3abe9c03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0e6596-a93b-40a0-bdfe-74d2975268e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(tokenizer._tokens_to_ids.keys())\n",
    "df[df['START']==93500052]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a203ca45-7b65-42c4-8e0b-ba172e255aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_file(csv_filename):\n",
    "\n",
    "    df=pd.read_csv(csv_filename)\n",
    "    print(df.shape)\n",
    "    # new_arr = data_array[:, :-4]\n",
    "    # last_column = data_array[:, -1]\n",
    "    # data_array = np.column_stack((new_arr, last_column))\n",
    "    \n",
    "    column_names = [f'{i}' for i in range(0, df.shape[1]-6)]\n",
    "    column_names.extend(['CHROM', 'START', 'SIZE', 'REF', 'ALT',  'y'])\n",
    "    \n",
    "    df.columns = column_names\n",
    "    return df\n",
    "\n",
    "df = load_embedding_file(csv_Filename)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b655bb-9f26-488b-af6c-336a964924d6",
   "metadata": {},
   "source": [
    "### Load CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a58831-49e1-4ff6-8bd1-1e3204ee49d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_file(csv_filename):\n",
    "\n",
    "    df=pd.read_csv(csv_filename)\n",
    "    print(df.shape)\n",
    "    # new_arr = data_array[:, :-4]\n",
    "    # last_column = data_array[:, -1]\n",
    "    # data_array = np.column_stack((new_arr, last_column))\n",
    "    \n",
    "    column_names = [f'{i}' for i in range(0, df.shape[1]-6)]\n",
    "    column_names.extend(['CHROM', 'START', 'SIZE', 'REF', 'ALT',  'y'])\n",
    "    \n",
    "    df.columns = column_names\n",
    "    return df\n",
    "\n",
    "df = load_embedding_file('./Homo_sapiens_embeddings_nt.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda08e0a-bae2-4ffb-b656-914f152633ea",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7587f30-df2f-48cc-b1e2-52d7011b902f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df1 = load_embedding_file('./gnomad.v4.1.exon.nt.csv')\n",
    "# df1['y']=0\n",
    "\n",
    "# df2 = load_embedding_file('./gnomad.v4.1.intergenic.nt.csv')\n",
    "# df2['y']=1\n",
    "\n",
    "# df3 = load_embedding_file('./gnomad.v4.1.proximity.nt.csv')\n",
    "# df3['y']=2\n",
    "\n",
    "\n",
    "# df = pd.concat([df1, df2,df3], ignore_index=True)\n",
    "# df.to_csv('gnomad.v4.1.nt.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf230c99-af62-4f55-9b18-236cd66b3edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = load_embedding_file('./gnomad.v4.1.nt.csv')\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33192457-f156-4565-8bad-b7869f895c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df[df['POS']==843234]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09805401-e1e4-4bfe-8fc3-d44ead3af462",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_embedding_file('./Homo_sapiens_embeddings.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60590485-8c48-460a-9fc5-e3e1f84ebb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "file_path = 'clinvar_noncoding_embeddings_nt.csv'\n",
    "\n",
    "# Get the file size in bytes\n",
    "file_size = os.path.getsize(file_path)\n",
    "file_size_kb = file_size / 1024  # size in KB\n",
    "file_size_mb = file_size_kb / 1024  # size in MB\n",
    "\n",
    "# Print the file size in a readable format (optional)\n",
    "print(f\"File size: {file_size} bytes\")\n",
    "print(f\"File size: {file_size_kb:.2f} KB\")\n",
    "print(f\"File size: {file_size_mb:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731d4604-8376-474c-ba67-9c88cf367039",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mport os\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "file_path = 'clinvar_noncoding_embeddings_nt.csv'\n",
    "\n",
    "# Get the file size in bytes\n",
    "file_size = os.path.getsize(file_path)\n",
    "file_size_kb = file_size / 1024  # size in KB\n",
    "file_size_mb = file_size_kb / 1024  # size in MB\n",
    "\n",
    "# Print the file size in a readable format (optional)\n",
    "print(f\"File size: {file_size} bytes\")\n",
    "print(f\"File size: {file_size_kb:.2f} KB\")\n",
    "print(f\"File size: {file_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43360bf1-f0bf-4fc1-99f7-5ae87dad12d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Specify the path to your CSV file\n",
    "file_path = 'clinvar_noncoding_embeddings_nt.csv'\n",
    "\n",
    "# Open the file and count the rows\n",
    "with open(file_path, 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    row_count = sum(1 for row in reader)\n",
    "\n",
    "# Print the number of rows\n",
    "print(f\"Number of rows: {row_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68047b69-e12c-4b85-93d1-410e66afd0cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a53674-f9cf-491d-901d-e59c3cbc9a53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "794da137-a313-47de-a263-710a4afc3e8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6e955c-35ba-4513-8807-fa23ba172d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from nucleotide_transformer.pretrained import get_pretrained_model\n",
    "\n",
    "# Get pretrained model\n",
    "parameters, forward_fn, tokenizer, config = get_pretrained_model(\n",
    "    model_name=\"500M_human_ref\",\n",
    "    embeddings_layers_to_save=(20,),\n",
    "    max_positions=32,\n",
    ")\n",
    "forward_fn = hk.transform(forward_fn)\n",
    "\n",
    "# Get data and tokenize it\n",
    "sequences = [\"ATTCCGATTCCGATTCCG\", \"ATTTCTCTCTCTCTCTGAGATCGATCGATCGAT\"]\n",
    "sequences = [\"ATTTCTCTCTCTCTCTGAGATCGATCGATCGAT\"]\n",
    "tokens_ids = [b[1] for b in tokenizer.batch_tokenize(sequences)]\n",
    "tokens_str = [b[0] for b in tokenizer.batch_tokenize(sequences)]\n",
    "tokens = jnp.asarray(tokens_ids, dtype=jnp.int32)\n",
    "\n",
    "# Initialize random key\n",
    "random_key = jax.random.PRNGKey(0)\n",
    "\n",
    "# Infer\n",
    "outs = forward_fn.apply(parameters, random_key, tokens)\n",
    "\n",
    "# Get embeddings at layer 20\n",
    "# print(outs[\"embeddings_20\"].shape)\n",
    "\n",
    "my_embedding=outs[\"embeddings_20\"][:,0,:]\n",
    "    \n",
    "print(\"my_embedding shape=\",my_embedding.shape)\n",
    "column_names = [f'{i}' for i in range(0, my_embedding.shape[1])]\n",
    "embedding_df = pd.DataFrame(my_embedding, columns=column_names)\n",
    "embedding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c9b432-00fc-4af1-835b-393d8cb52d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_key = jax.random.PRNGKey(0)\n",
    "def get_embeddings(tokens):\n",
    "\n",
    "    # Initialize random key\n",
    "    \n",
    "    # Infer\n",
    "    outs = forward_fn.apply(parameters, random_key, tokens)    \n",
    "    # print(outs.keys())\n",
    "    # print(outs[\"embeddings_20\"].shape)\n",
    "    # outs[\"embeddings_20\"]\n",
    "    \n",
    "    # my_embedding=outs[\"embeddings_20\"][:,16,:]\n",
    "    my_embedding=outs[\"embeddings_20\"][:,0,:]\n",
    "    \n",
    "    my_embedding.shape\n",
    "    column_names = [f'{i}' for i in range(0, my_embedding.shape[1])]\n",
    "    embedding_df = pd.DataFrame(my_embedding, columns=column_names)\n",
    "    return embedding_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f9425149-b3c3-4810-9007-a64f1fa25bab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.82661685, 0.013857  , 0.05041127, 0.16914426, 0.27685434,\n",
       "        0.48058288, 0.76627826, 0.43385749, 0.41895577, 0.97018492]),\n",
       " array([0.68989487, 0.99791568, 0.68382645, 0.59061445, 0.97438343,\n",
       "        0.93641663, 0.36037968, 0.48704185, 0.35121413, 0.48548722]),\n",
       " array([0.13053355, 0.74875501, 0.77265241, 0.93544951, 0.48985972,\n",
       "        0.43907184, 0.02972745, 0.3836498 , 0.53054333, 0.88076955]),\n",
       " array([0.75930673, 0.11030291, 0.627487  , 0.3309788 , 0.32840613,\n",
       "        0.75120948, 0.59324971, 0.52827394, 0.66185284, 0.71089183]),\n",
       " array([0.73636719, 0.28920172, 0.35876011, 0.6249102 , 0.93867097,\n",
       "        0.80249523, 0.98652413, 0.63058877, 0.72700444, 0.3312359 ])]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Example list of embeddings (ensure these are numpy arrays)\n",
    "embeddings = [np.random.rand(10) for _ in range(5)]  # List of 5 numpy arrays of size 10\n",
    "embeddings\n",
    "# len(embeddings)\n",
    "\n",
    "\n",
    "# # Create a DataFrame\n",
    "# df = pd.DataFrame()\n",
    "\n",
    "# # Assign the embeddings directly to the DataFrame\n",
    "# df['embedding'] = embeddings  # This will keep them as numpy arrays\n",
    "\n",
    "# df.to_csv('demo.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-2.2.0",
   "language": "python",
   "name": "pytorch-2.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0d11a89-1cc9-4384-ac3c-ccc24a66f7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install genomic_benchmarks\n",
    "# !pip install omegaconf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94159a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/local/49146768/ipykernel_2607204/1527746069.py:5: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n",
      "/home/sunhuaikuan/.local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# using Kernal PyTorch-2.0.1\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from transformers import AutoModel #, AutoModelForMaskedLM\n",
    "\n",
    "# import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "# import pickle\n",
    "# import re\n",
    "import os\n",
    "import csv\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "%run hyena_utility.py\n",
    "\n",
    "%run preprocess_utility.py\n",
    "\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0495a5df-3a69-4196-b12d-ab504d00f4d4",
   "metadata": {},
   "source": [
    "### Load Human Chrom Sequences from .fa File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5760522-3c27-4a0c-9fe0-209a212edc62",
   "metadata": {},
   "outputs": [],
   "source": [
    "fasta_file = \"../genome.hg38rg.fa\"\n",
    "chrom_sequences = read_fasta(fasta_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89af52c7-67c6-4427-b9d4-6f2fe12a34ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Subsequence2Embedding(subsequence):\n",
    "    tok_seq = tokenizer(subsequence)\n",
    "    tok_seq = tok_seq[\"input_ids\"]  \n",
    "\n",
    "    # place on device, convert to tensor\n",
    "    tok_seq = torch.LongTensor(tok_seq).unsqueeze(0)  # unsqueeze for batch dim\n",
    "    tok_seq = tok_seq.to(device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        embeddings = model(tok_seq)\n",
    "\n",
    "    mean_embeddings = embeddings.mean(dim=1) # Mean across the sequence length dimension\n",
    "    mean_embeddings = mean_embeddings.squeeze(0)  # This will change the shape to [256]\n",
    "    \n",
    "    return mean_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bc056b3-d504-495e-852e-2851ad0d2bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_rows_to_csv(csv_Filename, rows):\n",
    "    with open(csv_Filename, mode='a', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        for row in rows:\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c1ea2f-6b84-4308-a4d0-4e018b1d446f",
   "metadata": {},
   "source": [
    "### Main Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d899db1-344c-4812-8392-248e309c12df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Git LFS initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'hyenadna-medium-160k-seqlen'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights ok!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HyenaDNAModel(\n",
       "  (backbone): LMBackbone(\n",
       "    (embeddings): GPT2Embeddings(\n",
       "      (word_embeddings): Embedding(16, 256)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0): Block(\n",
       "        (mixer): HyenaOperator(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (in_proj): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (short_filter): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(2,), groups=768)\n",
       "          (filter_fn): HyenaFilter(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb): PositionalEmbedding()\n",
       "            (implicit_filter): Sequential(\n",
       "              (0): Linear(in_features=5, out_features=64, bias=True)\n",
       "              (1): Sin()\n",
       "              (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (3): Sin()\n",
       "              (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (5): Sin()\n",
       "              (6): Linear(in_features=64, out_features=256, bias=False)\n",
       "            )\n",
       "            (modulation): ExponentialModulation()\n",
       "          )\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.1, inplace=False)\n",
       "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (1-7): 7 x Block(\n",
       "        (mixer): HyenaOperator(\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (in_proj): Linear(in_features=256, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "          (short_filter): Conv1d(768, 768, kernel_size=(3,), stride=(1,), padding=(2,), groups=768)\n",
       "          (filter_fn): HyenaFilter(\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "            (pos_emb): PositionalEmbedding()\n",
       "            (implicit_filter): Sequential(\n",
       "              (0): Linear(in_features=5, out_features=64, bias=True)\n",
       "              (1): Sin()\n",
       "              (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (3): Sin()\n",
       "              (4): Linear(in_features=64, out_features=64, bias=True)\n",
       "              (5): Sin()\n",
       "              (6): Linear(in_features=64, out_features=256, bias=False)\n",
       "            )\n",
       "            (modulation): ExponentialModulation()\n",
       "          )\n",
       "        )\n",
       "        (dropout1): Dropout(p=0.0, inplace=False)\n",
       "        (drop_path1): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): Mlp(\n",
       "          (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "          (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "        )\n",
       "        (dropout2): Dropout(p=0.0, inplace=False)\n",
       "        (drop_path2): StochasticDepth(p=0.0, mode=row)\n",
       "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (drop_f): Dropout(p=0.0, inplace=False)\n",
       "    (ln_f): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_model_name = 'hyenadna-medium-160k-seqlen'\n",
    "model, tokenizer, max_length =  get_model_tokenizer_maxlen(pretrained_model_name)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6553147-bfc8-48e8-9045-604061d2b52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subsequence(chrom_name, start_pos, length):\n",
    "    \n",
    "    if chrom_name in chrom_sequences:\n",
    "        sequence = chrom_sequences[chrom_name]\n",
    "        subsequence = sequence[start_pos:start_pos + length]\n",
    "        return subsequence\n",
    "    else:\n",
    "        raise ValueError(f\"Chromosome '{chrom_name}' not found in the FASTA file.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a73b5a5-247c-4cc8-8dc2-bc2d44e7f9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index = 2000 completed\n",
      "index = 4000 completed\n",
      "index = 6000 completed\n",
      "index = 8000 completed\n",
      "index = 10000 completed\n",
      "index = 12000 completed\n",
      "index = 14000 completed\n",
      "index = 16000 completed\n",
      "index = 18000 completed\n",
      "index = 20000 completed\n",
      "index = 22000 completed\n",
      "index = 24000 completed\n",
      "index = 26000 completed\n",
      "index = 28000 completed\n",
      "index = 30000 completed\n",
      "index = 32000 completed\n",
      "Create File: ./homo_sapiens_hyena_embedding.csv\n",
      "CPU times: user 4min 23s, sys: 742 ms, total: 4min 23s\n",
      "Wall time: 4min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "comp = {'A':1, 'C':2, 'G':3, 'T':4}\n",
    "\n",
    "max_length = 128\n",
    "\n",
    "csv_Filename = './homo_sapiens_hyena_embedding.csv'\n",
    "if os.path.exists(csv_Filename):\n",
    "    os.remove(csv_Filename)\n",
    "\n",
    "datafile_path = '../../datasets/task03-genomic-regions/Homo_sapiens.GRCh38.109.txt.gz'  \n",
    "df = preprocess_home_sapiens_datafile(datafile_path)\n",
    "\n",
    "\n",
    "df.loc[df['SIZE'] > max_length, 'END'] = df['START'] + max_length\n",
    "df.loc[df['SIZE'] > max_length, 'SIZE'] = max_length\n",
    "df.drop(columns=['END','TYPE','CLUSTER'], inplace=True)\n",
    "# df\n",
    "\n",
    "\n",
    "rows=[]\n",
    "for index, row in df.iterrows():      \n",
    "    chrom=row['CHROM']\n",
    "    rowid=row['ROWID']\n",
    "    pos_start=row['START']\n",
    "\n",
    "    if pos_start<=1:\n",
    "        pos_start=1\n",
    "    y=row['y']\n",
    "    length = row['SIZE'] # max_length\n",
    "    \n",
    "    subsequence = get_subsequence(chrom, pos_start, length)\n",
    "    if 'N' in subsequence:\n",
    "        print(\"The character 'N' is present in the string.\")\n",
    "        \n",
    "    embedding = Subsequence2Embedding(subsequence)\n",
    "    # print (embedding)\n",
    "\n",
    "    # feature=np.array(embedding_df.iloc[64])\n",
    "    rows.append(np.append(embedding.cpu().numpy(),  [rowid, y]))  #chrom, length,  \n",
    "\n",
    "\n",
    "    if index > 0 and (index % 2000) == 0:\n",
    "        append_rows_to_csv(csv_Filename, rows)\n",
    "        rows=[]\n",
    "        print (f\"index = {index} completed\")\n",
    "        \n",
    "append_rows_to_csv(csv_Filename, rows)\n",
    "\n",
    "print(f\"Create File: \"+csv_Filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93f1c14-0ef8-4efa-84df-c94596ac0b59",
   "metadata": {},
   "source": [
    "### Load CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90d6b579-726c-438b-9c2b-1c91b2e81b6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>...</th>\n",
       "      <th>249</th>\n",
       "      <th>250</th>\n",
       "      <th>251</th>\n",
       "      <th>252</th>\n",
       "      <th>253</th>\n",
       "      <th>254</th>\n",
       "      <th>255</th>\n",
       "      <th>256</th>\n",
       "      <th>ROWID</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.098259</td>\n",
       "      <td>-0.778371</td>\n",
       "      <td>-0.567632</td>\n",
       "      <td>-0.864359</td>\n",
       "      <td>-0.912805</td>\n",
       "      <td>-0.907104</td>\n",
       "      <td>1.634788</td>\n",
       "      <td>-0.941197</td>\n",
       "      <td>0.169670</td>\n",
       "      <td>0.114994</td>\n",
       "      <td>...</td>\n",
       "      <td>0.455128</td>\n",
       "      <td>-0.650662</td>\n",
       "      <td>-1.605736</td>\n",
       "      <td>-1.492585</td>\n",
       "      <td>1.182727</td>\n",
       "      <td>0.519747</td>\n",
       "      <td>0.673476</td>\n",
       "      <td>-1.080819</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.225349</td>\n",
       "      <td>-0.772242</td>\n",
       "      <td>-0.580955</td>\n",
       "      <td>-1.056253</td>\n",
       "      <td>-1.319412</td>\n",
       "      <td>-0.937095</td>\n",
       "      <td>1.204180</td>\n",
       "      <td>-0.703993</td>\n",
       "      <td>-0.227651</td>\n",
       "      <td>0.182004</td>\n",
       "      <td>...</td>\n",
       "      <td>0.496497</td>\n",
       "      <td>-1.000214</td>\n",
       "      <td>-1.584138</td>\n",
       "      <td>-1.652526</td>\n",
       "      <td>1.257367</td>\n",
       "      <td>0.468274</td>\n",
       "      <td>0.278431</td>\n",
       "      <td>-0.962163</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000942</td>\n",
       "      <td>-0.598621</td>\n",
       "      <td>-0.672390</td>\n",
       "      <td>-0.641442</td>\n",
       "      <td>-1.040335</td>\n",
       "      <td>-0.755668</td>\n",
       "      <td>1.274980</td>\n",
       "      <td>-0.978171</td>\n",
       "      <td>-0.042830</td>\n",
       "      <td>0.124320</td>\n",
       "      <td>...</td>\n",
       "      <td>0.494047</td>\n",
       "      <td>-0.849385</td>\n",
       "      <td>-1.569252</td>\n",
       "      <td>-1.409305</td>\n",
       "      <td>1.183825</td>\n",
       "      <td>0.358066</td>\n",
       "      <td>0.373570</td>\n",
       "      <td>-1.182730</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.120756</td>\n",
       "      <td>-0.799536</td>\n",
       "      <td>-0.620240</td>\n",
       "      <td>-0.805628</td>\n",
       "      <td>-1.006669</td>\n",
       "      <td>-0.907013</td>\n",
       "      <td>1.548830</td>\n",
       "      <td>-0.936683</td>\n",
       "      <td>0.091012</td>\n",
       "      <td>0.218409</td>\n",
       "      <td>...</td>\n",
       "      <td>0.447258</td>\n",
       "      <td>-0.846772</td>\n",
       "      <td>-1.625529</td>\n",
       "      <td>-1.477122</td>\n",
       "      <td>1.251898</td>\n",
       "      <td>0.502077</td>\n",
       "      <td>0.407137</td>\n",
       "      <td>-1.082190</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.087019</td>\n",
       "      <td>-0.803882</td>\n",
       "      <td>-0.531570</td>\n",
       "      <td>-0.888757</td>\n",
       "      <td>-1.351001</td>\n",
       "      <td>-0.758447</td>\n",
       "      <td>1.058772</td>\n",
       "      <td>-0.702686</td>\n",
       "      <td>-0.500163</td>\n",
       "      <td>0.162552</td>\n",
       "      <td>...</td>\n",
       "      <td>0.480105</td>\n",
       "      <td>-1.057028</td>\n",
       "      <td>-1.528152</td>\n",
       "      <td>-1.568092</td>\n",
       "      <td>1.297123</td>\n",
       "      <td>0.465277</td>\n",
       "      <td>0.306736</td>\n",
       "      <td>-0.968242</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33502</th>\n",
       "      <td>-0.144893</td>\n",
       "      <td>-0.688863</td>\n",
       "      <td>-0.502946</td>\n",
       "      <td>-0.884095</td>\n",
       "      <td>-0.949489</td>\n",
       "      <td>-0.980342</td>\n",
       "      <td>1.229003</td>\n",
       "      <td>-0.794743</td>\n",
       "      <td>-0.065076</td>\n",
       "      <td>0.398823</td>\n",
       "      <td>...</td>\n",
       "      <td>0.456091</td>\n",
       "      <td>-0.921927</td>\n",
       "      <td>-1.539195</td>\n",
       "      <td>-1.564770</td>\n",
       "      <td>1.214185</td>\n",
       "      <td>0.504790</td>\n",
       "      <td>0.358191</td>\n",
       "      <td>-0.886767</td>\n",
       "      <td>33713.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33503</th>\n",
       "      <td>0.148355</td>\n",
       "      <td>-0.644058</td>\n",
       "      <td>-0.473307</td>\n",
       "      <td>-0.542517</td>\n",
       "      <td>-0.696149</td>\n",
       "      <td>-0.696782</td>\n",
       "      <td>1.290697</td>\n",
       "      <td>-0.928129</td>\n",
       "      <td>-0.102882</td>\n",
       "      <td>0.279975</td>\n",
       "      <td>...</td>\n",
       "      <td>0.486481</td>\n",
       "      <td>-0.966628</td>\n",
       "      <td>-1.423398</td>\n",
       "      <td>-1.251273</td>\n",
       "      <td>1.157585</td>\n",
       "      <td>0.344522</td>\n",
       "      <td>0.594526</td>\n",
       "      <td>-1.020150</td>\n",
       "      <td>33714.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33504</th>\n",
       "      <td>-0.148756</td>\n",
       "      <td>-0.699471</td>\n",
       "      <td>-0.448784</td>\n",
       "      <td>-0.855049</td>\n",
       "      <td>-0.818939</td>\n",
       "      <td>-0.974509</td>\n",
       "      <td>1.198471</td>\n",
       "      <td>-0.822832</td>\n",
       "      <td>0.027991</td>\n",
       "      <td>0.465258</td>\n",
       "      <td>...</td>\n",
       "      <td>0.452030</td>\n",
       "      <td>-0.963613</td>\n",
       "      <td>-1.493534</td>\n",
       "      <td>-1.490965</td>\n",
       "      <td>1.182805</td>\n",
       "      <td>0.537592</td>\n",
       "      <td>0.380066</td>\n",
       "      <td>-0.932183</td>\n",
       "      <td>33715.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33505</th>\n",
       "      <td>-0.018792</td>\n",
       "      <td>-0.577633</td>\n",
       "      <td>-0.492049</td>\n",
       "      <td>-0.767255</td>\n",
       "      <td>-0.861196</td>\n",
       "      <td>-0.903652</td>\n",
       "      <td>1.238136</td>\n",
       "      <td>-0.859306</td>\n",
       "      <td>0.040800</td>\n",
       "      <td>0.341311</td>\n",
       "      <td>...</td>\n",
       "      <td>0.494729</td>\n",
       "      <td>-0.926624</td>\n",
       "      <td>-1.494075</td>\n",
       "      <td>-1.484290</td>\n",
       "      <td>1.192897</td>\n",
       "      <td>0.479127</td>\n",
       "      <td>0.373609</td>\n",
       "      <td>-0.984641</td>\n",
       "      <td>33716.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33506</th>\n",
       "      <td>-0.000482</td>\n",
       "      <td>-0.664418</td>\n",
       "      <td>-0.515143</td>\n",
       "      <td>-0.587033</td>\n",
       "      <td>-1.007960</td>\n",
       "      <td>-0.820357</td>\n",
       "      <td>1.285978</td>\n",
       "      <td>-0.938048</td>\n",
       "      <td>-0.054065</td>\n",
       "      <td>0.248976</td>\n",
       "      <td>...</td>\n",
       "      <td>0.488178</td>\n",
       "      <td>-1.129672</td>\n",
       "      <td>-1.543631</td>\n",
       "      <td>-1.408268</td>\n",
       "      <td>1.152522</td>\n",
       "      <td>0.356141</td>\n",
       "      <td>0.530854</td>\n",
       "      <td>-1.081894</td>\n",
       "      <td>33717.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33507 rows × 258 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              1         2         3         4         5         6         7  \\\n",
       "0     -0.098259 -0.778371 -0.567632 -0.864359 -0.912805 -0.907104  1.634788   \n",
       "1     -0.225349 -0.772242 -0.580955 -1.056253 -1.319412 -0.937095  1.204180   \n",
       "2      0.000942 -0.598621 -0.672390 -0.641442 -1.040335 -0.755668  1.274980   \n",
       "3     -0.120756 -0.799536 -0.620240 -0.805628 -1.006669 -0.907013  1.548830   \n",
       "4     -0.087019 -0.803882 -0.531570 -0.888757 -1.351001 -0.758447  1.058772   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "33502 -0.144893 -0.688863 -0.502946 -0.884095 -0.949489 -0.980342  1.229003   \n",
       "33503  0.148355 -0.644058 -0.473307 -0.542517 -0.696149 -0.696782  1.290697   \n",
       "33504 -0.148756 -0.699471 -0.448784 -0.855049 -0.818939 -0.974509  1.198471   \n",
       "33505 -0.018792 -0.577633 -0.492049 -0.767255 -0.861196 -0.903652  1.238136   \n",
       "33506 -0.000482 -0.664418 -0.515143 -0.587033 -1.007960 -0.820357  1.285978   \n",
       "\n",
       "              8         9        10  ...       249       250       251  \\\n",
       "0     -0.941197  0.169670  0.114994  ...  0.455128 -0.650662 -1.605736   \n",
       "1     -0.703993 -0.227651  0.182004  ...  0.496497 -1.000214 -1.584138   \n",
       "2     -0.978171 -0.042830  0.124320  ...  0.494047 -0.849385 -1.569252   \n",
       "3     -0.936683  0.091012  0.218409  ...  0.447258 -0.846772 -1.625529   \n",
       "4     -0.702686 -0.500163  0.162552  ...  0.480105 -1.057028 -1.528152   \n",
       "...         ...       ...       ...  ...       ...       ...       ...   \n",
       "33502 -0.794743 -0.065076  0.398823  ...  0.456091 -0.921927 -1.539195   \n",
       "33503 -0.928129 -0.102882  0.279975  ...  0.486481 -0.966628 -1.423398   \n",
       "33504 -0.822832  0.027991  0.465258  ...  0.452030 -0.963613 -1.493534   \n",
       "33505 -0.859306  0.040800  0.341311  ...  0.494729 -0.926624 -1.494075   \n",
       "33506 -0.938048 -0.054065  0.248976  ...  0.488178 -1.129672 -1.543631   \n",
       "\n",
       "            252       253       254       255       256    ROWID    y  \n",
       "0     -1.492585  1.182727  0.519747  0.673476 -1.080819      1.0  0.0  \n",
       "1     -1.652526  1.257367  0.468274  0.278431 -0.962163      2.0  0.0  \n",
       "2     -1.409305  1.183825  0.358066  0.373570 -1.182730      3.0  0.0  \n",
       "3     -1.477122  1.251898  0.502077  0.407137 -1.082190      4.0  0.0  \n",
       "4     -1.568092  1.297123  0.465277  0.306736 -0.968242      5.0  0.0  \n",
       "...         ...       ...       ...       ...       ...      ...  ...  \n",
       "33502 -1.564770  1.214185  0.504790  0.358191 -0.886767  33713.0  6.0  \n",
       "33503 -1.251273  1.157585  0.344522  0.594526 -1.020150  33714.0  6.0  \n",
       "33504 -1.490965  1.182805  0.537592  0.380066 -0.932183  33715.0  6.0  \n",
       "33505 -1.484290  1.192897  0.479127  0.373609 -0.984641  33716.0  6.0  \n",
       "33506 -1.408268  1.152522  0.356141  0.530854 -1.081894  33717.0  6.0  \n",
       "\n",
       "[33507 rows x 258 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_embedding_file(csv_filename):\n",
    "\n",
    "    df=pd.read_csv(csv_filename)\n",
    "    \n",
    "    column_names = [f'{i}' for i in range(1, df.shape[1]-1)]\n",
    "    column_names.extend(['ROWID', 'y'])\n",
    "    \n",
    "    df.columns = column_names\n",
    "    return df\n",
    "df = load_embedding_file('./homo_sapiens_hyena_embedding.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0c84b0-a857-4285-9b5c-cac3cc2577d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-2.0.1",
   "language": "python",
   "name": "pytorch-2.0.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

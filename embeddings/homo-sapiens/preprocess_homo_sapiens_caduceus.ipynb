{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bcb45e4f-b2f2-4a50-8d66-8ff598a2e8b5",
   "metadata": {},
   "source": [
    "### Generate tokenizer and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef55f5d-c55e-4318-b832-0601921b8eb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# Load pre-trained Caduceus model and tokenizer\n",
    "model_name = \"kuleshov-group/caduceus-ph_seqlen-131k_d_model-256_n_layer-16\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForMaskedLM.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Move model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92385ec-6558-4b84-8b3f-5dd927919038",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "sequences = list(SeqIO.parse(\"../genome.hg38rg.fa\", \"fasta\"))\n",
    "\n",
    "def extract_sequence_segment(seq_id, start, end, sequences):\n",
    "    for seq_record in sequences:\n",
    "        if seq_record.id == seq_id:\n",
    "            segment = str(seq_record.seq[start:end])\n",
    "            return segment\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb3099b-dab9-46f9-b7f2-2427b15866eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run preprocess_utility.py\n",
    "\n",
    "datafile_path = '../../datasets/task03-homo-sapiens/Homo_sapiens.GRCh38.109.txt.gz'  \n",
    "dataframe = preprocess_home_sapiens_datafile(datafile_path)\n",
    "dataframe\n",
    "\n",
    "rows = []\n",
    "for index, row in dataframe.iterrows():\n",
    "    chrom=str(row['CHROM'])\n",
    "    start=int(row['START'])\n",
    "    end=int(row['END'])\n",
    "    y=row['y']\n",
    "    rowid=row['ROWID']\n",
    "    segment = extract_sequence_segment(chrom, start, end, sequences)\n",
    "    rows.append([segment, rowid, y])\n",
    "\n",
    "columns=['sequence','ROWID','y']\n",
    "df = pd.DataFrame(rows, columns=columns)\n",
    "df_origin = df\n",
    "df_origin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4977bece-f0ab-4808-abe0-82884a3d72ae",
   "metadata": {},
   "source": [
    "### Mathod One: compute embeddings one by one. (works but slow, takes 10+ min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e6aceb-aa9d-40f9-ab20-ef7468deaf23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import dask.dataframe as dd\n",
    "# from dask.diagnostics import ProgressBar\n",
    "# import numpy as np\n",
    "\n",
    "# # 设置并行参数和批处理大小\n",
    "# num_parallel = 10\n",
    "\n",
    "# # 创建一个DataFrame，包含DNA序列\n",
    "# segments_df = pd.DataFrame(segments, columns=['sequence'])\n",
    "\n",
    "# # 使用Dask对DataFrame进行并行化处理\n",
    "# segments_ddf = dd.from_pandas(segments_df, npartitions=num_parallel)\n",
    "\n",
    "# # 定义处理嵌入的函数\n",
    "# def process_embedding(df):\n",
    "#     embeddings = []\n",
    "#     for dna in df['sequence']:\n",
    "#         tokens = tokenizer(dna, return_tensors='pt', padding='max_length', max_length=512, truncation=True)\n",
    "#         tokens = {key: val.to(device) for key, val in tokens.items()}\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(**tokens, output_hidden_states=True)\n",
    "#             hidden_states = outputs.hidden_states\n",
    "#             last_layer_embeddings = hidden_states[-1]  # 获取最后一层的嵌入\n",
    "#             mean_embeddings = torch.mean(last_layer_embeddings, dim=1)  # 计算平均嵌入\n",
    "#             mean_embeddings = mean_embeddings.view(mean_embeddings.shape[0], -1)\n",
    "#             embeddings.append(mean_embeddings.cpu().numpy())\n",
    "\n",
    "#     # 将嵌入结果转换为DataFrame\n",
    "#     embeddings = np.vstack(embeddings)\n",
    "#     return embeddings\n",
    "\n",
    "# # 显示进度条并进行并行计算\n",
    "# with ProgressBar():\n",
    "#     ddf_embeddings = segments_ddf.map_partitions(process_embedding).compute()\n",
    "\n",
    "# df_embeddings = pd.DataFrame(ddf_embeddings)\n",
    "# df_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fab776-63ab-42fb-9139-b983eb14c525",
   "metadata": {},
   "source": [
    "### Method Two: compute in batches (fast take 1+ min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b59b383-8e87-4261-9a41-6be9b4404403",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "import jax.numpy as jnp\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "# Vectorized tokenization function\n",
    "def vectorized_tokenizer(subsequences):\n",
    "    # Tokenize the batch of sequences\n",
    "    tokens = tokenizer(subsequences, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    \n",
    "    # Move tokens to GPU\n",
    "    tokens = {key: val.to(device) for key, val in tokens.items()}\n",
    "    return tokens\n",
    "\n",
    "# Vectorized embedding function\n",
    "def vectorized_embedding(tokens):\n",
    "    # Forward pass to compute last layer embeddings for the batch\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens, output_hidden_states=True)  # Enable output of hidden states\n",
    "        hidden_states = outputs.hidden_states  # Access all hidden states\n",
    "        last_layer_embeddings = hidden_states[-1]  # Get the last layer embeddings (batch_size, seq_len, hidden_size)\n",
    "    \n",
    "    # Compute the mean of the last layer embeddings across the token (sequence) dimension for each sequence in the batch\n",
    "    # Dimension 1 corresponds to the token/sequence length, so we compute the mean along this axis\n",
    "    mean_embeddings = torch.mean(last_layer_embeddings, dim=1)  # (batch_size, hidden_size)\n",
    "    \n",
    "    # If needed, squeeze out any extra dimensions (though this shouldn't be necessary after mean calculation)\n",
    "    mean_embeddings_squeezed = mean_embeddings.squeeze(dim=1)\n",
    "\n",
    "    return mean_embeddings_squeezed\n",
    "\n",
    "# Tokenization and embedding combined in a batch-wise function\n",
    "def process_batch(subsequences):\n",
    "    tokens = vectorized_tokenizer(subsequences)\n",
    "    embeddings = vectorized_embedding(tokens)\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def apply_get_embeddings_dask(df):\n",
    "    subsequences = df['sequence'].tolist() \n",
    "    embeddings = process_batch(subsequences)  # Process in a vectorized manner\n",
    "    embeddings_cpu = embeddings.cpu().numpy()\n",
    "    \n",
    "    # df['embedding'] = list(embeddings_cpu)  # Assign embeddings back to the DataFrame\n",
    "    df2 = pd.DataFrame(embeddings_cpu, columns=[f'{i+1}' for i in range(embeddings_cpu.shape[1])])\n",
    "    df = pd.concat([df.reset_index(drop=True), df2.reset_index(drop=True)], axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec2bfdc-9725-4a6d-b5fc-2e26be0beb7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "%run preprocess_utility.py\n",
    "\n",
    "typename=\"homo_sapiens\"\n",
    "\n",
    "\n",
    "for chunkid in range(0,1):\n",
    "    \n",
    "    #================df -> df's chunks================\n",
    "    # Define the number of rows per chunk\n",
    "    chunk_size = 10000  \n",
    "    num_parallel = 10\n",
    "\n",
    "    # Calculate the number of chunks\n",
    "    num_chunks = int(np.ceil(len(df) / chunk_size))  \n",
    "    \n",
    "    # Split the DataFrame into chunks using array_split\n",
    "    chunks = np.array_split(df, num_chunks)\n",
    "    \n",
    "    # Initialize an empty list to store the processed chunks\n",
    "    processed_chunks = []\n",
    "    \n",
    "    #================process each chunk with dask's ddf================\n",
    "    # Iterate over each chunk\n",
    "    for chunk in chunks:\n",
    "\n",
    "        ddf = dd.from_pandas(chunk, npartitions=num_parallel) \n",
    "    \n",
    "        num_embedding_columns = 256\n",
    "        \n",
    "        meta = chunk.copy()\n",
    "        meta = meta.drop(columns=['embedding'], errors='ignore')  # Drop 'embedding' if it exists\n",
    "        # Add new embedding columns to the metadata\n",
    "        for i in range(num_embedding_columns):\n",
    "            # Adjust type, float is common for embeddings\n",
    "            meta[f'{i+1}'] = float  \n",
    "\n",
    "    \n",
    "        # Apply the function in parallel using Dask\n",
    "        ddf = ddf.map_partitions(apply_get_embeddings_dask, meta=meta)\n",
    "    \n",
    "        # Compute the result with progress tracking\n",
    "        with ProgressBar():\n",
    "            processed_chunk = ddf.compute()\n",
    "    \n",
    "        # Append processed chunk to list\n",
    "        processed_chunks.append(processed_chunk)\n",
    "\n",
    "        \n",
    "    # Concatenate all processed chunks into a final DataFrame\n",
    "    final_df = pd.concat(processed_chunks, ignore_index=True)\n",
    "\n",
    "    final_df = final_df.drop(columns=['sequence'])\n",
    "    final_df = swapfirst2last(final_df)\n",
    "    final_df = swapfirst2last(final_df)\n",
    "\n",
    "    final_df.to_csv(typename+'_caduceus_embedding_'+str(chunkid)+'.csv', index=False)\n",
    "    print(f\"{typename}_caduceus_embedding_{chunkid}.csv is created.\")\n",
    "\n",
    "final_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

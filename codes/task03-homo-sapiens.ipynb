{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d00d8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit # train_test_split, \n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%run utility.py\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b16e00d-5625-4743-814b-9080b51d3e16",
   "metadata": {},
   "source": [
    "### Main Homo-Sapiens Classification Algorightm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86e000d",
   "metadata": {},
   "source": [
    "### Build ANN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537d2740",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the MLP model (from previous examples)\n",
    "class TwoLayerMLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1,hidden_size2, num_classes):\n",
    "        super(TwoLayerMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.fc3 = nn.Linear(hidden_size2, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66875217-839f-4bb6-9622-53aec45d486b",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302b9866-faae-47a0-a772-b9073c0f7989",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training and validation loop\n",
    "\n",
    "def  trainModel(model, train_loader, val_loader):\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set model to training mode\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)  # Move data to GPU\n",
    "            optimizer.zero_grad()  # Zero the parameter gradients\n",
    "            outputs = model(inputs)  # Forward pass\n",
    "            loss = criterion(outputs, targets)  # Compute loss\n",
    "            loss.backward()  # Backward pass\n",
    "            optimizer.step()  # Update parameters\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()  # Set model to evaluation mode\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in val_loader:\n",
    "                inputs, targets = inputs.to(device), targets.to(device)  # Move data to GPU\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += (predicted == targets).sum().item()\n",
    "    \n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        val_accuracy = correct / total\n",
    "    \n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Accuracy: {val_accuracy * 100:.2f}%')\n",
    "    \n",
    "    \n",
    "    print('Training completed.')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e5e310-9d34-41bd-86ea-21e95172deab",
   "metadata": {},
   "source": [
    "### Overall Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb3ce1e-39ae-4e35-bb88-bf74ff4f1efb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Overall_Evaluate(model, test_loader):\n",
    "    # Evaluation on test data\n",
    "    model.eval() \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)  # Move data to GPU\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    print(f'Accuracy on test data: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ede763-e03d-432f-8823-269cf82cb549",
   "metadata": {},
   "source": [
    "### Accuracty Evaluation Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7a93a4-fd5c-421a-a573-b70daf1b306b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def Breakdown_Evaluate(model, test_loader):\n",
    "    model.eval() \n",
    "    class_correct = [0] * num_classes\n",
    "    class_total = [0] * num_classes\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device) \n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            # Update total count for each class\n",
    "            for i in range(targets.size(0)):\n",
    "                label = targets[i]\n",
    "                class_total[label] += 1\n",
    "                class_correct[label] += (predicted[i] == label).item()\n",
    "\n",
    "    acc_data=[]\n",
    "    \n",
    "    # Calculate and print accuracy for each class\n",
    "    for i in range(num_classes):\n",
    "        if class_total[i] > 0:  # Avoid division by zero\n",
    "            accuracy = class_correct[i] / class_total[i]\n",
    "            print(f'Accuracy of class {i}: {accuracy * 100:.2f}%')\n",
    "            acc_data.append(accuracy)\n",
    "        else:\n",
    "            print(f'No samples for class {i} in the test set.')\n",
    "\n",
    "    return acc_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e081f1e-371e-41cc-a8cc-c806c6eb8931",
   "metadata": {},
   "source": [
    "### AUC(Area Under the Curve) Evaluation Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6dfa48b-61ef-4a23-a562-1ab4e07d5e80",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def Breakdown_Evaluate_AUC(model, test_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_targets = []\n",
    "    all_outputs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)  # Move data to GPU\n",
    "            \n",
    "            # Get model outputs and apply softmax to get class probabilities\n",
    "            outputs = model(inputs)\n",
    "            probabilities = F.softmax(outputs, dim=1)  # Probabilities for each class\n",
    "            \n",
    "            # Collect all targets and probabilities\n",
    "            all_targets.append(targets.cpu())\n",
    "            all_outputs.append(probabilities.cpu())\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    all_targets = torch.cat(all_targets)\n",
    "    all_outputs = torch.cat(all_outputs)\n",
    "\n",
    "    auc_data = []\n",
    "    \n",
    "    # Compute AUC for each class (one-vs-rest)\n",
    "    for i in range(num_classes):\n",
    "        # Binarize the targets for class 'i'\n",
    "        binarized_targets = (all_targets == i).int()  # 1 if target is 'i', else 0\n",
    "        \n",
    "        # Get the predicted probabilities for class 'i'\n",
    "        class_probabilities = all_outputs[:, i]\n",
    "        \n",
    "        # Compute AUC for class 'i'\n",
    "        if len(set(binarized_targets.tolist())) > 1:  # Ensure we have both classes in the test set\n",
    "            auc = roc_auc_score(binarized_targets, class_probabilities)\n",
    "            print(f'AUC of class {i}: {auc:.2f}')\n",
    "            auc_data.append(auc)\n",
    "            \n",
    "        else:\n",
    "            print(f'Not enough data for class {i} to compute AUC.')\n",
    "    \n",
    "    return auc_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d263a6-90bf-4e88-9273-e824bc76208d",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2cbe9b-5593-47fe-a1a2-9c0acfb298a3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "hidden_size1 = 120\n",
    "hidden_size2 = 64\n",
    "num_classes = 7\n",
    "num_epochs = 20\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b37afc-97da-496d-bbae-02d801f54b83",
   "metadata": {},
   "source": [
    "### Accuracy Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467a0f64-b93d-4140-8322-05cb5aadca1e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "columns = [f'class{i}' for i in range(7)]  \n",
    "df_acc_nt = pd.DataFrame(columns=columns)\n",
    "df_acc_gpn = pd.DataFrame(columns=columns)\n",
    "df_acc_dnabert2 = pd.DataFrame(columns=columns)\n",
    "df_acc_hyena = pd.DataFrame(columns=columns)\n",
    "df_acc_caduceus = pd.DataFrame(columns=columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd777935-0bc4-4189-a45a-3df59826a78c",
   "metadata": {},
   "source": [
    "### T-Test Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976c63a7-cdd5-4a74-9686-caf1b0964360",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "columns = [f'class{i}' for i in range(7)]  \n",
    "df_auc_nt = pd.DataFrame(columns=columns)\n",
    "df_auc_gpn = pd.DataFrame(columns=columns)\n",
    "df_auc_dnabert2 = pd.DataFrame(columns=columns)\n",
    "df_auc_hyena = pd.DataFrame(columns=columns)\n",
    "df_auc_caduceus = pd.DataFrame(columns=columns)\n",
    "\n",
    "runcount=10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1769521e-46f8-4d15-9bd6-faa097c220b8",
   "metadata": {},
   "source": [
    "### split dataframe into 3 parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6099ccc-0fac-4caa-bcca-239e1c4d57a2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def run_test(runcount, model, train_loader,val_loader, test_loader, df_acc, df_auc):\n",
    "    \n",
    "    for i in range(0, runcount):\n",
    "        # train_loader,val_loader,test_loader =prepare_dataloader(data_array)\n",
    "        model = trainModel(model, train_loader,val_loader)\n",
    "        \n",
    "        Overall_Evaluate(model, test_loader)\n",
    "    \n",
    "        acc_data=Breakdown_Evaluate(model,test_loader)\n",
    "        df_acc.loc[len(df_acc)] = acc_data  \n",
    "        \n",
    "        auc_data = Breakdown_Evaluate_AUC(model,test_loader)\n",
    "        df_auc.loc[len(df_auc)] = auc_data\n",
    "\n",
    "    return df_acc, df_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6024d0c-c387-4c0d-b711-75446b4aea69",
   "metadata": {},
   "source": [
    "### Filter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce77982-db30-49ee-98b4-5e8f46be10a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(dfA,dfB):\n",
    "    return dfA[dfA[['ROWID']].isin(dfB[['ROWID']].to_dict(orient='list')).all(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ab2cd2-e089-4ce4-aa18-235158d567d0",
   "metadata": {},
   "source": [
    "### Dataframe to dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8894486-6f84-4beb-9ade-cdced6c39398",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def df2dataloader(df_train, df_val, df_test):\n",
    "\n",
    "    X_train, y_train = df_train.iloc[:, :-1].values, df_train.iloc[:, -1].values\n",
    "    X_val, y_val = df_val.iloc[:, :-1].values, df_val.iloc[:, -1].values\n",
    "    X_test, y_test = df_test.iloc[:, :-1].values, df_test.iloc[:, -1].values\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.long)  # assuming labels are integers\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "    \n",
    "    # Create TensorDatasets\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "   \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)  \n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)     \n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)   \n",
    "\n",
    "    \n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96012dd3-fb45-4284-9961-2ae11e976b18",
   "metadata": {},
   "source": [
    "### Set base directory for embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2896cae5-27b1-426e-b456-1970aad7b38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = '../embeddings/homo-sapiens/embedding-csv/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08baabe7-3f9b-48f6-a40c-2492e22c47fb",
   "metadata": {},
   "source": [
    "### NT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fda7e49-fa89-4972-98ce-63c6b19e60c7",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def to_dataloader(X, y, batch_size=32, shuffle=True):\n",
    "    # Convert NumPy arrays to PyTorch tensors\n",
    "    X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y.values, dtype=torch.long)\n",
    "    \n",
    "    # Create TensorDataset\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    \n",
    "    # Create DataLoader\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle)\n",
    "    \n",
    "    return loader\n",
    "    \n",
    "\n",
    "def load_embedding_file_NT(csv_filename):\n",
    "\n",
    "    df=pd.read_csv(csv_filename)\n",
    "    \n",
    "    column_names = [f'{i}' for i in range(0, df.shape[1]-2)]\n",
    "    column_names.extend(['ROWID',  'y']) \n",
    "    df.columns = column_names\n",
    "    \n",
    "    # Split the dataframe into features (X) and labels (y)\n",
    "    features = df.iloc[:, :-1]  \n",
    "    labels = df.iloc[:, -1]     \n",
    "    \n",
    "    # Initialize Stratified Shuffle Split\n",
    "    split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=None)\n",
    "    \n",
    "    for train_index, test_index in split.split(features, labels):\n",
    "        # Use pandas indexing to split data\n",
    "        X_train_val, X_test = features.iloc[train_index], features.iloc[test_index]\n",
    "        y_train_val, y_test = labels.iloc[train_index], labels.iloc[test_index]\n",
    "    \n",
    "    # Now split the training set into training and validation sets\n",
    "    split = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=None)\n",
    "    \n",
    "    for train_index, val_index in split.split(X_train_val, y_train_val):\n",
    "        X_train, X_val = X_train_val.iloc[train_index], X_train_val.iloc[val_index]\n",
    "        y_train, y_val = y_train_val.iloc[train_index], y_train_val.iloc[val_index]    \n",
    "    \n",
    "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "def One_Run_NT(input_size, df_acc_nt, df_auc_nt):\n",
    "    \n",
    "    X_train_df, X_val_df, X_test_df, y_train, y_val, y_test =  load_embedding_file_NT(base_dir + 'homo_sapiens_nt_embedding.csv') \n",
    "    \n",
    "    dropcolumnns=['ROWID'] \n",
    "    X_train = X_train_df.drop(dropcolumnns, axis=1)\n",
    "    X_val   = X_val_df.drop(dropcolumnns, axis=1)\n",
    "    X_test  = X_test_df.drop(dropcolumnns, axis=1)\n",
    "    \n",
    "    train_loader = to_dataloader(X_train, y_train)\n",
    "    val_loader = to_dataloader(X_val, y_val)\n",
    "    test_loader = to_dataloader(X_test, y_test)\n",
    "    \n",
    "    model = TwoLayerMLP(input_size, hidden_size1,hidden_size2,  num_classes).to(device)    \n",
    "\n",
    "    df_acc_nt, df_auc_nt = run_test(1,model, train_loader,val_loader, test_loader, df_acc_nt, df_auc_nt)\n",
    "    return df_acc_nt, df_auc_nt, X_train_df, X_val_df, X_test_df\n",
    "    \n",
    "\n",
    "df_acc_nt, df_auc_nt, X_train_df, X_val_df, X_test_df = One_Run_NT(1280, df_acc_nt, df_auc_nt)\n",
    "\n",
    "print(len(X_train_df))\n",
    "print(len(X_val_df))\n",
    "print(len(X_test_df))\n",
    "\n",
    "print(df_acc_nt)\n",
    "print('\\n')\n",
    "print(df_auc_nt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efffb61c-494c-44a2-95a2-3d34d3fcbca2",
   "metadata": {},
   "source": [
    "##  Load embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcc1394-5aba-4c0a-866d-05b9aee1ff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding_file(csv_filename, X_train_df,X_val_df,X_test_df):\n",
    "\n",
    "    df=pd.read_csv(csv_filename)\n",
    "    \n",
    "    column_names = [f'{i}' for i in range(0, df.shape[1]-2)]\n",
    "    column_names.extend(['ROWID', 'y'])\n",
    "    df.columns = column_names\n",
    "                   \n",
    "    df_train= filter(df, X_train_df)\n",
    "    df_val  = filter(df, X_val_df)\n",
    "    df_test = filter(df, X_test_df)\n",
    "\n",
    "    dropcolumns=['ROWID']\n",
    "    df_train= df_train.drop(dropcolumns, axis=1) \n",
    "    df_val  = df_val.drop(dropcolumns, axis=1) \n",
    "    df_test = df_test.drop(dropcolumns, axis=1) \n",
    "\n",
    "    return df2dataloader(df_train, df_val, df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250f5491",
   "metadata": {},
   "source": [
    "### GPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d93792-5aa4-4ab8-872a-0c5d0bd377f0",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def One_Run_GPN(input_size, df_acc, df_auc):\n",
    "\n",
    "    train_loader, val_loader, test_loader=load_embedding_file(base_dir + 'homo_sapiens_gpn_embedding.csv', X_train_df,X_val_df,X_test_df)   \n",
    "    model = TwoLayerMLP(input_size, hidden_size1,hidden_size2,  num_classes).to(device)    \n",
    "    df_acc, df_auc = run_test(1,model, train_loader,val_loader, test_loader, df_acc, df_auc)\n",
    "    return df_acc, df_auc\n",
    "\n",
    "# input_size = 768\n",
    "df_acc_gpn, df_auc_gpn = One_Run_GPN(768, df_acc_gpn, df_auc_gpn)\n",
    "\n",
    "print(df_acc_gpn)\n",
    "print('\\n')\n",
    "print(df_auc_gpn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7058c258-f55c-4621-9214-2da150828219",
   "metadata": {},
   "source": [
    "### DNABERT2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bceb7b19-62f3-4b5b-8432-37049dfba26d",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def One_Run_DNABERT2(input_size, df_acc, df_auc):\n",
    "\n",
    "    train_loader, val_loader, test_loader=load_embedding_file(base_dir + 'homo_sapiens_dnabert2_embedding.csv', X_train_df,X_val_df,X_test_df)    \n",
    "    model = TwoLayerMLP(input_size, hidden_size1,hidden_size2,  num_classes).to(device)    \n",
    "    df_acc, df_auc = run_test(1,model, train_loader,val_loader, test_loader, df_acc, df_auc)\n",
    "    return df_acc, df_auc\n",
    "\n",
    "# input_size = 768\n",
    "df_acc_dnabert2, df_auc_dnabert2 = One_Run_DNABERT2(768, df_acc_dnabert2, df_auc_dnabert2)\n",
    "\n",
    "print(df_acc_dnabert2)\n",
    "print('\\n')\n",
    "print(df_auc_dnabert2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25928e08-41d6-4a21-8fb0-9babc1b71fe6",
   "metadata": {},
   "source": [
    "### HyenaDNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44484b0-9a25-4826-8ed1-a3f80b8a91ea",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def One_Run_HYENADNA(input_size, df_acc, df_auc):\n",
    "\n",
    "    train_loader, val_loader, test_loader=load_embedding_file(base_dir + 'homo_sapiens_hyena_embedding.csv', X_train_df,X_val_df,X_test_df)    \n",
    "    model = TwoLayerMLP(input_size, hidden_size1,hidden_size2,  num_classes).to(device)    \n",
    "    df_acc, df_auc = run_test(1,model,train_loader,val_loader, test_loader, df_acc, df_auc)\n",
    "    return df_acc, df_auc\n",
    "\n",
    "# input_size = 256\n",
    "df_acc_hyena, df_auc_hyena = One_Run_HYENADNA(256, df_acc_hyena, df_auc_hyena)\n",
    "\n",
    "print(df_acc_hyena)\n",
    "print('\\n')\n",
    "print(df_auc_hyena)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f739e9-2ff0-46c2-8382-9c58f867dbb8",
   "metadata": {},
   "source": [
    "### Caduceus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adab5dfc-68e2-40d9-a671-7624e6520d79",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def One_Run_CADUCEUS(input_size, df_acc, df_auc):\n",
    "\n",
    "    train_loader, val_loader, test_loader=load_embedding_file(base_dir + 'homo_sapiens_caduceus_embedding.csv', X_train_df,X_val_df,X_test_df)    \n",
    "    model = TwoLayerMLP(input_size, hidden_size1,hidden_size2,  num_classes).to(device)    \n",
    "    df_acc, df_auc = run_test(1,model,train_loader,val_loader, test_loader, df_acc, df_auc)\n",
    "    return df_acc, df_auc\n",
    "\n",
    "# input_size = 256\n",
    "df_acc_caduceus, df_auc_caduceus = One_Run_CADUCEUS(256, df_acc_caduceus, df_auc_caduceus)\n",
    "\n",
    "print(df_acc_caduceus)\n",
    "print('\\n')\n",
    "print(df_auc_caduceus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf31557-f402-45ce-9966-13b956164de0",
   "metadata": {},
   "source": [
    "## Perfrom T-Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42864f49-3032-4bbe-bea3-7c37abcdfa0b",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def clear_df(df):\n",
    "    df=df.drop(df.index)\n",
    "    return df\n",
    "\n",
    "df_acc_nt      =clear_df(df_acc_nt)\n",
    "df_acc_gpn     =clear_df(df_acc_gpn)\n",
    "df_acc_dnabert2=clear_df(df_acc_dnabert2)\n",
    "df_acc_hyena   =clear_df(df_acc_hyena)\n",
    "df_acc_caduceus=clear_df(df_acc_caduceus)\n",
    "\n",
    "df_auc_nt      =clear_df(df_auc_nt)\n",
    "df_auc_gpn     =clear_df(df_auc_gpn)\n",
    "df_auc_dnabert2=clear_df(df_auc_dnabert2)\n",
    "df_auc_hyena   =clear_df(df_auc_hyena)\n",
    "df_auc_caduceus=clear_df(df_auc_caduceus)\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    print(\"====round \"+str(i+1)+\"======\")\n",
    "    \n",
    "    df_acc_nt, df_auc_nt, X_train_df, X_val_df, X_test_df = One_Run_NT(1280, df_acc_nt, df_auc_nt)\n",
    "\n",
    "    df_acc_gpn, df_auc_gpn = One_Run_GPN(768, df_acc_gpn, df_auc_gpn)\n",
    "\n",
    "    df_acc_dnabert2, df_auc_dnabert2 = One_Run_DNABERT2(768, df_acc_dnabert2, df_auc_dnabert2)\n",
    "\n",
    "    df_acc_hyena, df_auc_hyena = One_Run_HYENADNA(256, df_acc_hyena, df_auc_hyena)\n",
    "    \n",
    "    df_acc_caduceus, df_auc_caduceus = One_Run_CADUCEUS(256, df_acc_caduceus, df_auc_caduceus)\n",
    "\n",
    "print(\"====Print Accuracy======\")\n",
    "print(df_acc_nt)\n",
    "print('\\n')\n",
    "print(df_acc_gpn)\n",
    "print('\\n')\n",
    "print(df_acc_dnabert2)\n",
    "print('\\n')\n",
    "print(df_acc_hyena)\n",
    "print('\\n')\n",
    "print(df_acc_caduceus)\n",
    "\n",
    "print(\"====Print AUC======\")\n",
    "print(df_auc_nt)\n",
    "print('\\n')\n",
    "print(df_auc_gpn)\n",
    "print('\\n')\n",
    "print(df_auc_dnabert2)\n",
    "print('\\n')\n",
    "print(df_auc_hyena)\n",
    "print('\\n')\n",
    "print(df_auc_caduceus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806407c5-0532-43e0-ae20-23b23b3e7d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_acc_nt.to_csv('t_acc_nt.csv', index=False)\n",
    "df_acc_gpn.to_csv('t_acc_gpn.csv', index=False)\n",
    "df_acc_dnabert2.to_csv('t_acc_dnabert2.csv', index=False)\n",
    "df_acc_hyena.to_csv('t_acc_hyena.csv', index=False)\n",
    "df_acc_caduceus.to_csv('t_acc_caduceus.csv', index=False)\n",
    "\n",
    "df_auc_nt.to_csv('t_auc_nt.csv', index=False)\n",
    "df_auc_gpn.to_csv('t_auc_gpn.csv', index=False)\n",
    "df_auc_dnabert2.to_csv('t_auc_dnabert2.csv', index=False)\n",
    "df_auc_hyena.to_csv('t_auc_hyena.csv', index=False)\n",
    "df_auc_caduceus.to_csv('t_auc_caduceus.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
